#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
OpenAIå®¢æˆ·ç«¯å®ç° - ç»Ÿä¸€LLMæ¥å£
OpenAI Client Implementation - Unified LLM Interface
"""

import time
import logging
from typing import List, Optional, Union, Dict, Any

try:
    import openai
    from openai import OpenAI, AzureOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    
from ..llm_base import (
    BaseLLMClient, LLMConfig, LLMResponse, LLMMessage, LLMUsage, 
    LLMProvider, LLMErrorType, create_error_response
)

logger = logging.getLogger(__name__)


class OpenAIClient(BaseLLMClient):
    """
    OpenAIå®¢æˆ·ç«¯ - å®ç°ç»Ÿä¸€LLMæ¥å£
    
    æ”¯æŒ:
    - OpenAI GPTæ¨¡å‹
    - Azure OpenAIæœåŠ¡
    - å®Œæ•´çš„èŠå¤©å®ŒæˆåŠŸèƒ½
    - é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
    """
    
    def __init__(self, config: LLMConfig):
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        
        Args:
            config: LLMé…ç½®å¯¹è±¡
        """
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAIåº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install openai")
        
        super().__init__(config)
        
        # æ ¹æ®æä¾›å•†ç±»å‹åˆ›å»ºå®¢æˆ·ç«¯
        if config.provider == LLMProvider.AZURE_OPENAI:
            # Azure OpenAIé…ç½®
            azure_endpoint = config.base_url or config.extra_params.get('azure_endpoint')
            api_version = config.extra_params.get('api_version', '2024-02-15-preview')
            
            if not azure_endpoint:
                raise ValueError("Azure OpenAIéœ€è¦æä¾›base_urlæˆ–azure_endpoint")
            
            self.client = AzureOpenAI(
                api_key=config.api_key,
                azure_endpoint=azure_endpoint,
                api_version=api_version,
                timeout=config.timeout[1]  # ä½¿ç”¨è¯»å–è¶…æ—¶
            )
            logger.info(f"ğŸ¤– Azure OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {azure_endpoint}")
            
        else:
            # æ ‡å‡†OpenAIé…ç½®
            self.client = OpenAI(
                api_key=config.api_key,
                base_url=config.base_url,
                timeout=config.timeout[1],
                max_retries=config.max_retries
            )
            logger.info(f"ğŸ¤– OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {config.model_name}")
    
    def chat_completion(self, 
                       messages: Union[str, List[LLMMessage]], 
                       temperature: Optional[float] = None,
                       max_tokens: Optional[int] = None,
                       **kwargs) -> LLMResponse:
        """
        OpenAIèŠå¤©å®Œæˆæ¥å£å®ç°
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: ç»Ÿä¸€çš„å“åº”å¯¹è±¡
        """
        start_time = time.time()
        
        try:
            # å‡†å¤‡æ¶ˆæ¯æ ¼å¼
            prepared_messages = self._prepare_messages(messages)
            
            # è½¬æ¢ä¸ºOpenAI APIæ ¼å¼
            openai_messages = []
            for msg in prepared_messages:
                openai_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # å‚æ•°å¤„ç†
            params = {
                "model": kwargs.get('model') or self.config.model_name,
                "messages": openai_messages,
                "temperature": temperature or self.config.temperature,
                "max_tokens": max_tokens or self.config.max_tokens,
                "top_p": kwargs.get('top_p') or self.config.top_p,
                "frequency_penalty": kwargs.get('frequency_penalty') or self.config.frequency_penalty,
                "presence_penalty": kwargs.get('presence_penalty') or self.config.presence_penalty
            }
            
            # è°ƒç”¨OpenAI API
            logger.debug(f"ğŸ¤– è°ƒç”¨OpenAI API: {params['model']}")
            response = self.client.chat.completions.create(**params)
            
            response_time = time.time() - start_time
            
            # æå–å“åº”å†…å®¹
            content = response.choices[0].message.content or ""
            finish_reason = response.choices[0].finish_reason
            
            # æ„å»ºä½¿ç”¨ç»Ÿè®¡
            usage = None
            if response.usage:
                usage = LLMUsage(
                    prompt_tokens=response.usage.prompt_tokens,
                    completion_tokens=response.usage.completion_tokens,
                    total_tokens=response.usage.total_tokens
                )
            
            # æ„å»ºå“åº”å¯¹è±¡
            llm_response = LLMResponse(
                success=True,
                content=content,
                provider=self.provider.value,
                model=response.model,
                response_time=response_time,
                usage=usage,
                finish_reason=finish_reason,
                raw_response=response.model_dump() if hasattr(response, 'model_dump') else None
            )
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(llm_response)
            
            logger.info(f"âœ… OpenAI APIè°ƒç”¨æˆåŠŸ: {content[:50]}...")
            return llm_response
            
        except openai.AuthenticationError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.AUTHENTICATION,
                error_message=f"OpenAIè®¤è¯å¤±è´¥: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ OpenAIè®¤è¯å¤±è´¥: {e}")
            return error_response
            
        except openai.RateLimitError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.RATE_LIMIT,
                error_message=f"OpenAIé€Ÿç‡é™åˆ¶: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ OpenAIé€Ÿç‡é™åˆ¶: {e}")
            return error_response
            
        except openai.BadRequestError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.INVALID_REQUEST,
                error_message=f"OpenAIè¯·æ±‚æ— æ•ˆ: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ OpenAIè¯·æ±‚æ— æ•ˆ: {e}")
            return error_response
            
        except openai.InternalServerError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.SERVER_ERROR,
                error_message=f"OpenAIæœåŠ¡å™¨é”™è¯¯: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ OpenAIæœåŠ¡å™¨é”™è¯¯: {e}")
            return error_response
            
        except Exception as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.UNKNOWN_ERROR,
                error_message=f"OpenAIæœªçŸ¥é”™è¯¯: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ OpenAIæœªçŸ¥é”™è¯¯: {e}")
            return error_response
    
    def validate_config(self) -> bool:
        """
        éªŒè¯OpenAIé…ç½®æ˜¯å¦æœ‰æ•ˆ
        
        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # ç®€å•æµ‹è¯•APIè¿é€šæ€§
            response = self.chat_completion(
                messages="Hello",
                max_tokens=5
            )
            return response.success
            
        except Exception as e:
            logger.error(f"âŒ OpenAIé…ç½®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Returns:
            List[str]: å¯ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
        """
        try:
            models_response = self.client.models.list()
            return [model.id for model in models_response.data]
            
        except Exception as e:
            logger.error(f"âŒ è·å–OpenAIæ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            # è¿”å›å¸¸è§çš„æ¨¡å‹
            if self.config.provider == LLMProvider.AZURE_OPENAI:
                return ["gpt-35-turbo", "gpt-4", "gpt-4-32k"]
            else:
                return [
                    "gpt-3.5-turbo", 
                    "gpt-3.5-turbo-16k",
                    "gpt-4", 
                    "gpt-4-turbo-preview",
                    "gpt-4o",
                    "gpt-4o-mini"
                ]
    
    def get_supported_features(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨
        
        Returns:
            List[str]: æ”¯æŒçš„åŠŸèƒ½
        """
        features = [
            "chat_completion", 
            "text_generation",
            "function_calling",
            "streaming",
            "vision",  # éƒ¨åˆ†æ¨¡å‹æ”¯æŒ
            "json_mode",
            "system_messages"
        ]
        
        if self.config.provider == LLMProvider.AZURE_OPENAI:
            features.append("azure_integration")
            
        return features


def create_openai_client(api_key: str, model_name: str = "gpt-3.5-turbo", **kwargs) -> OpenAIClient:
    """
    åˆ›å»ºOpenAIå®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        api_key: APIå¯†é’¥
        model_name: æ¨¡å‹åç§°
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        OpenAIClient: å®¢æˆ·ç«¯å®ä¾‹
    """
    config = LLMConfig(
        provider=LLMProvider.OPENAI,
        api_key=api_key,
        model_name=model_name,
        **kwargs
    )
    return OpenAIClient(config)


def create_azure_openai_client(api_key: str, azure_endpoint: str, 
                              model_name: str = "gpt-35-turbo", **kwargs) -> OpenAIClient:
    """
    åˆ›å»ºAzure OpenAIå®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        api_key: APIå¯†é’¥
        azure_endpoint: Azureç«¯ç‚¹
        model_name: æ¨¡å‹åç§°
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        OpenAIClient: å®¢æˆ·ç«¯å®ä¾‹
    """
    config = LLMConfig(
        provider=LLMProvider.AZURE_OPENAI,
        api_key=api_key,
        model_name=model_name,
        base_url=azure_endpoint,
        extra_params={"azure_endpoint": azure_endpoint},
        **kwargs
    )
    return OpenAIClient(config)