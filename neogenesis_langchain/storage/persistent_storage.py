#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Neogenesis System - Persistent Storage Engine
ä¼ä¸šçº§æŒä¹…åŒ–å­˜å‚¨å¼•æ“ï¼šæ”¯æŒå¤šç§å­˜å‚¨åç«¯å’Œé«˜çº§ç‰¹æ€§
"""

import json
import pickle
import gzip
import hashlib
import logging
import time
import threading
import shutil
import sqlite3
import os
from typing import Any, Dict, List, Optional, Union, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from abc import ABC, abstractmethod

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

try:
    import lmdb
    LMDB_AVAILABLE = True
except ImportError:
    LMDB_AVAILABLE = False

logger = logging.getLogger(__name__)

# =============================================================================
# å­˜å‚¨é…ç½®å’Œæšä¸¾
# =============================================================================

class StorageBackend(Enum):
    """å­˜å‚¨åç«¯ç±»å‹"""
    FILE_SYSTEM = "file_system"
    SQLITE = "sqlite"
    REDIS = "redis"
    LMDB = "lmdb"
    MEMORY = "memory"

class CompressionType(Enum):
    """å‹ç¼©ç±»å‹"""
    NONE = "none"
    GZIP = "gzip"
    LZMA = "lzma"

class SerializationType(Enum):
    """åºåˆ—åŒ–ç±»å‹"""
    JSON = "json"
    PICKLE = "pickle"
    MSGPACK = "msgpack"

@dataclass
class StorageConfig:
    """å­˜å‚¨é…ç½®"""
    backend: StorageBackend = StorageBackend.FILE_SYSTEM
    storage_path: str = "./neogenesis_storage"
    compression: CompressionType = CompressionType.GZIP
    serialization: SerializationType = SerializationType.PICKLE
    enable_encryption: bool = False
    enable_versioning: bool = True
    enable_backup: bool = True
    backup_interval: int = 3600  # å¤‡ä»½é—´éš”ï¼ˆç§’ï¼‰
    max_versions: int = 10
    cache_size: int = 1000
    auto_sync: bool = True
    sync_interval: float = 5.0
    compression_level: int = 6

@dataclass
class StorageMetadata:
    """å­˜å‚¨å…ƒæ•°æ®"""
    key: str
    size: int
    created_at: float
    updated_at: float
    version: int
    checksum: str
    compressed: bool = False
    encrypted: bool = False
    access_count: int = 0
    last_accessed: float = field(default_factory=time.time)

# =============================================================================
# æŠ½è±¡å­˜å‚¨æ¥å£
# =============================================================================

class BaseStorageBackend(ABC):
    """å­˜å‚¨åç«¯æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: StorageConfig):
        self.config = config
        self.metadata_cache = {}
        self._lock = threading.RLock()
    
    @abstractmethod
    def store(self, key: str, data: Any) -> bool:
        """å­˜å‚¨æ•°æ®"""
        pass
    
    @abstractmethod
    def retrieve(self, key: str) -> Optional[Any]:
        """æ£€ç´¢æ•°æ®"""
        pass
    
    @abstractmethod
    def delete(self, key: str) -> bool:
        """åˆ é™¤æ•°æ®"""
        pass
    
    @abstractmethod
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"""
        pass
    
    @abstractmethod
    def list_keys(self, prefix: str = "") -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é”®"""
        pass
    
    @abstractmethod
    def get_metadata(self, key: str) -> Optional[StorageMetadata]:
        """è·å–å…ƒæ•°æ®"""
        pass
    
    @abstractmethod
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        pass

# =============================================================================
# æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨åç«¯
# =============================================================================

class FileSystemBackend(BaseStorageBackend):
    """æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨åç«¯"""
    
    def __init__(self, config: StorageConfig):
        super().__init__(config)
        self.storage_path = Path(config.storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.data_dir = self.storage_path / "data"
        self.metadata_dir = self.storage_path / "metadata"
        self.versions_dir = self.storage_path / "versions"
        self.backup_dir = self.storage_path / "backup"
        
        for dir_path in [self.data_dir, self.metadata_dir, self.versions_dir, self.backup_dir]:
            dir_path.mkdir(exist_ok=True)
        
        logger.info(f"ğŸ“ æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨åç«¯åˆå§‹åŒ–: {self.storage_path}")
    
    def _get_file_path(self, key: str, directory: Path = None) -> Path:
        """è·å–æ–‡ä»¶è·¯å¾„"""
        if directory is None:
            directory = self.data_dir
        
        # å®‰å…¨çš„é”®åå¤„ç†
        safe_key = hashlib.md5(key.encode()).hexdigest()
        return directory / f"{safe_key}.dat"
    
    def _serialize_data(self, data: Any) -> bytes:
        """åºåˆ—åŒ–æ•°æ®"""
        if self.config.serialization == SerializationType.JSON:
            serialized = json.dumps(data, ensure_ascii=False).encode('utf-8')
        elif self.config.serialization == SerializationType.PICKLE:
            serialized = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
        else:
            # é»˜è®¤ä½¿ç”¨pickle
            serialized = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
        
        # å‹ç¼©
        if self.config.compression == CompressionType.GZIP:
            serialized = gzip.compress(serialized, compresslevel=self.config.compression_level)
        
        return serialized
    
    def _deserialize_data(self, data: bytes) -> Any:
        """ååºåˆ—åŒ–æ•°æ®"""
        # è§£å‹ç¼©
        if self.config.compression == CompressionType.GZIP:
            try:
                data = gzip.decompress(data)
            except gzip.BadGzipFile:
                # å¯èƒ½æ˜¯æœªå‹ç¼©çš„æ—§æ•°æ®
                pass
        
        # ååºåˆ—åŒ–
        if self.config.serialization == SerializationType.JSON:
            return json.loads(data.decode('utf-8'))
        elif self.config.serialization == SerializationType.PICKLE:
            return pickle.loads(data)
        else:
            return pickle.loads(data)
    
    def _calculate_checksum(self, data: bytes) -> str:
        """è®¡ç®—æ ¡éªŒå’Œ"""
        return hashlib.sha256(data).hexdigest()
    
    def store(self, key: str, data: Any) -> bool:
        """å­˜å‚¨æ•°æ®"""
        try:
            with self._lock:
                # åºåˆ—åŒ–æ•°æ®
                serialized_data = self._serialize_data(data)
                
                # è·å–æ–‡ä»¶è·¯å¾„
                file_path = self._get_file_path(key)
                
                # ç‰ˆæœ¬æ§åˆ¶
                if self.config.enable_versioning and file_path.exists():
                    self._backup_version(key)
                
                # å†™å…¥æ•°æ®
                with open(file_path, 'wb') as f:
                    f.write(serialized_data)
                
                # åˆ›å»ºå…ƒæ•°æ®
                metadata = StorageMetadata(
                    key=key,
                    size=len(serialized_data),
                    created_at=time.time(),
                    updated_at=time.time(),
                    version=self._get_next_version(key),
                    checksum=self._calculate_checksum(serialized_data),
                    compressed=self.config.compression != CompressionType.NONE,
                    encrypted=self.config.enable_encryption
                )
                
                # ä¿å­˜å…ƒæ•°æ®
                self._save_metadata(key, metadata)
                
                logger.debug(f"âœ… å­˜å‚¨æˆåŠŸ: {key} ({len(serialized_data)} bytes)")
                return True
                
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨å¤±è´¥: {key} - {e}")
            return False
    
    def retrieve(self, key: str) -> Optional[Any]:
        """æ£€ç´¢æ•°æ®"""
        try:
            with self._lock:
                file_path = self._get_file_path(key)
                
                if not file_path.exists():
                    return None
                
                # è¯»å–æ•°æ®
                with open(file_path, 'rb') as f:
                    serialized_data = f.read()
                
                # éªŒè¯æ ¡éªŒå’Œ
                metadata = self.get_metadata(key)
                if metadata:
                    current_checksum = self._calculate_checksum(serialized_data)
                    if current_checksum != metadata.checksum:
                        logger.warning(f"âš ï¸ æ ¡éªŒå’Œä¸åŒ¹é…: {key}")
                
                # ååºåˆ—åŒ–
                data = self._deserialize_data(serialized_data)
                
                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                if metadata:
                    metadata.access_count += 1
                    metadata.last_accessed = time.time()
                    self._save_metadata(key, metadata)
                
                logger.debug(f"âœ… æ£€ç´¢æˆåŠŸ: {key}")
                return data
                
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢å¤±è´¥: {key} - {e}")
            return None
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤æ•°æ®"""
        try:
            with self._lock:
                file_path = self._get_file_path(key)
                metadata_path = self._get_file_path(key, self.metadata_dir)
                
                # åˆ é™¤æ•°æ®æ–‡ä»¶
                if file_path.exists():
                    file_path.unlink()
                
                # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
                if metadata_path.exists():
                    metadata_path.unlink()
                
                # æ¸…ç†ç‰ˆæœ¬æ–‡ä»¶
                if self.config.enable_versioning:
                    self._cleanup_versions(key)
                
                logger.debug(f"âœ… åˆ é™¤æˆåŠŸ: {key}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤å¤±è´¥: {key} - {e}")
            return False
    
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"""
        file_path = self._get_file_path(key)
        return file_path.exists()
    
    def list_keys(self, prefix: str = "") -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é”®"""
        keys = []
        try:
            for metadata_file in self.metadata_dir.glob("*.dat"):
                try:
                    metadata = self._load_metadata_from_file(metadata_file)
                    if metadata and metadata.key.startswith(prefix):
                        keys.append(metadata.key)
                except:
                    continue
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºé”®å¤±è´¥: {e}")
        
        return keys
    
    def get_metadata(self, key: str) -> Optional[StorageMetadata]:
        """è·å–å…ƒæ•°æ®"""
        metadata_path = self._get_file_path(key, self.metadata_dir)
        
        if not metadata_path.exists():
            return None
        
        return self._load_metadata_from_file(metadata_path)
    
    def _save_metadata(self, key: str, metadata: StorageMetadata):
        """ä¿å­˜å…ƒæ•°æ®"""
        metadata_path = self._get_file_path(key, self.metadata_dir)
        
        try:
            with open(metadata_path, 'wb') as f:
                pickle.dump(metadata, f)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {key} - {e}")
    
    def _load_metadata_from_file(self, metadata_path: Path) -> Optional[StorageMetadata]:
        """ä»æ–‡ä»¶åŠ è½½å…ƒæ•°æ®"""
        try:
            with open(metadata_path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å…ƒæ•°æ®å¤±è´¥: {metadata_path} - {e}")
            return None
    
    def _get_next_version(self, key: str) -> int:
        """è·å–ä¸‹ä¸€ä¸ªç‰ˆæœ¬å·"""
        metadata = self.get_metadata(key)
        return (metadata.version + 1) if metadata else 1
    
    def _backup_version(self, key: str):
        """å¤‡ä»½ç‰ˆæœ¬"""
        if not self.config.enable_versioning:
            return
        
        try:
            file_path = self._get_file_path(key)
            if not file_path.exists():
                return
            
            metadata = self.get_metadata(key)
            if not metadata:
                return
            
            # åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶å
            version_filename = f"{hashlib.md5(key.encode()).hexdigest()}_v{metadata.version}.dat"
            version_path = self.versions_dir / version_filename
            
            # å¤åˆ¶å½“å‰æ–‡ä»¶åˆ°ç‰ˆæœ¬ç›®å½•
            shutil.copy2(file_path, version_path)
            
            # æ¸…ç†æ—§ç‰ˆæœ¬
            self._cleanup_old_versions(key)
            
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½ç‰ˆæœ¬å¤±è´¥: {key} - {e}")
    
    def _cleanup_old_versions(self, key: str):
        """æ¸…ç†æ—§ç‰ˆæœ¬"""
        if not self.config.enable_versioning:
            return
        
        try:
            key_hash = hashlib.md5(key.encode()).hexdigest()
            version_files = list(self.versions_dir.glob(f"{key_hash}_v*.dat"))
            
            # æŒ‰ç‰ˆæœ¬å·æ’åº
            version_files.sort(key=lambda x: int(x.stem.split('_v')[1]))
            
            # åˆ é™¤è¶…å‡ºé™åˆ¶çš„ç‰ˆæœ¬
            while len(version_files) > self.config.max_versions:
                old_version = version_files.pop(0)
                old_version.unlink()
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§ç‰ˆæœ¬å¤±è´¥: {key} - {e}")
    
    def _cleanup_versions(self, key: str):
        """æ¸…ç†æ‰€æœ‰ç‰ˆæœ¬"""
        try:
            key_hash = hashlib.md5(key.encode()).hexdigest()
            version_files = list(self.versions_dir.glob(f"{key_hash}_v*.dat"))
            
            for version_file in version_files:
                version_file.unlink()
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†ç‰ˆæœ¬å¤±è´¥: {key} - {e}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ“ æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨åç«¯æ¸…ç†å®Œæˆ")

# =============================================================================
# SQLiteå­˜å‚¨åç«¯
# =============================================================================

class SQLiteBackend(BaseStorageBackend):
    """SQLiteå­˜å‚¨åç«¯"""
    
    def __init__(self, config: StorageConfig):
        super().__init__(config)
        self.db_path = Path(config.storage_path) / "neogenesis.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_database()
        logger.info(f"ğŸ—„ï¸ SQLiteå­˜å‚¨åç«¯åˆå§‹åŒ–: {self.db_path}")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS storage_data (
                    key TEXT PRIMARY KEY,
                    data BLOB NOT NULL,
                    size INTEGER NOT NULL,
                    created_at REAL NOT NULL,
                    updated_at REAL NOT NULL,
                    version INTEGER NOT NULL DEFAULT 1,
                    checksum TEXT NOT NULL,
                    compressed BOOLEAN DEFAULT FALSE,
                    encrypted BOOLEAN DEFAULT FALSE,
                    access_count INTEGER DEFAULT 0,
                    last_accessed REAL NOT NULL
                )
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_storage_data_updated_at 
                ON storage_data(updated_at)
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_storage_data_last_accessed 
                ON storage_data(last_accessed)
            """)
            
            conn.commit()
    
    def _serialize_data(self, data: Any) -> bytes:
        """åºåˆ—åŒ–æ•°æ®"""
        if self.config.serialization == SerializationType.JSON:
            serialized = json.dumps(data, ensure_ascii=False).encode('utf-8')
        else:
            serialized = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
        
        if self.config.compression == CompressionType.GZIP:
            serialized = gzip.compress(serialized, compresslevel=self.config.compression_level)
        
        return serialized
    
    def _deserialize_data(self, data: bytes) -> Any:
        """ååºåˆ—åŒ–æ•°æ®"""
        if self.config.compression == CompressionType.GZIP:
            try:
                data = gzip.decompress(data)
            except gzip.BadGzipFile:
                pass
        
        if self.config.serialization == SerializationType.JSON:
            return json.loads(data.decode('utf-8'))
        else:
            return pickle.loads(data)
    
    def _calculate_checksum(self, data: bytes) -> str:
        """è®¡ç®—æ ¡éªŒå’Œ"""
        return hashlib.sha256(data).hexdigest()
    
    def store(self, key: str, data: Any) -> bool:
        """å­˜å‚¨æ•°æ®"""
        try:
            with self._lock:
                serialized_data = self._serialize_data(data)
                checksum = self._calculate_checksum(serialized_data)
                current_time = time.time()
                
                with sqlite3.connect(self.db_path) as conn:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    cursor = conn.execute("SELECT version FROM storage_data WHERE key = ?", (key,))
                    existing = cursor.fetchone()
                    version = (existing[0] + 1) if existing else 1
                    
                    # æ’å…¥æˆ–æ›´æ–°
                    conn.execute("""
                        INSERT OR REPLACE INTO storage_data 
                        (key, data, size, created_at, updated_at, version, checksum, 
                         compressed, encrypted, access_count, last_accessed)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 0, ?)
                    """, (
                        key, serialized_data, len(serialized_data),
                        current_time if not existing else None,
                        current_time, version, checksum,
                        self.config.compression != CompressionType.NONE,
                        self.config.enable_encryption, current_time
                    ))
                    
                    conn.commit()
                
                logger.debug(f"âœ… SQLiteå­˜å‚¨æˆåŠŸ: {key}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ SQLiteå­˜å‚¨å¤±è´¥: {key} - {e}")
            return False
    
    def retrieve(self, key: str) -> Optional[Any]:
        """æ£€ç´¢æ•°æ®"""
        try:
            with self._lock:
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.execute("""
                        SELECT data, checksum FROM storage_data WHERE key = ?
                    """, (key,))
                    
                    result = cursor.fetchone()
                    if not result:
                        return None
                    
                    data_blob, stored_checksum = result
                    
                    # éªŒè¯æ ¡éªŒå’Œ
                    current_checksum = self._calculate_checksum(data_blob)
                    if current_checksum != stored_checksum:
                        logger.warning(f"âš ï¸ SQLiteæ ¡éªŒå’Œä¸åŒ¹é…: {key}")
                    
                    # æ›´æ–°è®¿é—®ç»Ÿè®¡
                    conn.execute("""
                        UPDATE storage_data 
                        SET access_count = access_count + 1, last_accessed = ?
                        WHERE key = ?
                    """, (time.time(), key))
                    conn.commit()
                    
                    # ååºåˆ—åŒ–
                    data = self._deserialize_data(data_blob)
                    
                    logger.debug(f"âœ… SQLiteæ£€ç´¢æˆåŠŸ: {key}")
                    return data
                    
        except Exception as e:
            logger.error(f"âŒ SQLiteæ£€ç´¢å¤±è´¥: {key} - {e}")
            return None
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤æ•°æ®"""
        try:
            with self._lock:
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute("DELETE FROM storage_data WHERE key = ?", (key,))
                    conn.commit()
                
                logger.debug(f"âœ… SQLiteåˆ é™¤æˆåŠŸ: {key}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ SQLiteåˆ é™¤å¤±è´¥: {key} - {e}")
            return False
    
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("SELECT 1 FROM storage_data WHERE key = ? LIMIT 1", (key,))
                return cursor.fetchone() is not None
        except:
            return False
    
    def list_keys(self, prefix: str = "") -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é”®"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                if prefix:
                    cursor = conn.execute("SELECT key FROM storage_data WHERE key LIKE ?", (f"{prefix}%",))
                else:
                    cursor = conn.execute("SELECT key FROM storage_data")
                
                return [row[0] for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"âŒ SQLiteåˆ—å‡ºé”®å¤±è´¥: {e}")
            return []
    
    def get_metadata(self, key: str) -> Optional[StorageMetadata]:
        """è·å–å…ƒæ•°æ®"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT size, created_at, updated_at, version, checksum, 
                           compressed, encrypted, access_count, last_accessed
                    FROM storage_data WHERE key = ?
                """, (key,))
                
                result = cursor.fetchone()
                if not result:
                    return None
                
                return StorageMetadata(
                    key=key,
                    size=result[0],
                    created_at=result[1],
                    updated_at=result[2],
                    version=result[3],
                    checksum=result[4],
                    compressed=bool(result[5]),
                    encrypted=bool(result[6]),
                    access_count=result[7],
                    last_accessed=result[8]
                )
                
        except Exception as e:
            logger.error(f"âŒ SQLiteè·å–å…ƒæ•°æ®å¤±è´¥: {key} - {e}")
            return None
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ—„ï¸ SQLiteå­˜å‚¨åç«¯æ¸…ç†å®Œæˆ")

# =============================================================================
# LMDBå­˜å‚¨åç«¯
# =============================================================================

class LMDBBackend(BaseStorageBackend):
    """LMDBå­˜å‚¨åç«¯"""
    
    def __init__(self, config: StorageConfig):
        super().__init__(config)
        
        if not LMDB_AVAILABLE:
            raise ImportError("LMDB æ¨¡å—æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install lmdb")
        
        self.storage_path = Path(config.storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # LMDBç¯å¢ƒé…ç½®
        self.env_path = self.storage_path / "lmdb_data"
        self.env_path.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–LMDBç¯å¢ƒ
        self.env = lmdb.open(
            str(self.env_path),
            map_size=1024 * 1024 * 1024 * 10,  # 10GB max size
            subdir=True,
            readonly=False,
            create=True,
            max_dbs=2  # æ•°æ®åº“å’Œå…ƒæ•°æ®
        )
        
        # åˆ›å»ºæ•°æ®åº“
        with self.env.begin(write=True) as txn:
            self.data_db = self.env.open_db(b'data', txn=txn, create=True)
            self.metadata_db = self.env.open_db(b'metadata', txn=txn, create=True)
        
        logger.info(f"âš¡ LMDBå­˜å‚¨åç«¯åˆå§‹åŒ–: {self.env_path}")
    
    def _serialize_data(self, data: Any) -> bytes:
        """åºåˆ—åŒ–æ•°æ®"""
        if self.config.serialization == SerializationType.JSON:
            serialized = json.dumps(data, ensure_ascii=False).encode('utf-8')
        else:
            serialized = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
        
        if self.config.compression == CompressionType.GZIP:
            serialized = gzip.compress(serialized, compresslevel=self.config.compression_level)
        
        return serialized
    
    def _deserialize_data(self, data: bytes) -> Any:
        """ååºåˆ—åŒ–æ•°æ®"""
        if self.config.compression == CompressionType.GZIP:
            try:
                data = gzip.decompress(data)
            except gzip.BadGzipFile:
                pass
        
        if self.config.serialization == SerializationType.JSON:
            return json.loads(data.decode('utf-8'))
        else:
            return pickle.loads(data)
    
    def _calculate_checksum(self, data: bytes) -> str:
        """è®¡ç®—æ ¡éªŒå’Œ"""
        return hashlib.sha256(data).hexdigest()
    
    def store(self, key: str, data: Any) -> bool:
        """å­˜å‚¨æ•°æ®"""
        try:
            with self._lock:
                serialized_data = self._serialize_data(data)
                checksum = self._calculate_checksum(serialized_data)
                current_time = time.time()
                key_bytes = key.encode('utf-8')
                
                with self.env.begin(write=True) as txn:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing_metadata = txn.get(key_bytes, db=self.metadata_db)
                    version = 1
                    created_at = current_time
                    
                    if existing_metadata:
                        try:
                            old_metadata = pickle.loads(existing_metadata)
                            version = old_metadata.version + 1
                            created_at = old_metadata.created_at
                        except:
                            pass
                    
                    # å­˜å‚¨æ•°æ®
                    txn.put(key_bytes, serialized_data, db=self.data_db)
                    
                    # åˆ›å»ºå¹¶å­˜å‚¨å…ƒæ•°æ®
                    metadata = StorageMetadata(
                        key=key,
                        size=len(serialized_data),
                        created_at=created_at,
                        updated_at=current_time,
                        version=version,
                        checksum=checksum,
                        compressed=self.config.compression != CompressionType.NONE,
                        encrypted=self.config.enable_encryption,
                        access_count=0,
                        last_accessed=current_time
                    )
                    
                    metadata_bytes = pickle.dumps(metadata)
                    txn.put(key_bytes, metadata_bytes, db=self.metadata_db)
                
                logger.debug(f"âœ… LMDBå­˜å‚¨æˆåŠŸ: {key}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ LMDBå­˜å‚¨å¤±è´¥: {key} - {e}")
            return False
    
    def retrieve(self, key: str) -> Optional[Any]:
        """æ£€ç´¢æ•°æ®"""
        try:
            with self._lock:
                key_bytes = key.encode('utf-8')
                
                with self.env.begin(write=True) as txn:
                    # è·å–æ•°æ®
                    data_bytes = txn.get(key_bytes, db=self.data_db)
                    if data_bytes is None:
                        return None
                    
                    # è·å–å¹¶æ›´æ–°å…ƒæ•°æ®
                    metadata_bytes = txn.get(key_bytes, db=self.metadata_db)
                    if metadata_bytes:
                        try:
                            metadata = pickle.loads(metadata_bytes)
                            
                            # éªŒè¯æ ¡éªŒå’Œ
                            current_checksum = self._calculate_checksum(data_bytes)
                            if current_checksum != metadata.checksum:
                                logger.warning(f"âš ï¸ LMDBæ ¡éªŒå’Œä¸åŒ¹é…: {key}")
                            
                            # æ›´æ–°è®¿é—®ç»Ÿè®¡
                            metadata.access_count += 1
                            metadata.last_accessed = time.time()
                            
                            # ä¿å­˜æ›´æ–°çš„å…ƒæ•°æ®
                            updated_metadata_bytes = pickle.dumps(metadata)
                            txn.put(key_bytes, updated_metadata_bytes, db=self.metadata_db)
                        except:
                            pass
                    
                    # ååºåˆ—åŒ–æ•°æ®
                    data = self._deserialize_data(data_bytes)
                    
                    logger.debug(f"âœ… LMDBæ£€ç´¢æˆåŠŸ: {key}")
                    return data
                    
        except Exception as e:
            logger.error(f"âŒ LMDBæ£€ç´¢å¤±è´¥: {key} - {e}")
            return None
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤æ•°æ®"""
        try:
            with self._lock:
                key_bytes = key.encode('utf-8')
                
                with self.env.begin(write=True) as txn:
                    # åˆ é™¤æ•°æ®å’Œå…ƒæ•°æ®
                    data_deleted = txn.delete(key_bytes, db=self.data_db)
                    metadata_deleted = txn.delete(key_bytes, db=self.metadata_db)
                    
                    if data_deleted or metadata_deleted:
                        logger.debug(f"âœ… LMDBåˆ é™¤æˆåŠŸ: {key}")
                        return True
                    else:
                        return False
                        
        except Exception as e:
            logger.error(f"âŒ LMDBåˆ é™¤å¤±è´¥: {key} - {e}")
            return False
    
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"""
        try:
            key_bytes = key.encode('utf-8')
            with self.env.begin() as txn:
                return txn.get(key_bytes, db=self.data_db) is not None
        except:
            return False
    
    def list_keys(self, prefix: str = "") -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é”®"""
        keys = []
        try:
            prefix_bytes = prefix.encode('utf-8') if prefix else b''
            
            with self.env.begin() as txn:
                cursor = txn.cursor(db=self.data_db)
                
                if prefix_bytes:
                    # ä»å‰ç¼€å¼€å§‹éå†
                    if cursor.set_range(prefix_bytes):
                        for key_bytes, _ in cursor:
                            if key_bytes.startswith(prefix_bytes):
                                keys.append(key_bytes.decode('utf-8'))
                            else:
                                break
                else:
                    # éå†æ‰€æœ‰é”®
                    for key_bytes, _ in cursor:
                        keys.append(key_bytes.decode('utf-8'))
                        
        except Exception as e:
            logger.error(f"âŒ LMDBåˆ—å‡ºé”®å¤±è´¥: {e}")
            
        return keys
    
    def get_metadata(self, key: str) -> Optional[StorageMetadata]:
        """è·å–å…ƒæ•°æ®"""
        try:
            key_bytes = key.encode('utf-8')
            
            with self.env.begin() as txn:
                metadata_bytes = txn.get(key_bytes, db=self.metadata_db)
                if metadata_bytes:
                    return pickle.loads(metadata_bytes)
                return None
                
        except Exception as e:
            logger.error(f"âŒ LMDBè·å–å…ƒæ•°æ®å¤±è´¥: {key} - {e}")
            return None
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if hasattr(self, 'env') and self.env:
                self.env.close()
                logger.info("âš¡ LMDBå­˜å‚¨åç«¯æ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ LMDBæ¸…ç†å¤±è´¥: {e}")

# =============================================================================
# å†…å­˜å­˜å‚¨åç«¯
# =============================================================================

class MemoryBackend(BaseStorageBackend):
    """å†…å­˜å­˜å‚¨åç«¯ï¼ˆç”¨äºæµ‹è¯•å’Œç¼“å­˜ï¼‰"""
    
    def __init__(self, config: StorageConfig):
        super().__init__(config)
        self.data_store = {}
        self.metadata_store = {}
        logger.info("ğŸ’¾ å†…å­˜å­˜å‚¨åç«¯åˆå§‹åŒ–")
    
    def store(self, key: str, data: Any) -> bool:
        """å­˜å‚¨æ•°æ®"""
        try:
            with self._lock:
                import copy
                
                # æ·±æ‹·è´ä»¥é¿å…å¼•ç”¨é—®é¢˜
                stored_data = copy.deepcopy(data)
                self.data_store[key] = stored_data
                
                # åˆ›å»ºå…ƒæ•°æ®
                metadata = StorageMetadata(
                    key=key,
                    size=len(str(data)),  # ç®€åŒ–çš„å¤§å°è®¡ç®—
                    created_at=time.time(),
                    updated_at=time.time(),
                    version=self.metadata_store.get(key, StorageMetadata(key, 0, 0, 0, 0, "")).version + 1,
                    checksum=hashlib.md5(str(data).encode()).hexdigest()
                )
                
                self.metadata_store[key] = metadata
                
                logger.debug(f"âœ… å†…å­˜å­˜å‚¨æˆåŠŸ: {key}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ å†…å­˜å­˜å‚¨å¤±è´¥: {key} - {e}")
            return False
    
    def retrieve(self, key: str) -> Optional[Any]:
        """æ£€ç´¢æ•°æ®"""
        try:
            with self._lock:
                if key not in self.data_store:
                    return None
                
                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                if key in self.metadata_store:
                    metadata = self.metadata_store[key]
                    metadata.access_count += 1
                    metadata.last_accessed = time.time()
                
                import copy
                return copy.deepcopy(self.data_store[key])
                
        except Exception as e:
            logger.error(f"âŒ å†…å­˜æ£€ç´¢å¤±è´¥: {key} - {e}")
            return None
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤æ•°æ®"""
        try:
            with self._lock:
                if key in self.data_store:
                    del self.data_store[key]
                
                if key in self.metadata_store:
                    del self.metadata_store[key]
                
                return True
                
        except Exception as e:
            logger.error(f"âŒ å†…å­˜åˆ é™¤å¤±è´¥: {key} - {e}")
            return False
    
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"""
        return key in self.data_store
    
    def list_keys(self, prefix: str = "") -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é”®"""
        if prefix:
            return [key for key in self.data_store.keys() if key.startswith(prefix)]
        return list(self.data_store.keys())
    
    def get_metadata(self, key: str) -> Optional[StorageMetadata]:
        """è·å–å…ƒæ•°æ®"""
        return self.metadata_store.get(key)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        with self._lock:
            self.data_store.clear()
            self.metadata_store.clear()
        logger.info("ğŸ’¾ å†…å­˜å­˜å‚¨åç«¯æ¸…ç†å®Œæˆ")

# =============================================================================
# å­˜å‚¨å¼•æ“å·¥å‚
# =============================================================================

class PersistentStorageEngine:
    """æŒä¹…åŒ–å­˜å‚¨å¼•æ“"""
    
    def __init__(self, config: StorageConfig = None):
        """
        åˆå§‹åŒ–å­˜å‚¨å¼•æ“
        
        Args:
            config: å­˜å‚¨é…ç½®
        """
        self.config = config or StorageConfig()
        self.backend = self._create_backend()
        
        logger.info(f"ğŸš€ æŒä¹…åŒ–å­˜å‚¨å¼•æ“åˆå§‹åŒ–: {self.config.backend.value}")
    
    def _create_backend(self) -> BaseStorageBackend:
        """åˆ›å»ºå­˜å‚¨åç«¯"""
        if self.config.backend == StorageBackend.FILE_SYSTEM:
            return FileSystemBackend(self.config)
        elif self.config.backend == StorageBackend.SQLITE:
            return SQLiteBackend(self.config)
        elif self.config.backend == StorageBackend.LMDB:
            return LMDBBackend(self.config)
        elif self.config.backend == StorageBackend.MEMORY:
            return MemoryBackend(self.config)
        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„å­˜å‚¨åç«¯: {self.config.backend}ï¼Œä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿ")
            return FileSystemBackend(self.config)
    
    def store(self, key: str, data: Any) -> bool:
        """å­˜å‚¨æ•°æ®"""
        return self.backend.store(key, data)
    
    def retrieve(self, key: str) -> Optional[Any]:
        """æ£€ç´¢æ•°æ®"""
        return self.backend.retrieve(key)
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤æ•°æ®"""
        return self.backend.delete(key)
    
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"""
        return self.backend.exists(key)
    
    def list_keys(self, prefix: str = "") -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é”®"""
        return self.backend.list_keys(prefix)
    
    def get_metadata(self, key: str) -> Optional[StorageMetadata]:
        """è·å–å…ƒæ•°æ®"""
        return self.backend.get_metadata(key)
    
    def get_storage_stats(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        keys = self.list_keys()
        total_size = 0
        total_access_count = 0
        
        for key in keys[:100]:  # é™åˆ¶ç»Ÿè®¡æ•°é‡
            metadata = self.get_metadata(key)
            if metadata:
                total_size += metadata.size
                total_access_count += metadata.access_count
        
        return {
            "backend_type": self.config.backend.value,
            "total_keys": len(keys),
            "total_size_bytes": total_size,
            "total_access_count": total_access_count,
            "compression_enabled": self.config.compression != CompressionType.NONE,
            "versioning_enabled": self.config.enable_versioning
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.backend.cleanup()

# =============================================================================
# å·¥å‚å‡½æ•°
# =============================================================================

def create_storage_engine(
    backend_type: str = "file_system",
    storage_path: str = "./neogenesis_storage",
    **kwargs
) -> PersistentStorageEngine:
    """
    åˆ›å»ºå­˜å‚¨å¼•æ“
    
    Args:
        backend_type: åç«¯ç±»å‹
        storage_path: å­˜å‚¨è·¯å¾„
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        å­˜å‚¨å¼•æ“å®ä¾‹
    """
    config = StorageConfig(
        backend=StorageBackend(backend_type),
        storage_path=storage_path,
        **kwargs
    )
    
    return PersistentStorageEngine(config)

# =============================================================================
# æµ‹è¯•å’Œæ¼”ç¤º
# =============================================================================

if __name__ == "__main__":
    # æµ‹è¯•æŒä¹…åŒ–å­˜å‚¨å¼•æ“
    print("ğŸ§ª æµ‹è¯•æŒä¹…åŒ–å­˜å‚¨å¼•æ“...")
    
    # æµ‹è¯•æ–‡ä»¶ç³»ç»Ÿåç«¯
    print("\nğŸ“ æµ‹è¯•æ–‡ä»¶ç³»ç»Ÿåç«¯:")
    fs_engine = create_storage_engine("file_system", "./test_storage")
    
    # å­˜å‚¨æµ‹è¯•æ•°æ®
    test_data = {"message": "Hello, Neogenesis!", "timestamp": time.time()}
    success = fs_engine.store("test_key", test_data)
    print(f"âœ… å­˜å‚¨{'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # æ£€ç´¢æµ‹è¯•æ•°æ®
    retrieved_data = fs_engine.retrieve("test_key")
    print(f"âœ… æ£€ç´¢{'æˆåŠŸ' if retrieved_data else 'å¤±è´¥'}: {retrieved_data}")
    
    # è·å–å…ƒæ•°æ®
    metadata = fs_engine.get_metadata("test_key")
    if metadata:
        print(f"âœ… å…ƒæ•°æ®: å¤§å°={metadata.size}, ç‰ˆæœ¬={metadata.version}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = fs_engine.get_storage_stats()
    print(f"âœ… å­˜å‚¨ç»Ÿè®¡: {stats}")
    
    # æµ‹è¯•SQLiteåç«¯
    print("\nğŸ—„ï¸ æµ‹è¯•SQLiteåç«¯:")
    sqlite_engine = create_storage_engine("sqlite", "./test_storage_sqlite")
    
    success = sqlite_engine.store("sqlite_test", {"data": "SQLiteæµ‹è¯•æ•°æ®"})
    print(f"âœ… SQLiteå­˜å‚¨{'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    sqlite_data = sqlite_engine.retrieve("sqlite_test")
    print(f"âœ… SQLiteæ£€ç´¢{'æˆåŠŸ' if sqlite_data else 'å¤±è´¥'}: {sqlite_data}")
    
    # æµ‹è¯•LMDBåç«¯
    print("\nâš¡ æµ‹è¯•LMDBåç«¯:")
    if LMDB_AVAILABLE:
        try:
            lmdb_engine = create_storage_engine("lmdb", "./test_storage_lmdb")
            
            success = lmdb_engine.store("lmdb_test", {"data": "LMDBæµ‹è¯•æ•°æ®", "performance": "é«˜æ€§èƒ½"})
            print(f"âœ… LMDBå­˜å‚¨{'æˆåŠŸ' if success else 'å¤±è´¥'}")
            
            lmdb_data = lmdb_engine.retrieve("lmdb_test")
            print(f"âœ… LMDBæ£€ç´¢{'æˆåŠŸ' if lmdb_data else 'å¤±è´¥'}: {lmdb_data}")
            
            # æµ‹è¯•æ‰¹é‡æ“ä½œ
            for i in range(5):
                lmdb_engine.store(f"batch_test_{i}", {"index": i, "value": f"æµ‹è¯•æ•°æ®{i}"})
            
            keys = lmdb_engine.list_keys("batch_test_")
            print(f"âœ… LMDBæ‰¹é‡æµ‹è¯•: æ‰¾åˆ° {len(keys)} ä¸ªé”®")
            
            lmdb_engine.cleanup()
        except ImportError:
            print("âŒ LMDBæ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡LMDBæµ‹è¯•")
    else:
        print("âŒ LMDBä¸å¯ç”¨ï¼Œè·³è¿‡LMDBæµ‹è¯•")
    
    # æ¸…ç†
    fs_engine.cleanup()
    sqlite_engine.cleanup()
    
    print("âœ… æŒä¹…åŒ–å­˜å‚¨å¼•æ“æµ‹è¯•å®Œæˆ")
