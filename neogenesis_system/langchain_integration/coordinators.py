#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Neogenesis System - Tool Coordinators
æ™ºèƒ½å·¥å…·åè°ƒå™¨ï¼šåè°ƒäº”é˜¶æ®µå†³ç­–æµç¨‹ä¸­çš„å·¥å…·æ‰§è¡Œ
"""

import asyncio
import json
import logging
import time
from typing import Any, Dict, List, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from .tools import (
    NeogenesisThinkingSeedTool,
    NeogenesisRAGSeedTool,
    NeogenesisPathGeneratorTool,
    NeogenesisMABDecisionTool,
    NeogenesisIdeaVerificationTool
)
from .state_management import NeogenesisStateManager, DecisionStage, DecisionState

logger = logging.getLogger(__name__)

# =============================================================================
# æ‰§è¡Œç­–ç•¥å’Œæ¨¡å¼
# =============================================================================

class ExecutionMode(Enum):
    """æ‰§è¡Œæ¨¡å¼æšä¸¾"""
    SEQUENTIAL = "sequential"          # é¡ºåºæ‰§è¡Œ
    PARALLEL = "parallel"             # å¹¶è¡Œæ‰§è¡Œ
    ADAPTIVE = "adaptive"             # è‡ªé€‚åº”æ‰§è¡Œ
    PIPELINE = "pipeline"             # æµæ°´çº¿æ‰§è¡Œ
    FALLBACK_CASCADE = "fallback_cascade"  # çº§è”å›é€€æ‰§è¡Œ

class ToolPriority(Enum):
    """å·¥å…·ä¼˜å…ˆçº§"""
    CRITICAL = "critical"     # å…³é”®å·¥å…·ï¼Œå¿…é¡»æˆåŠŸ
    HIGH = "high"            # é«˜ä¼˜å…ˆçº§
    MEDIUM = "medium"        # ä¸­ç­‰ä¼˜å…ˆçº§
    LOW = "low"              # ä½ä¼˜å…ˆçº§ï¼Œå¯é€‰
    OPTIONAL = "optional"    # å®Œå…¨å¯é€‰

@dataclass
class ExecutionContext:
    """æ‰§è¡Œä¸Šä¸‹æ–‡"""
    session_id: str
    user_query: str
    execution_mode: ExecutionMode = ExecutionMode.ADAPTIVE
    timeout: float = 60.0
    retry_count: int = 3
    enable_caching: bool = True
    enable_verification: bool = True
    max_parallel_tools: int = 3
    fallback_enabled: bool = True
    custom_config: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ToolExecutionPlan:
    """å·¥å…·æ‰§è¡Œè®¡åˆ’"""
    stage: DecisionStage
    tool_name: str
    tool_instance: Any
    priority: ToolPriority
    dependencies: List[str] = field(default_factory=list)
    timeout: float = 30.0
    retry_enabled: bool = True
    fallback_tool: Optional[str] = None
    execution_order: int = 0

@dataclass
class ExecutionResult:
    """æ‰§è¡Œç»“æœ"""
    tool_name: str
    stage: DecisionStage
    success: bool
    data: Any = None
    execution_time: float = 0.0
    error_message: Optional[str] = None
    retry_count: int = 0
    cache_hit: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)

# =============================================================================
# æ™ºèƒ½å·¥å…·åè°ƒå™¨
# =============================================================================

class NeogenesisToolCoordinator:
    """
    Neogenesisæ™ºèƒ½å·¥å…·åè°ƒå™¨
    
    åŠŸèƒ½ï¼š
    - åè°ƒäº”é˜¶æ®µå†³ç­–æµç¨‹ä¸­çš„å·¥å…·æ‰§è¡Œ
    - æ™ºèƒ½æ‰§è¡Œç­–ç•¥é€‰æ‹©
    - ä¾èµ–ç®¡ç†å’Œæ‰§è¡Œé¡ºåºä¼˜åŒ–
    - é”™è¯¯å¤„ç†å’Œè‡ªåŠ¨æ¢å¤
    - æ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜ç®¡ç†
    """
    
    def __init__(self,
                 api_key: str = "",
                 search_engine: str = "duckduckgo",
                 llm_client=None,
                 web_search_client=None,
                 state_manager: Optional[NeogenesisStateManager] = None,
                 max_workers: int = 4):
        """
        åˆå§‹åŒ–åè°ƒå™¨
        
        Args:
            api_key: APIå¯†é’¥
            search_engine: æœç´¢å¼•æ“ç±»å‹
            llm_client: LLMå®¢æˆ·ç«¯
            web_search_client: ç½‘ç»œæœç´¢å®¢æˆ·ç«¯
            state_manager: çŠ¶æ€ç®¡ç†å™¨
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.api_key = api_key
        self.search_engine = search_engine
        self.llm_client = llm_client
        self.web_search_client = web_search_client
        self.state_manager = state_manager
        self.max_workers = max_workers
        
        # åˆå§‹åŒ–å·¥å…·å®ä¾‹
        self._initialize_tools()
        
        # æ‰§è¡Œç»Ÿè®¡
        self.execution_stats = {
            "total_executions": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "cache_hits": 0,
            "total_execution_time": 0.0,
            "tool_performance": {},
            "error_patterns": {}
        }
        
        # ç¼“å­˜å’Œä¼˜åŒ–
        self.result_cache = {}
        self.execution_history = []
        self.performance_optimizer = PerformanceOptimizer()
        
        # çº¿ç¨‹æ± 
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        
        logger.info("ğŸ¯ NeogenesisToolCoordinator åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_tools(self):
        """åˆå§‹åŒ–æ‰€æœ‰å·¥å…·å®ä¾‹"""
        self.tools = {
            "thinking_seed": NeogenesisThinkingSeedTool(api_key=self.api_key),
            "rag_seed": NeogenesisRAGSeedTool(
                api_key=self.api_key,
                search_engine=self.search_engine,
                llm_client=self.llm_client,
                web_search_client=self.web_search_client
            ),
            "path_generator": NeogenesisPathGeneratorTool(
                api_key=self.api_key,
                llm_client=self.llm_client
            ),
            "mab_decision": NeogenesisMABDecisionTool(
                api_key=self.api_key,
                llm_client=self.llm_client
            ),
            "idea_verification": NeogenesisIdeaVerificationTool(
                search_engine=self.search_engine
            )
        }
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–äº† {len(self.tools)} ä¸ªå·¥å…·")
    
    def create_execution_plan(self, 
                            context: ExecutionContext,
                            stages: List[DecisionStage] = None) -> List[ToolExecutionPlan]:
        """
        åˆ›å»ºæ™ºèƒ½æ‰§è¡Œè®¡åˆ’
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            stages: è¦æ‰§è¡Œçš„é˜¶æ®µåˆ—è¡¨
            
        Returns:
            æ‰§è¡Œè®¡åˆ’åˆ—è¡¨
        """
        if stages is None:
            stages = [
                DecisionStage.THINKING_SEED,
                DecisionStage.SEED_VERIFICATION,
                DecisionStage.PATH_GENERATION,
                DecisionStage.PATH_VERIFICATION,
                DecisionStage.MAB_DECISION
            ]
        
        execution_plan = []
        
        for i, stage in enumerate(stages):
            if stage == DecisionStage.THINKING_SEED:
                # é˜¶æ®µä¸€ï¼šæ€ç»´ç§å­ç”Ÿæˆ
                plan = ToolExecutionPlan(
                    stage=stage,
                    tool_name="thinking_seed",
                    tool_instance=self.tools["thinking_seed"],
                    priority=ToolPriority.CRITICAL,
                    timeout=20.0,
                    execution_order=i * 10
                )
                execution_plan.append(plan)
                
            elif stage == DecisionStage.SEED_VERIFICATION:
                # é˜¶æ®µäºŒï¼šç§å­éªŒè¯ï¼ˆå¯é€‰ï¼‰
                if context.enable_verification:
                    plan = ToolExecutionPlan(
                        stage=stage,
                        tool_name="idea_verification",
                        tool_instance=self.tools["idea_verification"],
                        priority=ToolPriority.MEDIUM,
                        dependencies=["thinking_seed"],
                        timeout=15.0,
                        execution_order=i * 10
                    )
                    execution_plan.append(plan)
                    
            elif stage == DecisionStage.PATH_GENERATION:
                # é˜¶æ®µä¸‰ï¼šè·¯å¾„ç”Ÿæˆ
                plan = ToolExecutionPlan(
                    stage=stage,
                    tool_name="path_generator",
                    tool_instance=self.tools["path_generator"],
                    priority=ToolPriority.CRITICAL,
                    dependencies=["thinking_seed"],
                    timeout=25.0,
                    execution_order=i * 10
                )
                execution_plan.append(plan)
                
            elif stage == DecisionStage.PATH_VERIFICATION:
                # é˜¶æ®µå››ï¼šè·¯å¾„éªŒè¯ï¼ˆå¯é€‰ï¼‰
                if context.enable_verification:
                    plan = ToolExecutionPlan(
                        stage=stage,
                        tool_name="idea_verification",
                        tool_instance=self.tools["idea_verification"],
                        priority=ToolPriority.HIGH,
                        dependencies=["path_generator"],
                        timeout=20.0,
                        execution_order=i * 10
                    )
                    execution_plan.append(plan)
                    
            elif stage == DecisionStage.MAB_DECISION:
                # é˜¶æ®µäº”ï¼šMABå†³ç­–
                plan = ToolExecutionPlan(
                    stage=stage,
                    tool_name="mab_decision",
                    tool_instance=self.tools["mab_decision"],
                    priority=ToolPriority.CRITICAL,
                    dependencies=["path_generator"],
                    timeout=15.0,
                    execution_order=i * 10
                )
                execution_plan.append(plan)
        
        # æ ¹æ®æ‰§è¡Œæ¨¡å¼ä¼˜åŒ–è®¡åˆ’
        execution_plan = self._optimize_execution_plan(execution_plan, context)
        
        logger.info(f"ğŸ“‹ åˆ›å»ºæ‰§è¡Œè®¡åˆ’: {len(execution_plan)} ä¸ªå·¥å…·ï¼Œæ¨¡å¼={context.execution_mode.value}")
        return execution_plan
    
    def _optimize_execution_plan(self, 
                                plan: List[ToolExecutionPlan],
                                context: ExecutionContext) -> List[ToolExecutionPlan]:
        """
        ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’
        
        Args:
            plan: åŸå§‹æ‰§è¡Œè®¡åˆ’
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            ä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’
        """
        if context.execution_mode == ExecutionMode.PARALLEL:
            # å¹¶è¡Œæ¨¡å¼ï¼šæ ‡è¯†å¯å¹¶è¡Œæ‰§è¡Œçš„å·¥å…·
            for tool_plan in plan:
                if not tool_plan.dependencies:
                    tool_plan.execution_order = 0  # å¯ç«‹å³æ‰§è¡Œ
                else:
                    tool_plan.execution_order = len(tool_plan.dependencies) * 10
                    
        elif context.execution_mode == ExecutionMode.ADAPTIVE:
            # è‡ªé€‚åº”æ¨¡å¼ï¼šæ ¹æ®å†å²æ€§èƒ½è°ƒæ•´
            for tool_plan in plan:
                historical_performance = self.execution_stats.get("tool_performance", {}).get(
                    tool_plan.tool_name, {"avg_time": 10.0, "success_rate": 0.9}
                )
                
                # æ ¹æ®å†å²æ€§èƒ½è°ƒæ•´è¶…æ—¶æ—¶é—´
                tool_plan.timeout = max(
                    tool_plan.timeout,
                    historical_performance["avg_time"] * 1.5
                )
                
                # æ ¹æ®æˆåŠŸç‡è°ƒæ•´é‡è¯•ç­–ç•¥
                if historical_performance["success_rate"] < 0.8:
                    tool_plan.retry_enabled = True
                    tool_plan.fallback_tool = self._get_fallback_tool(tool_plan.tool_name)
        
        elif context.execution_mode == ExecutionMode.PIPELINE:
            # æµæ°´çº¿æ¨¡å¼ï¼šä¼˜åŒ–æ‰§è¡Œé¡ºåº
            plan.sort(key=lambda x: (len(x.dependencies), x.execution_order))
        
        return plan
    
    def _get_fallback_tool(self, tool_name: str) -> Optional[str]:
        """è·å–å·¥å…·çš„å›é€€é€‰é¡¹"""
        fallback_mapping = {
            "rag_seed": "thinking_seed",
            "idea_verification": None,  # éªŒè¯å·¥å…·æ— å›é€€ï¼Œå¯è·³è¿‡
            "mab_decision": None       # å†³ç­–å·¥å…·å…³é”®ï¼Œæ— å›é€€
        }
        return fallback_mapping.get(tool_name)
    
    async def execute_plan_async(self, 
                                plan: List[ToolExecutionPlan],
                                context: ExecutionContext) -> Dict[str, ExecutionResult]:
        """
        å¼‚æ­¥æ‰§è¡Œè®¡åˆ’
        
        Args:
            plan: æ‰§è¡Œè®¡åˆ’
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        results = {}
        execution_order_groups = {}
        
        # æŒ‰æ‰§è¡Œé¡ºåºåˆ†ç»„
        for tool_plan in plan:
            order = tool_plan.execution_order
            if order not in execution_order_groups:
                execution_order_groups[order] = []
            execution_order_groups[order].append(tool_plan)
        
        # æŒ‰é¡ºåºæ‰§è¡Œæ¯ç»„
        for order in sorted(execution_order_groups.keys()):
            group = execution_order_groups[order]
            
            if context.execution_mode == ExecutionMode.PARALLEL and len(group) > 1:
                # å¹¶è¡Œæ‰§è¡Œç»„å†…å·¥å…·
                group_results = await self._execute_group_parallel(group, context, results)
            else:
                # é¡ºåºæ‰§è¡Œç»„å†…å·¥å…·
                group_results = await self._execute_group_sequential(group, context, results)
            
            results.update(group_results)
            
            # æ£€æŸ¥å…³é”®å·¥å…·æ˜¯å¦æˆåŠŸ
            critical_failed = any(
                not result.success for tool_plan in group
                for result in [results.get(tool_plan.tool_name)]
                if tool_plan.priority == ToolPriority.CRITICAL and result
            )
            
            if critical_failed and not context.fallback_enabled:
                logger.error("âŒ å…³é”®å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œç»ˆæ­¢æ‰§è¡Œ")
                break
        
        return results
    
    async def _execute_group_parallel(self,
                                     group: List[ToolExecutionPlan],
                                     context: ExecutionContext,
                                     previous_results: Dict[str, ExecutionResult]) -> Dict[str, ExecutionResult]:
        """å¹¶è¡Œæ‰§è¡Œå·¥å…·ç»„"""
        tasks = []
        
        for tool_plan in group:
            if self._check_dependencies(tool_plan, previous_results):
                task = asyncio.create_task(
                    self._execute_single_tool_async(tool_plan, context, previous_results)
                )
                tasks.append((tool_plan.tool_name, task))
        
        results = {}
        for tool_name, task in tasks:
            try:
                result = await task
                results[tool_name] = result
            except Exception as e:
                logger.error(f"âŒ å¹¶è¡Œæ‰§è¡Œå·¥å…· {tool_name} å¤±è´¥: {e}")
                results[tool_name] = ExecutionResult(
                    tool_name=tool_name,
                    stage=DecisionStage.ERROR,
                    success=False,
                    error_message=str(e)
                )
        
        return results
    
    async def _execute_group_sequential(self,
                                       group: List[ToolExecutionPlan],
                                       context: ExecutionContext,
                                       previous_results: Dict[str, ExecutionResult]) -> Dict[str, ExecutionResult]:
        """é¡ºåºæ‰§è¡Œå·¥å…·ç»„"""
        results = {}
        
        for tool_plan in group:
            if self._check_dependencies(tool_plan, previous_results):
                result = await self._execute_single_tool_async(tool_plan, context, previous_results)
                results[tool_plan.tool_name] = result
                
                # å¦‚æœå…³é”®å·¥å…·å¤±è´¥ï¼Œè€ƒè™‘å›é€€
                if not result.success and tool_plan.priority == ToolPriority.CRITICAL:
                    fallback_result = await self._try_fallback(tool_plan, context, previous_results)
                    if fallback_result:
                        results[tool_plan.tool_name] = fallback_result
        
        return results
    
    async def _execute_single_tool_async(self,
                                        tool_plan: ToolExecutionPlan,
                                        context: ExecutionContext,
                                        previous_results: Dict[str, ExecutionResult]) -> ExecutionResult:
        """å¼‚æ­¥æ‰§è¡Œå•ä¸ªå·¥å…·"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(tool_plan, context, previous_results)
            if context.enable_caching and cache_key in self.result_cache:
                cached_result = self.result_cache[cache_key]
                cached_result.cache_hit = True
                logger.debug(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {tool_plan.tool_name}")
                return cached_result
            
            # å‡†å¤‡å·¥å…·è¾“å…¥
            tool_input = self._prepare_tool_input(tool_plan, context, previous_results)
            
            # æ‰§è¡Œå·¥å…·
            loop = asyncio.get_event_loop()
            result_data = await loop.run_in_executor(
                self.thread_pool,
                self._execute_tool_sync,
                tool_plan.tool_instance,
                tool_input
            )
            
            execution_time = time.time() - start_time
            
            # åˆ›å»ºæ‰§è¡Œç»“æœ
            result = ExecutionResult(
                tool_name=tool_plan.tool_name,
                stage=tool_plan.stage,
                success=True,
                data=result_data,
                execution_time=execution_time,
                metadata={
                    "input": tool_input,
                    "timeout": tool_plan.timeout,
                    "priority": tool_plan.priority.value
                }
            )
            
            # ç¼“å­˜ç»“æœ
            if context.enable_caching:
                self.result_cache[cache_key] = result
            
            # æ›´æ–°çŠ¶æ€ç®¡ç†å™¨
            if self.state_manager:
                self.state_manager.update_session_stage(
                    session_id=context.session_id,
                    stage=tool_plan.stage,
                    success=True,
                    data={"tool_result": result_data},
                    execution_time=execution_time
                )
            
            self._update_performance_stats(tool_plan.tool_name, execution_time, True)
            
            logger.info(f"âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ: {tool_plan.tool_name} ({execution_time:.2f}s)")
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_plan.tool_name} - {error_msg}")
            
            result = ExecutionResult(
                tool_name=tool_plan.tool_name,
                stage=tool_plan.stage,
                success=False,
                execution_time=execution_time,
                error_message=error_msg
            )
            
            # æ›´æ–°çŠ¶æ€ç®¡ç†å™¨
            if self.state_manager:
                self.state_manager.update_session_stage(
                    session_id=context.session_id,
                    stage=tool_plan.stage,
                    success=False,
                    data={"error": error_msg},
                    execution_time=execution_time,
                    error_message=error_msg
                )
            
            self._update_performance_stats(tool_plan.tool_name, execution_time, False)
            
            return result
    
    def _execute_tool_sync(self, tool_instance, tool_input: Dict[str, Any]) -> Any:
        """åŒæ­¥æ‰§è¡Œå·¥å…·ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
        return tool_instance.run(**tool_input)
    
    def _check_dependencies(self, 
                          tool_plan: ToolExecutionPlan,
                          previous_results: Dict[str, ExecutionResult]) -> bool:
        """æ£€æŸ¥å·¥å…·ä¾èµ–æ˜¯å¦æ»¡è¶³"""
        for dependency in tool_plan.dependencies:
            if dependency not in previous_results:
                logger.warning(f"âš ï¸ ä¾èµ–æœªæ»¡è¶³: {tool_plan.tool_name} éœ€è¦ {dependency}")
                return False
            
            if not previous_results[dependency].success:
                logger.warning(f"âš ï¸ ä¾èµ–å¤±è´¥: {dependency} æ‰§è¡Œå¤±è´¥ï¼Œå½±å“ {tool_plan.tool_name}")
                if tool_plan.priority == ToolPriority.CRITICAL:
                    return False
        
        return True
    
    def _prepare_tool_input(self,
                          tool_plan: ToolExecutionPlan,
                          context: ExecutionContext,
                          previous_results: Dict[str, ExecutionResult]) -> Dict[str, Any]:
        """å‡†å¤‡å·¥å…·è¾“å…¥å‚æ•°"""
        tool_input = {
            "execution_context": context.custom_config
        }
        
        if tool_plan.tool_name == "thinking_seed":
            tool_input.update({
                "user_query": context.user_query,
                "execution_context": context.custom_config
            })
            
        elif tool_plan.tool_name == "rag_seed":
            tool_input.update({
                "user_query": context.user_query,
                "execution_context": context.custom_config
            })
            
        elif tool_plan.tool_name == "path_generator":
            # éœ€è¦æ€ç»´ç§å­
            seed_result = previous_results.get("thinking_seed") or previous_results.get("rag_seed")
            if seed_result and seed_result.success:
                try:
                    seed_data = json.loads(seed_result.data) if isinstance(seed_result.data, str) else seed_result.data
                    thinking_seed = seed_data.get("thinking_seed") or seed_data.get("rag_enhanced_seed", "")
                except:
                    thinking_seed = str(seed_result.data)
                
                tool_input.update({
                    "thinking_seed": thinking_seed,
                    "task": context.user_query,
                    "max_paths": context.custom_config.get("max_paths", 4)
                })
            
        elif tool_plan.tool_name == "mab_decision":
            # éœ€è¦æ€ç»´è·¯å¾„
            path_result = previous_results.get("path_generator")
            if path_result and path_result.success:
                try:
                    path_data = json.loads(path_result.data) if isinstance(path_result.data, str) else path_result.data
                    reasoning_paths = path_data.get("reasoning_paths", [])
                except:
                    reasoning_paths = []
                
                tool_input.update({
                    "reasoning_paths": reasoning_paths,
                    "user_query": context.user_query,
                    "execution_context": context.custom_config
                })
                
        elif tool_plan.tool_name == "idea_verification":
            # æ ¹æ®é˜¶æ®µç¡®å®šéªŒè¯å†…å®¹
            if tool_plan.stage == DecisionStage.SEED_VERIFICATION:
                seed_result = previous_results.get("thinking_seed") or previous_results.get("rag_seed")
                if seed_result and seed_result.success:
                    try:
                        seed_data = json.loads(seed_result.data) if isinstance(seed_result.data, str) else seed_result.data
                        idea_text = seed_data.get("thinking_seed") or seed_data.get("rag_enhanced_seed", "")
                    except:
                        idea_text = str(seed_result.data)
                    
                    tool_input.update({
                        "idea_text": idea_text,
                        "context": {"stage": "thinking_seed", "query": context.user_query}
                    })
                    
            elif tool_plan.stage == DecisionStage.PATH_VERIFICATION:
                path_result = previous_results.get("path_generator")
                if path_result and path_result.success:
                    try:
                        path_data = json.loads(path_result.data) if isinstance(path_result.data, str) else path_result.data
                        reasoning_paths = path_data.get("reasoning_paths", [])
                        if reasoning_paths:
                            # éªŒè¯ç¬¬ä¸€ä¸ªè·¯å¾„ä½œä¸ºç¤ºä¾‹
                            first_path = reasoning_paths[0]
                            idea_text = f"{first_path.get('path_type', '')}: {first_path.get('description', '')}"
                            
                            tool_input.update({
                                "idea_text": idea_text,
                                "context": {"stage": "reasoning_path", "query": context.user_query}
                            })
                    except:
                        pass
        
        return tool_input
    
    async def _try_fallback(self,
                          failed_plan: ToolExecutionPlan,
                          context: ExecutionContext,
                          previous_results: Dict[str, ExecutionResult]) -> Optional[ExecutionResult]:
        """å°è¯•å›é€€ç­–ç•¥"""
        if not failed_plan.fallback_tool or not context.fallback_enabled:
            return None
        
        fallback_tool = self.tools.get(failed_plan.fallback_tool)
        if not fallback_tool:
            return None
        
        logger.info(f"ğŸ”„ å°è¯•å›é€€: {failed_plan.tool_name} -> {failed_plan.fallback_tool}")
        
        try:
            # åˆ›å»ºå›é€€æ‰§è¡Œè®¡åˆ’
            fallback_plan = ToolExecutionPlan(
                stage=failed_plan.stage,
                tool_name=failed_plan.fallback_tool,
                tool_instance=fallback_tool,
                priority=ToolPriority.HIGH,
                timeout=failed_plan.timeout * 0.8  # ç¼©çŸ­è¶…æ—¶æ—¶é—´
            )
            
            result = await self._execute_single_tool_async(fallback_plan, context, previous_results)
            
            if result.success:
                logger.info(f"âœ… å›é€€æˆåŠŸ: {failed_plan.fallback_tool}")
                result.metadata["fallback_from"] = failed_plan.tool_name
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ å›é€€å¤±è´¥: {e}")
            return None
    
    def _generate_cache_key(self,
                          tool_plan: ToolExecutionPlan,
                          context: ExecutionContext,
                          previous_results: Dict[str, ExecutionResult]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_components = [
            tool_plan.tool_name,
            context.user_query,
            str(context.custom_config),
            str([r.data for r in previous_results.values() if r.success])
        ]
        return f"cache_{hash('_'.join(key_components))}"
    
    def _update_performance_stats(self, tool_name: str, execution_time: float, success: bool):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.execution_stats["total_executions"] += 1
        
        if success:
            self.execution_stats["successful_executions"] += 1
        else:
            self.execution_stats["failed_executions"] += 1
        
        self.execution_stats["total_execution_time"] += execution_time
        
        # å·¥å…·çº§ç»Ÿè®¡
        if tool_name not in self.execution_stats["tool_performance"]:
            self.execution_stats["tool_performance"][tool_name] = {
                "executions": 0,
                "successes": 0,
                "total_time": 0.0,
                "avg_time": 0.0,
                "success_rate": 0.0
            }
        
        tool_stats = self.execution_stats["tool_performance"][tool_name]
        tool_stats["executions"] += 1
        tool_stats["total_time"] += execution_time
        
        if success:
            tool_stats["successes"] += 1
        
        tool_stats["avg_time"] = tool_stats["total_time"] / tool_stats["executions"]
        tool_stats["success_rate"] = tool_stats["successes"] / tool_stats["executions"]
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            "summary": self.execution_stats,
            "cache_efficiency": {
                "cache_size": len(self.result_cache),
                "cache_hit_rate": self.execution_stats.get("cache_hits", 0) / max(self.execution_stats["total_executions"], 1)
            },
            "tool_rankings": sorted(
                self.execution_stats["tool_performance"].items(),
                key=lambda x: x[1]["success_rate"],
                reverse=True
            )
        }

# =============================================================================
# æ€§èƒ½ä¼˜åŒ–å™¨
# =============================================================================

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.optimization_history = []
        self.performance_thresholds = {
            "max_execution_time": 60.0,
            "min_success_rate": 0.8,
            "max_cache_size": 1000
        }
    
    def analyze_performance(self, coordinator: NeogenesisToolCoordinator) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½å¹¶æä¾›ä¼˜åŒ–å»ºè®®"""
        report = coordinator.get_performance_report()
        
        recommendations = []
        
        # åˆ†ææ‰§è¡Œæ—¶é—´
        avg_time = coordinator.execution_stats["total_execution_time"] / max(
            coordinator.execution_stats["total_executions"], 1
        )
        
        if avg_time > self.performance_thresholds["max_execution_time"]:
            recommendations.append({
                "type": "execution_time",
                "message": f"å¹³å‡æ‰§è¡Œæ—¶é—´è¿‡é•¿: {avg_time:.2f}s",
                "suggestion": "è€ƒè™‘å¯ç”¨å¹¶è¡Œæ‰§è¡Œæˆ–å¢åŠ ç¼“å­˜"
            })
        
        # åˆ†ææˆåŠŸç‡
        success_rate = coordinator.execution_stats["successful_executions"] / max(
            coordinator.execution_stats["total_executions"], 1
        )
        
        if success_rate < self.performance_thresholds["min_success_rate"]:
            recommendations.append({
                "type": "success_rate",
                "message": f"æˆåŠŸç‡åä½: {success_rate:.1%}",
                "suggestion": "æ£€æŸ¥å·¥å…·é…ç½®æˆ–å¯ç”¨å›é€€æœºåˆ¶"
            })
        
        # åˆ†æç¼“å­˜æ•ˆç‡
        if len(coordinator.result_cache) > self.performance_thresholds["max_cache_size"]:
            recommendations.append({
                "type": "cache_size",
                "message": "ç¼“å­˜å¤§å°è¶…é™",
                "suggestion": "æ¸…ç†æ—§ç¼“å­˜æˆ–è°ƒæ•´ç¼“å­˜ç­–ç•¥"
            })
        
        return {
            "performance_metrics": report,
            "recommendations": recommendations,
            "optimization_score": self._calculate_optimization_score(report)
        }
    
    def _calculate_optimization_score(self, report: Dict[str, Any]) -> float:
        """è®¡ç®—ä¼˜åŒ–åˆ†æ•°ï¼ˆ0-100ï¼‰"""
        stats = report["summary"]
        
        # æˆåŠŸç‡æƒé‡40%
        success_rate = stats["successful_executions"] / max(stats["total_executions"], 1)
        success_score = success_rate * 40
        
        # å¹³å‡æ‰§è¡Œæ—¶é—´æƒé‡30%
        avg_time = stats["total_execution_time"] / max(stats["total_executions"], 1)
        time_score = max(0, (60 - avg_time) / 60) * 30
        
        # ç¼“å­˜å‘½ä¸­ç‡æƒé‡30%
        cache_hit_rate = report["cache_efficiency"]["cache_hit_rate"]
        cache_score = cache_hit_rate * 30
        
        return success_score + time_score + cache_score

if __name__ == "__main__":
    # æµ‹è¯•åè°ƒå™¨
    print("ğŸ§ª æµ‹è¯•Neogenesiså·¥å…·åè°ƒå™¨...")
    
    # åˆ›å»ºåè°ƒå™¨
    coordinator = NeogenesisToolCoordinator(api_key="test_key")
    
    # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
    context = ExecutionContext(
        session_id="test_session",
        user_query="è®¾è®¡ä¸€ä¸ªWebåº”ç”¨æ¶æ„",
        execution_mode=ExecutionMode.ADAPTIVE
    )
    
    # åˆ›å»ºæ‰§è¡Œè®¡åˆ’
    plan = coordinator.create_execution_plan(context)
    print(f"âœ… åˆ›å»ºæ‰§è¡Œè®¡åˆ’: {len(plan)} ä¸ªå·¥å…·")
    
    # æ¨¡æ‹Ÿå¼‚æ­¥æ‰§è¡Œ
    async def test_execution():
        results = await coordinator.execute_plan_async(plan, context)
        print(f"âœ… æ‰§è¡Œå®Œæˆ: {len(results)} ä¸ªç»“æœ")
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        report = coordinator.get_performance_report()
        print(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Š: {report}")
    
    # è¿è¡Œæµ‹è¯•
    try:
        asyncio.run(test_execution())
        print("âœ… åè°ƒå™¨æµ‹è¯•å®Œæˆ")
    except Exception as e:
        print(f"âŒ åè°ƒå™¨æµ‹è¯•å¤±è´¥: {e}")
