#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Neogenesis System - Execution Engines
æ™ºèƒ½æ‰§è¡Œå¼•æ“ï¼šé«˜çº§å·¥å…·æ‰§è¡Œç­–ç•¥å’Œä¼˜åŒ–
"""

import asyncio
import json
import logging
import time
from typing import Any, Dict, List, Optional, Tuple, Callable, Set
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, Future, as_completed
import threading
from collections import defaultdict, deque
import heapq

from .coordinators import (
    ExecutionContext,
    ExecutionMode,
    ToolExecutionPlan,
    ExecutionResult,
    ToolPriority
)

logger = logging.getLogger(__name__)

# =============================================================================
# æ‰§è¡Œç­–ç•¥å’Œç®—æ³•
# =============================================================================

class ExecutionStrategy(Enum):
    """æ‰§è¡Œç­–ç•¥æšä¸¾"""
    ROUND_ROBIN = "round_robin"           # è½®è¯¢æ‰§è¡Œ
    PRIORITY_QUEUE = "priority_queue"     # ä¼˜å…ˆçº§é˜Ÿåˆ—
    DEPENDENCY_GRAPH = "dependency_graph" # ä¾èµ–å›¾æ‹“æ‰‘æ’åº
    LOAD_BALANCED = "load_balanced"       # è´Ÿè½½å‡è¡¡
    CRITICAL_PATH = "critical_path"       # å…³é”®è·¯å¾„ä¼˜å…ˆ
    ADAPTIVE_BATCH = "adaptive_batch"     # è‡ªé€‚åº”æ‰¹å¤„ç†

class ResourceType(Enum):
    """èµ„æºç±»å‹"""
    CPU = "cpu"
    MEMORY = "memory"
    NETWORK = "network"
    API_QUOTA = "api_quota"
    CACHE = "cache"

@dataclass
class ResourceConstraint:
    """èµ„æºçº¦æŸ"""
    resource_type: ResourceType
    max_usage: float
    current_usage: float = 0.0
    allocation_strategy: str = "fair"  # fair, priority, greedy

@dataclass
class ExecutionNode:
    """æ‰§è¡ŒèŠ‚ç‚¹"""
    plan: ToolExecutionPlan
    dependencies: Set[str] = field(default_factory=set)
    dependents: Set[str] = field(default_factory=set)
    estimated_duration: float = 10.0
    resource_requirements: Dict[ResourceType, float] = field(default_factory=dict)
    scheduling_priority: float = 0.0

@dataclass
class ExecutionBatch:
    """æ‰§è¡Œæ‰¹æ¬¡"""
    batch_id: str
    nodes: List[ExecutionNode]
    estimated_total_time: float
    resource_usage: Dict[ResourceType, float] = field(default_factory=dict)
    can_parallel: bool = True

# =============================================================================
# æ™ºèƒ½æ‰§è¡Œå¼•æ“
# =============================================================================

class SmartExecutionEngine:
    """
    æ™ºèƒ½æ‰§è¡Œå¼•æ“
    
    åŠŸèƒ½ï¼š
    - ä¾èµ–å…³ç³»åˆ†æå’Œæ‹“æ‰‘æ’åº
    - èµ„æºç®¡ç†å’Œçº¦æŸæ£€æŸ¥
    - æ™ºèƒ½æ‰¹å¤„ç†å’Œè°ƒåº¦
    - åŠ¨æ€è´Ÿè½½å‡è¡¡
    - æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–
    """
    
    def __init__(self,
                 max_parallel_workers: int = 4,
                 enable_resource_management: bool = True,
                 enable_smart_scheduling: bool = True):
        """
        åˆå§‹åŒ–æ™ºèƒ½æ‰§è¡Œå¼•æ“
        
        Args:
            max_parallel_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
            enable_resource_management: æ˜¯å¦å¯ç”¨èµ„æºç®¡ç†
            enable_smart_scheduling: æ˜¯å¦å¯ç”¨æ™ºèƒ½è°ƒåº¦
        """
        self.max_parallel_workers = max_parallel_workers
        self.enable_resource_management = enable_resource_management
        self.enable_smart_scheduling = enable_smart_scheduling
        
        # æ‰§è¡Œå™¨
        self.thread_pool = ThreadPoolExecutor(max_workers=max_parallel_workers)
        
        # èµ„æºç®¡ç†
        self.resource_constraints = {
            ResourceType.CPU: ResourceConstraint(ResourceType.CPU, max_usage=1.0),
            ResourceType.MEMORY: ResourceConstraint(ResourceType.MEMORY, max_usage=1024.0),  # MB
            ResourceType.NETWORK: ResourceConstraint(ResourceType.NETWORK, max_usage=10.0),  # requests/sec
            ResourceType.API_QUOTA: ResourceConstraint(ResourceType.API_QUOTA, max_usage=100.0),  # calls/min
        }
        
        # è°ƒåº¦å™¨
        self.scheduler = IntelligentScheduler()
        self.dependency_analyzer = DependencyAnalyzer()
        self.performance_tracker = PerformanceTracker()
        
        # æ‰§è¡ŒçŠ¶æ€
        self.active_executions = {}
        self.execution_history = deque(maxlen=1000)
        self.resource_allocations = defaultdict(float)
        
        logger.info("ğŸš€ SmartExecutionEngine åˆå§‹åŒ–å®Œæˆ")
    
    async def execute_optimized_plan(self,
                                   execution_plans: List[ToolExecutionPlan],
                                   context: ExecutionContext) -> Dict[str, ExecutionResult]:
        """
        æ‰§è¡Œä¼˜åŒ–çš„æ‰§è¡Œè®¡åˆ’
        
        Args:
            execution_plans: æ‰§è¡Œè®¡åˆ’åˆ—è¡¨
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        logger.info(f"ğŸ¯ å¼€å§‹ä¼˜åŒ–æ‰§è¡Œ: {len(execution_plans)} ä¸ªå·¥å…·")
        
        try:
            # 1. æ„å»ºæ‰§è¡Œå›¾
            execution_graph = self._build_execution_graph(execution_plans, context)
            
            # 2. ä¾èµ–åˆ†æ
            dependency_order = self.dependency_analyzer.analyze_dependencies(execution_graph)
            
            # 3. æ™ºèƒ½è°ƒåº¦
            execution_batches = self.scheduler.create_execution_batches(
                execution_graph, dependency_order, context
            )
            
            # 4. èµ„æºåˆ†é…
            if self.enable_resource_management:
                execution_batches = self._allocate_resources(execution_batches)
            
            # 5. æ‰§è¡Œæ‰¹æ¬¡
            results = await self._execute_batches(execution_batches, context)
            
            # 6. æ€§èƒ½è·Ÿè¸ª
            self.performance_tracker.record_execution(execution_plans, results, context)
            
            logger.info(f"âœ… ä¼˜åŒ–æ‰§è¡Œå®Œæˆ: {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–æ‰§è¡Œå¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•æ‰§è¡Œ
            return await self._fallback_execution(execution_plans, context)
    
    def _build_execution_graph(self,
                             execution_plans: List[ToolExecutionPlan],
                             context: ExecutionContext) -> Dict[str, ExecutionNode]:
        """æ„å»ºæ‰§è¡Œå›¾"""
        graph = {}
        
        for plan in execution_plans:
            # ä¼°ç®—æ‰§è¡Œæ—¶é—´
            estimated_duration = self._estimate_execution_time(plan, context)
            
            # ä¼°ç®—èµ„æºéœ€æ±‚
            resource_requirements = self._estimate_resource_requirements(plan, context)
            
            # è®¡ç®—è°ƒåº¦ä¼˜å…ˆçº§
            scheduling_priority = self._calculate_scheduling_priority(plan, context)
            
            # åˆ›å»ºæ‰§è¡ŒèŠ‚ç‚¹
            node = ExecutionNode(
                plan=plan,
                dependencies=set(plan.dependencies),
                estimated_duration=estimated_duration,
                resource_requirements=resource_requirements,
                scheduling_priority=scheduling_priority
            )
            
            graph[plan.tool_name] = node
        
        # å»ºç«‹ä¾èµ–å…³ç³»
        for tool_name, node in graph.items():
            for dep_name in node.dependencies:
                if dep_name in graph:
                    graph[dep_name].dependents.add(tool_name)
        
        logger.info(f"ğŸ“Š æ„å»ºæ‰§è¡Œå›¾: {len(graph)} ä¸ªèŠ‚ç‚¹")
        return graph
    
    def _estimate_execution_time(self, plan: ToolExecutionPlan, context: ExecutionContext) -> float:
        """ä¼°ç®—æ‰§è¡Œæ—¶é—´"""
        # åŸºç¡€æ—¶é—´ä¼°ç®—
        base_times = {
            "thinking_seed": 8.0,
            "rag_seed": 15.0,
            "path_generator": 12.0,
            "mab_decision": 6.0,
            "idea_verification": 10.0
        }
        
        base_time = base_times.get(plan.tool_name, 10.0)
        
        # æ ¹æ®å†å²æ€§èƒ½è°ƒæ•´
        historical_avg = self.performance_tracker.get_average_execution_time(plan.tool_name)
        if historical_avg > 0:
            base_time = (base_time + historical_avg) / 2
        
        # æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦è°ƒæ•´
        query_complexity = len(context.user_query) / 100.0
        complexity_factor = min(2.0, 1.0 + query_complexity * 0.5)
        
        return base_time * complexity_factor
    
    def _estimate_resource_requirements(self,
                                      plan: ToolExecutionPlan,
                                      context: ExecutionContext) -> Dict[ResourceType, float]:
        """ä¼°ç®—èµ„æºéœ€æ±‚"""
        requirements = {}
        
        # CPUéœ€æ±‚
        cpu_intensive_tools = {"path_generator", "mab_decision"}
        if plan.tool_name in cpu_intensive_tools:
            requirements[ResourceType.CPU] = 0.3
        else:
            requirements[ResourceType.CPU] = 0.1
        
        # å†…å­˜éœ€æ±‚
        memory_intensive_tools = {"rag_seed", "path_generator"}
        if plan.tool_name in memory_intensive_tools:
            requirements[ResourceType.MEMORY] = 50.0  # MB
        else:
            requirements[ResourceType.MEMORY] = 20.0
        
        # ç½‘ç»œéœ€æ±‚
        network_intensive_tools = {"rag_seed", "idea_verification"}
        if plan.tool_name in network_intensive_tools:
            requirements[ResourceType.NETWORK] = 2.0  # requests/sec
        else:
            requirements[ResourceType.NETWORK] = 0.5
        
        # APIé…é¢éœ€æ±‚
        api_intensive_tools = {"thinking_seed", "rag_seed", "path_generator", "mab_decision"}
        if plan.tool_name in api_intensive_tools:
            requirements[ResourceType.API_QUOTA] = 5.0  # calls/min
        else:
            requirements[ResourceType.API_QUOTA] = 1.0
        
        return requirements
    
    def _calculate_scheduling_priority(self,
                                     plan: ToolExecutionPlan,
                                     context: ExecutionContext) -> float:
        """è®¡ç®—è°ƒåº¦ä¼˜å…ˆçº§"""
        priority_score = 0.0
        
        # å·¥å…·ä¼˜å…ˆçº§æƒé‡
        priority_weights = {
            ToolPriority.CRITICAL: 100.0,
            ToolPriority.HIGH: 75.0,
            ToolPriority.MEDIUM: 50.0,
            ToolPriority.LOW: 25.0,
            ToolPriority.OPTIONAL: 10.0
        }
        
        priority_score += priority_weights.get(plan.priority, 50.0)
        
        # ä¾èµ–æ•°é‡æƒé‡ï¼ˆä¾èµ–å°‘çš„ä¼˜å…ˆï¼‰
        dependency_penalty = len(plan.dependencies) * 10.0
        priority_score -= dependency_penalty
        
        # æ‰§è¡Œæ—¶é—´æƒé‡ï¼ˆæ—¶é—´çŸ­çš„ä¼˜å…ˆï¼‰
        estimated_time = self._estimate_execution_time(plan, context)
        time_bonus = max(0, 20.0 - estimated_time)
        priority_score += time_bonus
        
        # å†å²æˆåŠŸç‡æƒé‡
        success_rate = self.performance_tracker.get_success_rate(plan.tool_name)
        success_bonus = success_rate * 20.0
        priority_score += success_bonus
        
        return priority_score
    
    def _allocate_resources(self, execution_batches: List[ExecutionBatch]) -> List[ExecutionBatch]:
        """åˆ†é…èµ„æº"""
        allocated_batches = []
        
        for batch in execution_batches:
            # æ£€æŸ¥èµ„æºçº¦æŸ
            if self._check_resource_availability(batch):
                # åˆ†é…èµ„æº
                self._allocate_batch_resources(batch)
                allocated_batches.append(batch)
            else:
                # èµ„æºä¸è¶³ï¼Œæ‹†åˆ†æ‰¹æ¬¡
                split_batches = self._split_batch(batch)
                allocated_batches.extend(split_batches)
        
        return allocated_batches
    
    def _check_resource_availability(self, batch: ExecutionBatch) -> bool:
        """æ£€æŸ¥èµ„æºå¯ç”¨æ€§"""
        for resource_type, required in batch.resource_usage.items():
            constraint = self.resource_constraints[resource_type]
            if constraint.current_usage + required > constraint.max_usage:
                logger.debug(f"âš ï¸ èµ„æºä¸è¶³: {resource_type.value} éœ€è¦ {required}, å¯ç”¨ {constraint.max_usage - constraint.current_usage}")
                return False
        return True
    
    def _allocate_batch_resources(self, batch: ExecutionBatch):
        """åˆ†é…æ‰¹æ¬¡èµ„æº"""
        for resource_type, required in batch.resource_usage.items():
            self.resource_constraints[resource_type].current_usage += required
            self.resource_allocations[f"{batch.batch_id}_{resource_type.value}"] = required
    
    def _split_batch(self, batch: ExecutionBatch) -> List[ExecutionBatch]:
        """æ‹†åˆ†æ‰¹æ¬¡"""
        # ç®€å•ç­–ç•¥ï¼šæŒ‰ä¼˜å…ˆçº§åˆ†ç»„
        high_priority_nodes = [node for node in batch.nodes if node.plan.priority in [ToolPriority.CRITICAL, ToolPriority.HIGH]]
        low_priority_nodes = [node for node in batch.nodes if node.plan.priority not in [ToolPriority.CRITICAL, ToolPriority.HIGH]]
        
        split_batches = []
        
        if high_priority_nodes:
            high_batch = ExecutionBatch(
                batch_id=f"{batch.batch_id}_high",
                nodes=high_priority_nodes,
                estimated_total_time=sum(node.estimated_duration for node in high_priority_nodes)
            )
            split_batches.append(high_batch)
        
        if low_priority_nodes:
            low_batch = ExecutionBatch(
                batch_id=f"{batch.batch_id}_low",
                nodes=low_priority_nodes,
                estimated_total_time=sum(node.estimated_duration for node in low_priority_nodes)
            )
            split_batches.append(low_batch)
        
        return split_batches
    
    async def _execute_batches(self,
                             execution_batches: List[ExecutionBatch],
                             context: ExecutionContext) -> Dict[str, ExecutionResult]:
        """æ‰§è¡Œæ‰¹æ¬¡"""
        results = {}
        
        for batch in execution_batches:
            logger.info(f"ğŸ”„ æ‰§è¡Œæ‰¹æ¬¡: {batch.batch_id} ({len(batch.nodes)} ä¸ªå·¥å…·)")
            
            if batch.can_parallel and len(batch.nodes) > 1:
                # å¹¶è¡Œæ‰§è¡Œ
                batch_results = await self._execute_batch_parallel(batch, context, results)
            else:
                # é¡ºåºæ‰§è¡Œ
                batch_results = await self._execute_batch_sequential(batch, context, results)
            
            results.update(batch_results)
            
            # é‡Šæ”¾èµ„æº
            self._release_batch_resources(batch)
        
        return results
    
    async def _execute_batch_parallel(self,
                                    batch: ExecutionBatch,
                                    context: ExecutionContext,
                                    previous_results: Dict[str, ExecutionResult]) -> Dict[str, ExecutionResult]:
        """å¹¶è¡Œæ‰§è¡Œæ‰¹æ¬¡"""
        tasks = []
        
        for node in batch.nodes:
            task = asyncio.create_task(
                self._execute_single_node(node, context, previous_results)
            )
            tasks.append((node.plan.tool_name, task))
        
        results = {}
        for tool_name, task in tasks:
            try:
                result = await task
                results[tool_name] = result
            except Exception as e:
                logger.error(f"âŒ å¹¶è¡Œæ‰§è¡ŒèŠ‚ç‚¹å¤±è´¥: {tool_name} - {e}")
                results[tool_name] = ExecutionResult(
                    tool_name=tool_name,
                    stage=batch.nodes[0].plan.stage,  # ä½¿ç”¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„é˜¶æ®µ
                    success=False,
                    error_message=str(e)
                )
        
        return results
    
    async def _execute_batch_sequential(self,
                                      batch: ExecutionBatch,
                                      context: ExecutionContext,
                                      previous_results: Dict[str, ExecutionResult]) -> Dict[str, ExecutionResult]:
        """é¡ºåºæ‰§è¡Œæ‰¹æ¬¡"""
        results = {}
        
        for node in batch.nodes:
            result = await self._execute_single_node(node, context, previous_results)
            results[node.plan.tool_name] = result
            
            # æ›´æ–°ä¹‹å‰çš„ç»“æœï¼Œä¾›åç»­èŠ‚ç‚¹ä½¿ç”¨
            previous_results[node.plan.tool_name] = result
        
        return results
    
    async def _execute_single_node(self,
                                 node: ExecutionNode,
                                 context: ExecutionContext,
                                 previous_results: Dict[str, ExecutionResult]) -> ExecutionResult:
        """æ‰§è¡Œå•ä¸ªèŠ‚ç‚¹"""
        start_time = time.time()
        
        try:
            # å‡†å¤‡å·¥å…·è¾“å…¥ï¼ˆéœ€è¦åè°ƒå™¨çš„æ–¹æ³•ï¼‰
            tool_input = self._prepare_node_input(node, context, previous_results)
            
            # æ‰§è¡Œå·¥å…·
            loop = asyncio.get_event_loop()
            result_data = await loop.run_in_executor(
                self.thread_pool,
                node.plan.tool_instance.run,
                **tool_input
            )
            
            execution_time = time.time() - start_time
            
            result = ExecutionResult(
                tool_name=node.plan.tool_name,
                stage=node.plan.stage,
                success=True,
                data=result_data,
                execution_time=execution_time,
                metadata={
                    "node_priority": node.scheduling_priority,
                    "estimated_duration": node.estimated_duration,
                    "actual_duration": execution_time
                }
            )
            
            logger.info(f"âœ… èŠ‚ç‚¹æ‰§è¡ŒæˆåŠŸ: {node.plan.tool_name} ({execution_time:.2f}s)")
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            logger.error(f"âŒ èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {node.plan.tool_name} - {error_msg}")
            
            return ExecutionResult(
                tool_name=node.plan.tool_name,
                stage=node.plan.stage,
                success=False,
                execution_time=execution_time,
                error_message=error_msg
            )
    
    def _prepare_node_input(self,
                          node: ExecutionNode,
                          context: ExecutionContext,
                          previous_results: Dict[str, ExecutionResult]) -> Dict[str, Any]:
        """å‡†å¤‡èŠ‚ç‚¹è¾“å…¥ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œåº”è¯¥ä¸åè°ƒå™¨çš„_prepare_tool_inputæ–¹æ³•ä¿æŒä¸€è‡´
        # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œæä¾›åŸºç¡€å®ç°
        
        tool_input = {
            "execution_context": context.custom_config
        }
        
        if node.plan.tool_name == "thinking_seed":
            tool_input.update({
                "user_query": context.user_query,
                "execution_context": context.custom_config
            })
        elif node.plan.tool_name == "rag_seed":
            tool_input.update({
                "user_query": context.user_query,
                "execution_context": context.custom_config
            })
        # å…¶ä»–å·¥å…·çš„è¾“å…¥å‡†å¤‡é€»è¾‘...
        
        return tool_input
    
    def _release_batch_resources(self, batch: ExecutionBatch):
        """é‡Šæ”¾æ‰¹æ¬¡èµ„æº"""
        for resource_type, allocated in batch.resource_usage.items():
            self.resource_constraints[resource_type].current_usage -= allocated
            allocation_key = f"{batch.batch_id}_{resource_type.value}"
            if allocation_key in self.resource_allocations:
                del self.resource_allocations[allocation_key]
    
    async def _fallback_execution(self,
                                execution_plans: List[ToolExecutionPlan],
                                context: ExecutionContext) -> Dict[str, ExecutionResult]:
        """å›é€€æ‰§è¡Œæ–¹æ¡ˆ"""
        logger.warning("ğŸ”„ ä½¿ç”¨å›é€€æ‰§è¡Œæ–¹æ¡ˆ")
        
        results = {}
        
        for plan in execution_plans:
            try:
                # æ£€æŸ¥ä¾èµ–
                dependencies_met = all(
                    dep in results and results[dep].success
                    for dep in plan.dependencies
                )
                
                if not dependencies_met and plan.dependencies:
                    logger.warning(f"âš ï¸ è·³è¿‡å·¥å…· {plan.tool_name}ï¼šä¾èµ–æœªæ»¡è¶³")
                    continue
                
                # ç®€å•æ‰§è¡Œ
                start_time = time.time()
                result_data = plan.tool_instance.run()  # ç®€åŒ–è°ƒç”¨
                execution_time = time.time() - start_time
                
                result = ExecutionResult(
                    tool_name=plan.tool_name,
                    stage=plan.stage,
                    success=True,
                    data=result_data,
                    execution_time=execution_time
                )
                
                results[plan.tool_name] = result
                
            except Exception as e:
                logger.error(f"âŒ å›é€€æ‰§è¡Œå¤±è´¥: {plan.tool_name} - {e}")
                result = ExecutionResult(
                    tool_name=plan.tool_name,
                    stage=plan.stage,
                    success=False,
                    error_message=str(e),
                    execution_time=0.0
                )
                results[plan.tool_name] = result
        
        return results

# =============================================================================
# ä¾èµ–åˆ†æå™¨
# =============================================================================

class DependencyAnalyzer:
    """ä¾èµ–å…³ç³»åˆ†æå™¨"""
    
    def analyze_dependencies(self, execution_graph: Dict[str, ExecutionNode]) -> List[List[str]]:
        """
        åˆ†æä¾èµ–å…³ç³»ï¼Œè¿”å›æ‹“æ‰‘æ’åºç»“æœ
        
        Args:
            execution_graph: æ‰§è¡Œå›¾
            
        Returns:
            æŒ‰ä¾èµ–é¡ºåºåˆ†ç»„çš„å·¥å…·åç§°åˆ—è¡¨
        """
        # æ‹“æ‰‘æ’åº
        in_degree = {}
        for tool_name, node in execution_graph.items():
            in_degree[tool_name] = len(node.dependencies)
        
        # æ‰¾åˆ°æ²¡æœ‰ä¾èµ–çš„èŠ‚ç‚¹
        queue = [tool_name for tool_name, degree in in_degree.items() if degree == 0]
        ordered_groups = []
        
        while queue:
            # å½“å‰å±‚çš„æ‰€æœ‰æ— ä¾èµ–èŠ‚ç‚¹
            current_level = queue[:]
            queue.clear()
            ordered_groups.append(current_level)
            
            # å¤„ç†å½“å‰å±‚çš„èŠ‚ç‚¹
            for tool_name in current_level:
                node = execution_graph[tool_name]
                # å‡å°‘ä¾èµ–æ­¤èŠ‚ç‚¹çš„å…¶ä»–èŠ‚ç‚¹çš„å…¥åº¦
                for dependent in node.dependents:
                    in_degree[dependent] -= 1
                    if in_degree[dependent] == 0:
                        queue.append(dependent)
        
        logger.info(f"ğŸ“Š ä¾èµ–åˆ†æå®Œæˆ: {len(ordered_groups)} å±‚ï¼Œ{sum(len(group) for group in ordered_groups)} ä¸ªèŠ‚ç‚¹")
        return ordered_groups

# =============================================================================
# æ™ºèƒ½è°ƒåº¦å™¨
# =============================================================================

class IntelligentScheduler:
    """æ™ºèƒ½è°ƒåº¦å™¨"""
    
    def create_execution_batches(self,
                                execution_graph: Dict[str, ExecutionNode],
                                dependency_order: List[List[str]],
                                context: ExecutionContext) -> List[ExecutionBatch]:
        """
        åˆ›å»ºæ‰§è¡Œæ‰¹æ¬¡
        
        Args:
            execution_graph: æ‰§è¡Œå›¾
            dependency_order: ä¾èµ–é¡ºåº
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æ‰§è¡Œæ‰¹æ¬¡åˆ—è¡¨
        """
        batches = []
        
        for level_idx, tool_names in enumerate(dependency_order):
            if not tool_names:
                continue
            
            nodes = [execution_graph[tool_name] for tool_name in tool_names]
            
            # æ ¹æ®æ‰§è¡Œæ¨¡å¼å†³å®šæ‰¹æ¬¡ç­–ç•¥
            if context.execution_mode == ExecutionMode.PARALLEL:
                # å¹¶è¡Œæ¨¡å¼ï¼šåŒå±‚èŠ‚ç‚¹æ”¾åœ¨ä¸€ä¸ªæ‰¹æ¬¡
                batch = ExecutionBatch(
                    batch_id=f"parallel_batch_level_{level_idx}",
                    nodes=nodes,
                    estimated_total_time=max(node.estimated_duration for node in nodes),
                    can_parallel=True
                )
                batches.append(batch)
                
            elif context.execution_mode == ExecutionMode.SEQUENTIAL:
                # é¡ºåºæ¨¡å¼ï¼šæ¯ä¸ªèŠ‚ç‚¹ä¸€ä¸ªæ‰¹æ¬¡
                for node_idx, node in enumerate(nodes):
                    batch = ExecutionBatch(
                        batch_id=f"sequential_batch_level_{level_idx}_node_{node_idx}",
                        nodes=[node],
                        estimated_total_time=node.estimated_duration,
                        can_parallel=False
                    )
                    batches.append(batch)
                    
            else:  # ADAPTIVE
                # è‡ªé€‚åº”æ¨¡å¼ï¼šæ ¹æ®èŠ‚ç‚¹ç‰¹å¾æ™ºèƒ½åˆ†ç»„
                adaptive_batches = self._create_adaptive_batches(nodes, level_idx)
                batches.extend(adaptive_batches)
        
        # è®¡ç®—æ‰¹æ¬¡èµ„æºä½¿ç”¨
        for batch in batches:
            batch.resource_usage = self._calculate_batch_resource_usage(batch)
        
        logger.info(f"ğŸ“‹ åˆ›å»ºæ‰§è¡Œæ‰¹æ¬¡: {len(batches)} ä¸ªæ‰¹æ¬¡")
        return batches
    
    def _create_adaptive_batches(self,
                               nodes: List[ExecutionNode],
                               level_idx: int) -> List[ExecutionBatch]:
        """åˆ›å»ºè‡ªé€‚åº”æ‰¹æ¬¡"""
        # æŒ‰ä¼˜å…ˆçº§å’Œä¼°ç®—æ—¶é—´åˆ†ç»„
        high_priority_fast = []
        high_priority_slow = []
        low_priority = []
        
        for node in nodes:
            if node.plan.priority in [ToolPriority.CRITICAL, ToolPriority.HIGH]:
                if node.estimated_duration <= 10.0:
                    high_priority_fast.append(node)
                else:
                    high_priority_slow.append(node)
            else:
                low_priority.append(node)
        
        batches = []
        
        # é«˜ä¼˜å…ˆçº§å¿«é€Ÿä»»åŠ¡ - å¹¶è¡Œæ‰§è¡Œ
        if high_priority_fast:
            batch = ExecutionBatch(
                batch_id=f"adaptive_high_fast_level_{level_idx}",
                nodes=high_priority_fast,
                estimated_total_time=max(node.estimated_duration for node in high_priority_fast),
                can_parallel=True
            )
            batches.append(batch)
        
        # é«˜ä¼˜å…ˆçº§æ…¢é€Ÿä»»åŠ¡ - é¡ºåºæ‰§è¡Œ
        for idx, node in enumerate(high_priority_slow):
            batch = ExecutionBatch(
                batch_id=f"adaptive_high_slow_level_{level_idx}_node_{idx}",
                nodes=[node],
                estimated_total_time=node.estimated_duration,
                can_parallel=False
            )
            batches.append(batch)
        
        # ä½ä¼˜å…ˆçº§ä»»åŠ¡ - å¹¶è¡Œæ‰§è¡Œ
        if low_priority:
            batch = ExecutionBatch(
                batch_id=f"adaptive_low_level_{level_idx}",
                nodes=low_priority,
                estimated_total_time=max(node.estimated_duration for node in low_priority),
                can_parallel=True
            )
            batches.append(batch)
        
        return batches
    
    def _calculate_batch_resource_usage(self, batch: ExecutionBatch) -> Dict[ResourceType, float]:
        """è®¡ç®—æ‰¹æ¬¡èµ„æºä½¿ç”¨"""
        resource_usage = defaultdict(float)
        
        if batch.can_parallel:
            # å¹¶è¡Œæ‰§è¡Œï¼šå–æœ€å¤§å€¼
            for node in batch.nodes:
                for resource_type, requirement in node.resource_requirements.items():
                    resource_usage[resource_type] = max(resource_usage[resource_type], requirement)
        else:
            # é¡ºåºæ‰§è¡Œï¼šå–å¹³å‡å€¼
            node_count = len(batch.nodes)
            for node in batch.nodes:
                for resource_type, requirement in node.resource_requirements.items():
                    resource_usage[resource_type] += requirement / node_count
        
        return dict(resource_usage)

# =============================================================================
# æ€§èƒ½è·Ÿè¸ªå™¨
# =============================================================================

class PerformanceTracker:
    """æ€§èƒ½è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.execution_history = []
        self.tool_stats = defaultdict(lambda: {
            "executions": 0,
            "successes": 0,
            "total_time": 0.0,
            "avg_time": 0.0,
            "success_rate": 0.0
        })
    
    def record_execution(self,
                        execution_plans: List[ToolExecutionPlan],
                        results: Dict[str, ExecutionResult],
                        context: ExecutionContext):
        """è®°å½•æ‰§è¡Œæ€§èƒ½"""
        execution_record = {
            "timestamp": time.time(),
            "context": context,
            "plans": execution_plans,
            "results": results,
            "total_time": sum(r.execution_time for r in results.values()),
            "success_count": sum(1 for r in results.values() if r.success)
        }
        
        self.execution_history.append(execution_record)
        
        # æ›´æ–°å·¥å…·ç»Ÿè®¡
        for tool_name, result in results.items():
            stats = self.tool_stats[tool_name]
            stats["executions"] += 1
            stats["total_time"] += result.execution_time
            
            if result.success:
                stats["successes"] += 1
            
            stats["avg_time"] = stats["total_time"] / stats["executions"]
            stats["success_rate"] = stats["successes"] / stats["executions"]
    
    def get_average_execution_time(self, tool_name: str) -> float:
        """è·å–å·¥å…·å¹³å‡æ‰§è¡Œæ—¶é—´"""
        return self.tool_stats[tool_name]["avg_time"]
    
    def get_success_rate(self, tool_name: str) -> float:
        """è·å–å·¥å…·æˆåŠŸç‡"""
        return self.tool_stats[tool_name]["success_rate"]
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        return {
            "total_executions": len(self.execution_history),
            "tool_stats": dict(self.tool_stats),
            "recent_performance": self.execution_history[-10:] if self.execution_history else []
        }

# =============================================================================
# æµ‹è¯•å’Œæ¼”ç¤º
# =============================================================================

if __name__ == "__main__":
    # æµ‹è¯•æ™ºèƒ½æ‰§è¡Œå¼•æ“
    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½æ‰§è¡Œå¼•æ“...")
    
    # åˆ›å»ºæ‰§è¡Œå¼•æ“
    engine = SmartExecutionEngine(
        max_parallel_workers=4,
        enable_resource_management=True,
        enable_smart_scheduling=True
    )
    
    print("âœ… æ™ºèƒ½æ‰§è¡Œå¼•æ“åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ‰§è¡Œè®¡åˆ’
    from .coordinators import ToolExecutionPlan, DecisionStage, ToolPriority
    
    mock_plans = [
        ToolExecutionPlan(
            stage=DecisionStage.THINKING_SEED,
            tool_name="thinking_seed",
            tool_instance=None,  # æ¨¡æ‹Ÿ
            priority=ToolPriority.CRITICAL
        ),
        ToolExecutionPlan(
            stage=DecisionStage.PATH_GENERATION,
            tool_name="path_generator",
            tool_instance=None,
            priority=ToolPriority.HIGH,
            dependencies=["thinking_seed"]
        )
    ]
    
    # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
    mock_context = ExecutionContext(
        session_id="test_session",
        user_query="æµ‹è¯•æŸ¥è¯¢",
        execution_mode=ExecutionMode.ADAPTIVE
    )
    
    # æ„å»ºæ‰§è¡Œå›¾
    execution_graph = engine._build_execution_graph(mock_plans, mock_context)
    print(f"âœ… æ„å»ºæ‰§è¡Œå›¾: {len(execution_graph)} ä¸ªèŠ‚ç‚¹")
    
    # ä¾èµ–åˆ†æ
    dependency_order = engine.dependency_analyzer.analyze_dependencies(execution_graph)
    print(f"âœ… ä¾èµ–åˆ†æ: {len(dependency_order)} å±‚")
    
    # åˆ›å»ºæ‰§è¡Œæ‰¹æ¬¡
    execution_batches = engine.scheduler.create_execution_batches(
        execution_graph, dependency_order, mock_context
    )
    print(f"âœ… åˆ›å»ºæ‰¹æ¬¡: {len(execution_batches)} ä¸ªæ‰¹æ¬¡")
    
    print("âœ… æ™ºèƒ½æ‰§è¡Œå¼•æ“æµ‹è¯•å®Œæˆ")
