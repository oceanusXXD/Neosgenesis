#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æœç´¢å·¥å…·å®¢æˆ·ç«¯ - ç”¨äºè¿æ¥å¤–éƒ¨æœç´¢å¼•æ“
Search Tool Client - for connecting to external search engines
"""

import json
import logging
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import requests

# ğŸ”§ æ–°å¢ï¼šæ”¯æŒçœŸå®DuckDuckGoæœç´¢
try:
    from duckduckgo_search import DDGS
    REAL_SEARCH_AVAILABLE = True
except ImportError:
    REAL_SEARCH_AVAILABLE = False
    logging.warning("âš ï¸ duckduckgo-searchåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢ç»“æœ")

# å¯¼å…¥é…ç½®
try:
    from config import RAG_CONFIG
except ImportError:
    # é»˜è®¤é…ç½®ï¼ˆå¦‚æœæ— æ³•å¯¼å…¥ï¼‰
    RAG_CONFIG = {"enable_real_web_search": False}

logger = logging.getLogger(__name__)

# å…¨å±€æœç´¢è¯·æ±‚ç®¡ç†å™¨ï¼Œç”¨äºæ§åˆ¶è¯·æ±‚é¢‘ç‡
class SearchRateLimiter:
    """æœç´¢é€Ÿç‡é™åˆ¶ç®¡ç†å™¨"""
    def __init__(self):
        self.last_request_time = 0
        # ä»é…ç½®è·å–è¯·æ±‚é—´éš”ï¼Œå¦‚æœé…ç½®ä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        self.min_interval = RAG_CONFIG.get("search_rate_limit_interval", 1.5)
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…åˆ°ä¸‹ä¸€ä¸ªå…è®¸çš„è¯·æ±‚æ—¶é—´"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_interval:
            wait_time = self.min_interval - time_since_last
            logger.debug(f"â³ é€Ÿç‡é™åˆ¶ç­‰å¾…: {wait_time:.1f}ç§’")
            time.sleep(wait_time)
        
        self.last_request_time = time.time()

# å…¨å±€é€Ÿç‡é™åˆ¶å™¨å®ä¾‹
_rate_limiter = SearchRateLimiter()

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    title: str
    snippet: str
    url: str
    relevance_score: float = 0.0

@dataclass
class SearchResponse:
    """æœç´¢å“åº”æ•°æ®ç»“æ„"""
    query: str
    results: List[SearchResult]
    total_results: int
    search_time: float
    success: bool = True
    error_message: str = ""

@dataclass
class IdeaVerificationResult:
    """æƒ³æ³•éªŒè¯ç»“æœæ•°æ®ç»“æ„"""
    idea_text: str
    feasibility_score: float
    analysis_summary: str
    search_results: List[SearchResult]
    success: bool = True
    error_message: str = ""

class WebSearchClient:
    """ç½‘ç»œæœç´¢å®¢æˆ·ç«¯"""
    
    def __init__(self, search_engine: str = "duckduckgo", max_results: int = 5):
        """
        åˆå§‹åŒ–æœç´¢å®¢æˆ·ç«¯
        
        Args:
            search_engine: æœç´¢å¼•æ“ç±»å‹ ("duckduckgo", "bing", "google")
            max_results: æœ€å¤§ç»“æœæ•°é‡
        """
        self.search_engine = search_engine
        self.max_results = max_results
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # æœç´¢ç»Ÿè®¡
        self.search_stats = {
            'total_searches': 0,
            'successful_searches': 0,
            'total_search_time': 0.0,
            'avg_search_time': 0.0
        }
        
        logger.info(f"ğŸ” WebSearchClientåˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨{search_engine}æœç´¢å¼•æ“")
    
    def search(self, query: str, max_results: Optional[int] = None) -> SearchResponse:
        """
        æ‰§è¡Œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°é‡ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
            
        Returns:
            SearchResponse: æœç´¢å“åº”
        """
        start_time = time.time()
        max_results = max_results or self.max_results
        
        logger.info(f"ğŸ” å¼€å§‹æœç´¢: {query[:50]}...")
        
        try:
            if self.search_engine == "duckduckgo":
                response = self._search_duckduckgo(query, max_results)
            elif self.search_engine == "bing":
                response = self._search_bing(query, max_results)
            else:
                response = self._search_fallback(query, max_results)
            
            search_time = time.time() - start_time
            response.search_time = search_time
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_search_stats(search_time, response.success)
            
            logger.info(f"ğŸ” æœç´¢å®Œæˆ: æ‰¾åˆ°{len(response.results)}ä¸ªç»“æœï¼Œè€—æ—¶{search_time:.2f}ç§’")
            return response
            
        except Exception as e:
            search_time = time.time() - start_time
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            
            return SearchResponse(
                query=query,
                results=[],
                total_results=0,
                search_time=search_time,
                success=False,
                error_message=str(e)
            )
    
    def _search_duckduckgo(self, query: str, max_results: int) -> SearchResponse:
        """ä½¿ç”¨DuckDuckGoæœç´¢ - å¸¦æ™ºèƒ½å¤‡ç”¨æœºåˆ¶"""
        
        # ğŸ”§ æ£€æŸ¥æ˜¯å¦å¯ç”¨çœŸå®æœç´¢
        if RAG_CONFIG.get("enable_real_web_search", False) and REAL_SEARCH_AVAILABLE:
            # å°è¯•çœŸå®æœç´¢
            response = self._search_duckduckgo_real(query, max_results)
            
            # å¦‚æœçœŸå®æœç´¢å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥æš‚æ—¶ç¦ç”¨çœŸå®æœç´¢
            if not response.success:
                logger.warning(f"ğŸš¨ DuckDuckGoçœŸå®æœç´¢æŒç»­å¤±è´¥ï¼Œå»ºè®®æš‚æ—¶åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæœç´¢æ¨¡å¼")
                logger.info(f"ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šåœ¨config.pyä¸­è®¾ç½® 'enable_real_web_search': False")
            
            return response
        else:
            return self._search_duckduckgo_mock(query, max_results)
    
    def _search_duckduckgo_real(self, query: str, max_results: int) -> SearchResponse:
        """çœŸå®çš„DuckDuckGoæœç´¢ - å¸¦é€Ÿç‡é™åˆ¶å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
        max_retries = RAG_CONFIG.get("search_max_retries", 3)
        base_delay = RAG_CONFIG.get("search_retry_base_delay", 1.0)
        
        for attempt in range(max_retries):
            try:
                # åº”ç”¨å…¨å±€é€Ÿç‡é™åˆ¶
                _rate_limiter.wait_if_needed()
                
                # æ·»åŠ é¢å¤–çš„è¯·æ±‚é—´éš”ä»¥é¿å…é€Ÿç‡é™åˆ¶
                if attempt > 0:
                    delay = base_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    logger.info(f"â³ ç¬¬{attempt + 1}æ¬¡å°è¯•ï¼Œç­‰å¾…{delay:.1f}ç§’...")
                    time.sleep(delay)
                
                logger.info(f"ğŸŒ å¼€å§‹çœŸå®DuckDuckGoæœç´¢: {query} (å°è¯• {attempt + 1}/{max_retries})")
                search_start_time = time.time()
                
                # ä½¿ç”¨æœ€ä¿å®ˆçš„æœç´¢é…ç½®é¿å…é€Ÿç‡é™åˆ¶
                with DDGS() as ddgs:
                    # æ‰§è¡Œæœç´¢ï¼Œä½¿ç”¨é»˜è®¤åç«¯è‡ªåŠ¨é€‰æ‹©
                    ddgs_results = list(ddgs.text(
                        keywords=query,
                        max_results=max_results,
                        region='wt-wt',  # å…¨çƒæœç´¢
                        safesearch='moderate',
                        timelimit=None
                        # ä¸æŒ‡å®šbackendï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©æœ€ç¨³å®šçš„
                    ))
                
                search_time = time.time() - search_start_time
                
                # è½¬æ¢ç»“æœæ ¼å¼
                results = []
                for result in ddgs_results:
                    results.append(SearchResult(
                        title=result.get('title', ''),
                        snippet=result.get('body', ''),
                        url=result.get('href', ''),
                        relevance_score=0.8  # DuckDuckGoä¸æä¾›ç›¸å…³æ€§åˆ†æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    ))
                
                logger.info(f"âœ… çœŸå®æœç´¢æˆåŠŸ: æ‰¾åˆ°{len(results)}ä¸ªç»“æœï¼Œè€—æ—¶{search_time:.2f}ç§’")
                
                return SearchResponse(
                    query=query,
                    results=results,
                    total_results=len(results),
                    search_time=search_time,
                    success=True
                )
                
            except Exception as e:
                error_msg = str(e).lower()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
                if any(keyword in error_msg for keyword in ['rate', 'limit', '202', 'too many']):
                    logger.warning(f"âš ï¸ é‡åˆ°é€Ÿç‡é™åˆ¶ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        continue  # ç»§ç»­é‡è¯•
                    else:
                        logger.error(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé€Ÿç‡é™åˆ¶ä»ç„¶å­˜åœ¨")
                else:
                    # å…¶ä»–é”™è¯¯ï¼Œç«‹å³é™çº§
                    logger.error(f"âŒ çœŸå®DuckDuckGoæœç´¢å¤±è´¥ (éé€Ÿç‡é™åˆ¶): {e}")
                    break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œé™çº§åˆ°æ¨¡æ‹Ÿæœç´¢
        logger.warning("ğŸ”„ é™çº§åˆ°æ¨¡æ‹Ÿæœç´¢ç»“æœ")
        return self._search_duckduckgo_mock(query, max_results)
    
    def _search_duckduckgo_mock(self, query: str, max_results: int) -> SearchResponse:
        """æ¨¡æ‹ŸDuckDuckGoæœç´¢ç»“æœ"""
        logger.debug(f"ğŸ­ ä½¿ç”¨æ¨¡æ‹ŸDuckDuckGoæœç´¢: {query}")
        
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        import random
        mock_delay = random.uniform(0.1, 0.5)
        time.sleep(mock_delay)
        
        mock_results = [
            SearchResult(
                title=f"å…³äº'{query}'çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆ",
                snippet=f"è¿™æ˜¯å…³äº{query}çš„è¯¦ç»†æŠ€æœ¯åˆ†æå’Œå®ç°æ–¹æ¡ˆï¼ŒåŒ…å«äº†æœ€ä½³å®è·µå’Œå¸¸è§é—®é¢˜è§£å†³æ–¹æ³•ã€‚",
                url=f"https://example.com/tech-solution-{hash(query) % 1000}",
                relevance_score=0.9
            ),
            SearchResult(
                title=f"'{query}'å®æ–½æŒ‡å—å’Œæ¡ˆä¾‹ç ”ç©¶",
                snippet=f"é€šè¿‡å®é™…æ¡ˆä¾‹äº†è§£{query}çš„å®æ–½è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æŠ€æœ¯é€‰å‹ã€æ¶æ„è®¾è®¡å’Œæ€§èƒ½ä¼˜åŒ–ã€‚",
                url=f"https://example.com/implementation-guide-{hash(query) % 1000}",
                relevance_score=0.8
            ),
            SearchResult(
                title=f"'{query}'çš„æŒ‘æˆ˜ä¸é£é™©åˆ†æ",
                snippet=f"æ·±å…¥åˆ†æ{query}å¯èƒ½é¢ä¸´çš„æŠ€æœ¯æŒ‘æˆ˜ã€æ½œåœ¨é£é™©ä»¥åŠç›¸åº”çš„åº”å¯¹ç­–ç•¥ã€‚",
                url=f"https://example.com/risk-analysis-{hash(query) % 1000}",
                relevance_score=0.7
            )
        ]
        
        return SearchResponse(
            query=query,
            results=mock_results[:max_results],
            total_results=len(mock_results),
            search_time=mock_delay,  # ğŸ”§ ä¿®å¤ï¼šç°åœ¨æ˜¾ç¤ºçœŸå®çš„æ¨¡æ‹Ÿå»¶è¿Ÿæ—¶é—´
            success=True
        )
    
    def _search_bing(self, query: str, max_results: int) -> SearchResponse:
        """ä½¿ç”¨Bingæœç´¢ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰"""
        # è¿™é‡Œå¯ä»¥é›†æˆçœŸå®çš„Bing Search API
        return self._search_fallback(query, max_results)
    
    def _search_fallback(self, query: str, max_results: int) -> SearchResponse:
        """å¤‡ç”¨æœç´¢æ–¹æ³•"""
        logger.warning("âš ï¸ ä½¿ç”¨å¤‡ç”¨æœç´¢æ–¹æ³•ï¼ˆæ¨¡æ‹Ÿç»“æœï¼‰")
        
        fallback_results = [
            SearchResult(
                title=f"æŠ€æœ¯æ–‡æ¡£: {query}",
                snippet=f"è¿™æ˜¯å…³äº{query}çš„æŠ€æœ¯æ–‡æ¡£å’ŒæŒ‡å—ï¼Œæä¾›äº†åŸºç¡€çš„å®ç°æ–¹æ³•ã€‚",
                url="https://docs.example.com/tech-docs",
                relevance_score=0.6
            ),
            SearchResult(
                title=f"ç¤¾åŒºè®¨è®º: {query}",
                snippet=f"å¼€å‘è€…ç¤¾åŒºå…³äº{query}çš„è®¨è®ºå’Œç»éªŒåˆ†äº«ã€‚",
                url="https://community.example.com/discussions",
                relevance_score=0.5
            )
        ]
        
        return SearchResponse(
            query=query,
            results=fallback_results[:max_results],
            total_results=len(fallback_results),
            search_time=0.0,
            success=True
        )
    
    def _update_search_stats(self, search_time: float, success: bool):
        """æ›´æ–°æœç´¢ç»Ÿè®¡"""
        self.search_stats['total_searches'] += 1
        if success:
            self.search_stats['successful_searches'] += 1
        
        self.search_stats['total_search_time'] += search_time
        self.search_stats['avg_search_time'] = (
            self.search_stats['total_search_time'] / self.search_stats['total_searches']
        )
    
    def get_search_stats(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return self.search_stats.copy()

class IdeaVerificationSearchClient:
    """æƒ³æ³•éªŒè¯ä¸“ç”¨æœç´¢å®¢æˆ·ç«¯"""
    
    def __init__(self, web_search_client: WebSearchClient):
        """
        åˆå§‹åŒ–æƒ³æ³•éªŒè¯æœç´¢å®¢æˆ·ç«¯
        
        Args:
            web_search_client: ç½‘ç»œæœç´¢å®¢æˆ·ç«¯
        """
        self.web_search_client = web_search_client
        self.verification_cache = {}  # éªŒè¯ç»“æœç¼“å­˜
        
        logger.info("ğŸ” IdeaVerificationSearchClientåˆå§‹åŒ–å®Œæˆ")
    
    def search_for_idea_verification(self, idea_text: str, context: Optional[Dict] = None) -> SearchResponse:
        """
        ä¸ºæƒ³æ³•éªŒè¯è¿›è¡Œä¸“é—¨çš„æœç´¢
        
        Args:
            idea_text: éœ€è¦éªŒè¯çš„æƒ³æ³•æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            SearchResponse: æœç´¢å“åº”
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{idea_text}_{hash(str(context))}"
        if cache_key in self.verification_cache:
            logger.debug(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„éªŒè¯æœç´¢ç»“æœ: {cache_key}")
            return self.verification_cache[cache_key]
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_query = self._build_verification_query(idea_text, context)
        
        # æ‰§è¡Œæœç´¢
        search_response = self.web_search_client.search(search_query, max_results=5)
        
        # ç¼“å­˜ç»“æœ
        if search_response.success:
            self.verification_cache[cache_key] = search_response
        
        return search_response
    
    def _build_verification_query(self, idea_text: str, context: Optional[Dict] = None) -> str:
        """
        æ„å»ºç”¨äºæƒ³æ³•éªŒè¯çš„æœç´¢æŸ¥è¯¢
        
        Args:
            idea_text: æƒ³æ³•æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡
            
        Returns:
            str: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # æå–å…³é”®æ¦‚å¿µ
        key_concepts = self._extract_key_concepts(idea_text)
        
        # æ„å»ºæŸ¥è¯¢
        if len(key_concepts) >= 2:
            query = f"'{key_concepts[0]} {key_concepts[1]}' å¯è¡Œæ€§ å®ç°æ–¹æ³• æŠ€æœ¯é£é™©"
        else:
            query = f"'{idea_text[:50]}' æŠ€æœ¯æ–¹æ¡ˆ å®æ–½æŒ‡å— æŒ‘æˆ˜"
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if context and 'domain' in context:
            query = f"{query} {context['domain']}"
        
        logger.debug(f"ğŸ” æ„å»ºéªŒè¯æŸ¥è¯¢: {query}")
        return query
    
    def _extract_key_concepts(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬ä¸­çš„å…³é”®æ¦‚å¿µ"""
        # ç®€åŒ–çš„å…³é”®æ¦‚å¿µæå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæ–¹æ³•ï¼‰
        tech_keywords = [
            'API', 'api', 'ç®—æ³•', 'æ•°æ®åº“', 'ç³»ç»Ÿ', 'æ¶æ„', 'ä¼˜åŒ–',
            'æœºå™¨å­¦ä¹ ', 'ML', 'AI', 'äººå·¥æ™ºèƒ½', 'æ·±åº¦å­¦ä¹ ',
            'ç½‘ç»œ', 'çˆ¬è™«', 'æ•°æ®åˆ†æ', 'å®æ—¶', 'æ€§èƒ½', 'å®‰å…¨',
            'å¹¶å‘', 'åˆ†å¸ƒå¼', 'å¾®æœåŠ¡', 'å®¹å™¨', 'äº‘è®¡ç®—'
        ]
        
        concepts = []
        for keyword in tech_keywords:
            if keyword in text:
                concepts.append(keyword)
        
        return concepts[:3]  # è¿”å›å‰3ä¸ªå…³é”®æ¦‚å¿µ
    
    def verify_idea_feasibility(self, idea_text: str, context: Optional[Dict] = None) -> IdeaVerificationResult:
        """
        éªŒè¯æƒ³æ³•çš„å¯è¡Œæ€§
        
        Args:
            idea_text: éœ€è¦éªŒè¯çš„æƒ³æ³•æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            IdeaVerificationResult: éªŒè¯ç»“æœ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹éªŒè¯æƒ³æ³•å¯è¡Œæ€§: {idea_text[:50]}...")
            
            # æ‰§è¡Œä¸“é—¨çš„éªŒè¯æœç´¢
            search_response = self.search_for_idea_verification(idea_text, context)
            
            if not search_response.success:
                return IdeaVerificationResult(
                    idea_text=idea_text,
                    feasibility_score=0.0,
                    analysis_summary="æœç´¢å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¯è¡Œæ€§éªŒè¯",
                    search_results=[],
                    success=False,
                    error_message=search_response.error_message
                )
            
            # åˆ†ææœç´¢ç»“æœè®¡ç®—å¯è¡Œæ€§åˆ†æ•°
            feasibility_score = self._calculate_feasibility_score(search_response.results, idea_text)
            
            # ç”Ÿæˆåˆ†ææ‘˜è¦
            analysis_summary = self._generate_analysis_summary(search_response.results, idea_text, feasibility_score)
            
            logger.info(f"âœ… æƒ³æ³•éªŒè¯å®Œæˆ - å¯è¡Œæ€§åˆ†æ•°: {feasibility_score:.2f}")
            
            return IdeaVerificationResult(
                idea_text=idea_text,
                feasibility_score=feasibility_score,
                analysis_summary=analysis_summary,
                search_results=search_response.results,
                success=True,
                error_message=""
            )
            
        except Exception as e:
            logger.error(f"âŒ æƒ³æ³•éªŒè¯å¤±è´¥: {e}")
            return IdeaVerificationResult(
                idea_text=idea_text,
                feasibility_score=0.0,
                analysis_summary=f"éªŒè¯è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}",
                search_results=[],
                success=False,
                error_message=str(e)
            )
    
    def _calculate_feasibility_score(self, search_results: List[SearchResult], idea_text: str) -> float:
        """
        åŸºäºæœç´¢ç»“æœè®¡ç®—å¯è¡Œæ€§åˆ†æ•°
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            idea_text: æƒ³æ³•æ–‡æœ¬
            
        Returns:
            float: å¯è¡Œæ€§åˆ†æ•° (0.0-1.0)
        """
        if not search_results:
            return 0.1  # å¦‚æœæ²¡æœ‰æœç´¢ç»“æœï¼Œç»™ä¸€ä¸ªå¾ˆä½çš„åˆ†æ•°
        
        # å…³é”®æŒ‡æ ‡
        total_score = 0.0
        max_score = 0.0
        
        # 1. ç»“æœæ•°é‡æŒ‡æ ‡ (æƒé‡: 0.2)
        result_count_score = min(len(search_results) / 5.0, 1.0)  # 5ä¸ªç»“æœä¸ºæ»¡åˆ†
        total_score += result_count_score * 0.2
        max_score += 0.2
        
        # 2. å†…å®¹ç›¸å…³æ€§æŒ‡æ ‡ (æƒé‡: 0.4)
        key_concepts = self._extract_key_concepts(idea_text)
        relevance_scores = []
        
        for result in search_results:
            content = (result.title + " " + result.snippet).lower()
            concept_matches = sum(1 for concept in key_concepts if concept.lower() in content)
            relevance = concept_matches / max(len(key_concepts), 1)
            relevance_scores.append(relevance)
        
        if relevance_scores:
            avg_relevance = sum(relevance_scores) / len(relevance_scores)
            total_score += avg_relevance * 0.4
        max_score += 0.4
        
        # 3. å®ç°å¯èƒ½æ€§æŒ‡æ ‡ (æƒé‡: 0.3)
        implementation_keywords = [
            'å®ç°', 'æ–¹æ³•', 'æŠ€æœ¯', 'è§£å†³æ–¹æ¡ˆ', 'å¼€å‘', 'æ„å»º', 'è®¾è®¡',
            'implement', 'solution', 'method', 'approach', 'technology'
        ]
        
        implementation_scores = []
        for result in search_results:
            content = (result.title + " " + result.snippet).lower()
            keyword_matches = sum(1 for keyword in implementation_keywords if keyword in content)
            impl_score = min(keyword_matches / 3.0, 1.0)  # 3ä¸ªå…³é”®è¯ä¸ºæ»¡åˆ†
            implementation_scores.append(impl_score)
        
        if implementation_scores:
            avg_implementation = sum(implementation_scores) / len(implementation_scores)
            total_score += avg_implementation * 0.3
        max_score += 0.3
        
        # 4. é£é™©æŒ‡æ ‡ (æƒé‡: 0.1ï¼Œè´Ÿé¢å½±å“)
        risk_keywords = [
            'å›°éš¾', 'æŒ‘æˆ˜', 'é—®é¢˜', 'é£é™©', 'é™åˆ¶', 'éšœç¢',
            'difficult', 'challenge', 'problem', 'risk', 'limitation', 'obstacle'
        ]
        
        risk_scores = []
        for result in search_results:
            content = (result.title + " " + result.snippet).lower()
            risk_matches = sum(1 for keyword in risk_keywords if keyword in content)
            risk_score = min(risk_matches / 2.0, 1.0)  # é£é™©æŒ‡æ ‡ï¼Œè¶Šé«˜è¶Šä¸å¥½
            risk_scores.append(risk_score)
        
        if risk_scores:
            avg_risk = sum(risk_scores) / len(risk_scores)
            risk_penalty = avg_risk * 0.1
            total_score = max(0, total_score - risk_penalty)
        
        # å½’ä¸€åŒ–åˆ†æ•°
        final_score = total_score / max_score if max_score > 0 else 0.0
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        return max(0.0, min(1.0, final_score))
    
    def _generate_analysis_summary(self, search_results: List[SearchResult], 
                                 idea_text: str, feasibility_score: float) -> str:
        """
        ç”Ÿæˆåˆ†ææ‘˜è¦
        
        Args:
            search_results: æœç´¢ç»“æœ
            idea_text: æƒ³æ³•æ–‡æœ¬
            feasibility_score: å¯è¡Œæ€§åˆ†æ•°
            
        Returns:
            str: åˆ†ææ‘˜è¦
        """
        if not search_results:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œå¯è¡Œæ€§åˆ†æå—é™ã€‚"
        
        # æ ¹æ®å¯è¡Œæ€§åˆ†æ•°ç”ŸæˆåŸºç¡€è¯„ä¼°
        if feasibility_score >= 0.8:
            base_assessment = "è¯¥æƒ³æ³•å…·æœ‰å¾ˆé«˜çš„å¯è¡Œæ€§"
        elif feasibility_score >= 0.6:
            base_assessment = "è¯¥æƒ³æ³•å…·æœ‰è¾ƒå¥½çš„å¯è¡Œæ€§" 
        elif feasibility_score >= 0.4:
            base_assessment = "è¯¥æƒ³æ³•å…·æœ‰ä¸€å®šçš„å¯è¡Œæ€§ï¼Œä½†éœ€è¦è°¨æ…è¯„ä¼°"
        elif feasibility_score >= 0.2:
            base_assessment = "è¯¥æƒ³æ³•å¯è¡Œæ€§è¾ƒä½ï¼Œå­˜åœ¨è¾ƒå¤§æŒ‘æˆ˜"
        else:
            base_assessment = "è¯¥æƒ³æ³•å¯è¡Œæ€§å¾ˆä½ï¼Œå®ç°å›°éš¾"
        
        # æå–å…³é”®ä¿¡æ¯
        key_findings = []
        
        # ç»Ÿè®¡ç›¸å…³ç»“æœæ•°é‡
        key_findings.append(f"æœç´¢åˆ°{len(search_results)}ä¸ªç›¸å…³ç»“æœ")
        
        # åˆ†ææŠ€æœ¯å…³é”®è¯
        tech_keywords = self._extract_key_concepts(idea_text)
        if tech_keywords:
            key_findings.append(f"æ¶‰åŠæŠ€æœ¯é¢†åŸŸ: {', '.join(tech_keywords[:3])}")
        
        # åˆ†ææœç´¢ç»“æœè´¨é‡
        if search_results:
            has_detailed_content = sum(1 for r in search_results if len(r.snippet) > 50)
            if has_detailed_content >= len(search_results) * 0.6:
                key_findings.append("æ‰¾åˆ°äº†è¯¦ç»†çš„æŠ€æœ¯èµ„æ–™")
            else:
                key_findings.append("ç›¸å…³èµ„æ–™æœ‰é™")
        
        # æ„å»ºå®Œæ•´æ‘˜è¦
        summary_parts = [base_assessment]
        
        if key_findings:
            summary_parts.append("ã€‚åˆ†æå‘ç°ï¼š" + "ï¼Œ".join(key_findings))
        
        summary_parts.append(f"ã€‚ç»¼åˆè¯„ä¼°å¯è¡Œæ€§å¾—åˆ†ä¸º{feasibility_score:.1f}/1.0ã€‚")
        
        return "".join(summary_parts)