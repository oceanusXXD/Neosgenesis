#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ€§èƒ½ä¼˜åŒ–å·¥å…·ç±» - Performance Optimization Utils
å®ç°å¹¶è¡ŒåŒ–ã€ç¼“å­˜ã€è‡ªé€‚åº”ç®—æ³•ç­‰æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½
"""

import asyncio
import time
import hashlib
import logging
from typing import Dict, List, Optional, Any, Callable, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from collections import defaultdict

logger = logging.getLogger(__name__)


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    data: Any
    timestamp: float
    access_count: int = 0
    ttl: float = 3600  # é»˜è®¤1å°æ—¶è¿‡æœŸ
    
    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        return time.time() - self.timestamp > self.ttl
    
    def access(self):
        """è®°å½•è®¿é—®"""
        self.access_count += 1


class IntelligentCache:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, default_ttl: float = 3600, max_size: int = 1000):
        """
        åˆå§‹åŒ–æ™ºèƒ½ç¼“å­˜
        
        Args:
            default_ttl: é»˜è®¤è¿‡æœŸæ—¶é—´(ç§’)
            max_size: æœ€å¤§ç¼“å­˜å¤§å°
        """
        self.cache: Dict[str, CacheEntry] = {}
        self.default_ttl = default_ttl
        self.max_size = max_size
        self.stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0
        }
        
        logger.info(f"ğŸ§  æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ– - TTL:{default_ttl}s, æœ€å¤§å®¹é‡:{max_size}")
    
    def _generate_cache_key(self, query: str, context: Optional[Dict] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{query}_{str(context) if context else ''}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, query: str, context: Optional[Dict] = None) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_cache_key(query, context)
        
        if cache_key in self.cache:
            entry = self.cache[cache_key]
            
            if entry.is_expired():
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self.cache[cache_key]
                self.stats['misses'] += 1
                logger.debug(f"ğŸ“‹ ç¼“å­˜è¿‡æœŸ: {cache_key[:16]}...")
                return None
            
            # ç¼“å­˜å‘½ä¸­
            entry.access()
            self.stats['hits'] += 1
            logger.debug(f"âœ… ç¼“å­˜å‘½ä¸­: {cache_key[:16]}... (è®¿é—®æ¬¡æ•°: {entry.access_count})")
            return entry.data
        
        # ç¼“å­˜æœªå‘½ä¸­
        self.stats['misses'] += 1
        logger.debug(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {cache_key[:16]}...")
        return None
    
    def set(self, query: str, data: Any, context: Optional[Dict] = None, ttl: Optional[float] = None):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_cache_key(query, context)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†ç¼“å­˜
        if len(self.cache) >= self.max_size:
            self._evict_least_used()
        
        # åˆ›å»ºç¼“å­˜æ¡ç›®
        entry = CacheEntry(
            data=data,
            timestamp=time.time(),
            ttl=ttl or self.default_ttl
        )
        
        self.cache[cache_key] = entry
        logger.debug(f"ğŸ’¾ ç¼“å­˜å·²è®¾ç½®: {cache_key[:16]}... (TTL: {entry.ttl}s)")
    
    def _evict_least_used(self):
        """æ¸…ç†æœ€å°‘ä½¿ç”¨çš„ç¼“å­˜æ¡ç›®"""
        if not self.cache:
            return
        
        # æ‰¾åˆ°è®¿é—®æ¬¡æ•°æœ€å°‘çš„æ¡ç›®
        least_used_key = min(self.cache.keys(), 
                           key=lambda k: self.cache[k].access_count)
        
        del self.cache[least_used_key]
        self.stats['evictions'] += 1
        logger.debug(f"ğŸ—‘ï¸ ç¼“å­˜æ¸…ç†: {least_used_key[:16]}...")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.stats['hits'] + self.stats['misses']
        hit_rate = self.stats['hits'] / max(total_requests, 1)
        
        return {
            **self.stats,
            'total_requests': total_requests,
            'hit_rate': hit_rate,
            'cache_size': len(self.cache)
        }
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        logger.info("ğŸ§¹ ç¼“å­˜å·²æ¸…ç©º")


class ParallelPathVerifier:
    """å¹¶è¡Œè·¯å¾„éªŒè¯å™¨"""
    
    def __init__(self, max_workers: int = 3):
        """
        åˆå§‹åŒ–å¹¶è¡ŒéªŒè¯å™¨
        
        Args:
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
        """
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        logger.info(f"âš¡ å¹¶è¡Œè·¯å¾„éªŒè¯å™¨åˆå§‹åŒ– - æœ€å¤§å¹¶å‘æ•°: {max_workers}")
    
    def verify_paths_parallel(self, verification_tasks: List[Tuple[Any, Callable]]) -> List[Any]:
        """
        å¹¶è¡ŒéªŒè¯å¤šæ¡è·¯å¾„
        
        Args:
            verification_tasks: éªŒè¯ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡æ˜¯(è·¯å¾„, éªŒè¯å‡½æ•°)çš„å…ƒç»„
            
        Returns:
            éªŒè¯ç»“æœåˆ—è¡¨
        """
        if not verification_tasks:
            return []
        
        logger.info(f"ğŸ”„ å¼€å§‹å¹¶è¡ŒéªŒè¯ {len(verification_tasks)} æ¡è·¯å¾„...")
        start_time = time.time()
        
        # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
        future_to_path = {}
        for path, verify_func in verification_tasks:
            future = self.executor.submit(verify_func, path)
            future_to_path[future] = path
        
        # æ”¶é›†ç»“æœ
        results = []
        completed_count = 0
        
        for future in as_completed(future_to_path):
            path = future_to_path[future]
            try:
                result = future.result()
                results.append(result)
                completed_count += 1
                
                logger.debug(f"âœ… è·¯å¾„éªŒè¯å®Œæˆ ({completed_count}/{len(verification_tasks)}): {path}")
                
            except Exception as e:
                logger.error(f"âŒ è·¯å¾„éªŒè¯å¤±è´¥: {path} - {e}")
                # æ·»åŠ å¤±è´¥ç»“æœï¼Œä¿æŒç»“æœæ•°é‡ä¸€è‡´
                results.append(None)
        
        duration = time.time() - start_time
        logger.info(f"ğŸ¯ å¹¶è¡ŒéªŒè¯å®Œæˆ - è€—æ—¶: {duration:.2f}s, æˆåŠŸ: {len([r for r in results if r is not None])}/{len(verification_tasks)}")
        
        return results
    
    def shutdown(self):
        """å…³é—­çº¿ç¨‹æ± """
        self.executor.shutdown(wait=True)
        logger.info("ğŸ”š å¹¶è¡ŒéªŒè¯å™¨å·²å…³é—­")


class AdaptivePathSelector:
    """è‡ªé€‚åº”è·¯å¾„é€‰æ‹©å™¨"""
    
    def __init__(self, performance_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”è·¯å¾„é€‰æ‹©å™¨
        
        Args:
            performance_config: æ€§èƒ½é…ç½®
        """
        self.config = performance_config
        self.path_performance_history = defaultdict(list)
        
        logger.info("ğŸ¯ è‡ªé€‚åº”è·¯å¾„é€‰æ‹©å™¨åˆå§‹åŒ–")
    
    def get_optimal_path_count(self, confidence: float, complexity: float) -> int:
        """
        åŸºäºç½®ä¿¡åº¦å’Œå¤æ‚åº¦ç¡®å®šæœ€ä¼˜è·¯å¾„æ•°é‡
        
        Args:
            confidence: ç½®ä¿¡åº¦ (0.0-1.0)
            complexity: å¤æ‚åº¦ (0.0-1.0)
            
        Returns:
            æœ€ä¼˜è·¯å¾„æ•°é‡
        """
        if not self.config.get("enable_adaptive_path_count", False):
            return self.config.get("max_verification_paths", 6)
        
        # åŸºäºç½®ä¿¡åº¦çš„æ˜ å°„
        confidence_mapping = self.config.get("confidence_path_mapping", {})
        
        # æ‰¾åˆ°åˆé€‚çš„è·¯å¾„æ•°é‡
        path_count = self.config.get("max_verification_paths", 6)
        for conf_threshold in sorted(confidence_mapping.keys(), reverse=True):
            if confidence >= conf_threshold:
                path_count = confidence_mapping[conf_threshold]
                break
        
        # å¤æ‚åº¦è°ƒæ•´
        if complexity > 0.8:
            path_count = min(path_count + 1, self.config.get("max_verification_paths", 6))
        elif complexity < 0.3:
            path_count = max(path_count - 1, self.config.get("min_verification_paths", 2))
        
        logger.info(f"ğŸ¯ è‡ªé€‚åº”è·¯å¾„é€‰æ‹©: ç½®ä¿¡åº¦={confidence:.2f}, å¤æ‚åº¦={complexity:.2f} -> {path_count}æ¡è·¯å¾„")
        
        return path_count
    
    def should_early_terminate(self, verified_results: List[Any], min_consistent: int = 3) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥æ—©æœŸç»ˆæ­¢éªŒè¯
        
        Args:
            verified_results: å·²éªŒè¯çš„ç»“æœåˆ—è¡¨
            min_consistent: æœ€å°ä¸€è‡´æ€§ç»“æœæ•°
            
        Returns:
            æ˜¯å¦åº”è¯¥æ—©æœŸç»ˆæ­¢
        """
        if not self.config.get("enable_early_termination", False):
            return False
        
        if len(verified_results) < min_consistent:
            return False
        
        # æ£€æŸ¥ç»“æœä¸€è‡´æ€§
        consistency_threshold = self.config.get("path_consistency_threshold", 0.8)
        
        # ç®€å•ä¸€è‡´æ€§æ£€æŸ¥ï¼šè®¡ç®—æˆåŠŸ/å¤±è´¥çš„æ¯”ä¾‹
        success_count = sum(1 for result in verified_results 
                          if result and getattr(result, 'success', False))
        
        success_rate = success_count / len(verified_results)
        
        # å¦‚æœä¸€è‡´æ€§è¶³å¤Ÿé«˜ï¼Œå¯ä»¥æ—©æœŸç»ˆæ­¢
        if success_rate >= consistency_threshold or success_rate <= (1 - consistency_threshold):
            logger.info(f"ğŸ”„ æ—©æœŸç»ˆæ­¢æ¡ä»¶æ»¡è¶³: æˆåŠŸç‡={success_rate:.2f}, å·²éªŒè¯={len(verified_results)}æ¡è·¯å¾„")
            return True
        
        return False
    
    def record_path_performance(self, path_id: str, performance_score: float):
        """è®°å½•è·¯å¾„æ€§èƒ½"""
        self.path_performance_history[path_id].append(performance_score)
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(self.path_performance_history[path_id]) > 50:
            self.path_performance_history[path_id] = self.path_performance_history[path_id][-25:]


class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨ä¸»ç±»"""
    
    def __init__(self, performance_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨
        
        Args:
            performance_config: æ€§èƒ½é…ç½®
        """
        self.config = performance_config
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.cache = IntelligentCache(
            default_ttl=performance_config.get("cache_ttl_seconds", 3600),
            max_size=performance_config.get("cache_max_size", 1000)
        )
        
        self.parallel_verifier = ParallelPathVerifier(
            max_workers=performance_config.get("max_concurrent_verifications", 3)
        )
        
        self.adaptive_selector = AdaptivePathSelector(performance_config)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'optimization_enabled_time': time.time(),
            'total_optimized_decisions': 0,
            'time_saved_seconds': 0,
            'cache_hits': 0,
            'parallel_speedup_factor': 0
        }
        
        logger.info("ğŸš€ æ€§èƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        cache_stats = self.cache.get_stats()
        
        return {
            'optimization_stats': self.performance_stats,
            'cache_stats': cache_stats,
            'config': self.config,
            'uptime_hours': (time.time() - self.performance_stats['optimization_enabled_time']) / 3600
        }
    
    def shutdown(self):
        """å…³é—­ä¼˜åŒ–å™¨"""
        self.parallel_verifier.shutdown()
        logger.info("ğŸ”š æ€§èƒ½ä¼˜åŒ–å™¨å·²å…³é—­")