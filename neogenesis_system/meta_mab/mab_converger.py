#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
MABæ”¶æ•›å™¨ - é˜¶æ®µä¸‰ï¼šæ€ç»´è·¯å¾„é€‰æ‹©å™¨
è´Ÿè´£ä»å¤šä¸ªæ€ç»´è·¯å¾„ä¸­é€‰æ‹©æœ€ä¼˜è·¯å¾„çš„å¤šè‡‚è€è™æœºç®—æ³•
MAB Converger - Stage 3: Reasoning Path Selector
Responsible for selecting optimal reasoning path from multiple paths using MAB algorithms
"""

import time
import logging
import numpy as np
from typing import Dict, List, Optional
from collections import defaultdict
from dataclasses import dataclass

from .data_structures import EnhancedDecisionArm, ReasoningPath
try:
    from neogenesis_system.config import MAB_CONFIG
except ImportError:
    try:
        from ..config import MAB_CONFIG
    except ImportError:
        MAB_CONFIG = {
            "convergence_threshold": 0.95,
            "min_samples": 10
        }

logger = logging.getLogger(__name__)


@dataclass
class MABConverger:
    """MABæ”¶æ•›å™¨ - é˜¶æ®µä¸‰ï¼šæ€ç»´è·¯å¾„é€‰æ‹©å™¨"""
    
    def __init__(self):
        # æ”¹ä¸ºå­˜å‚¨è·¯å¾„çº§åˆ«çš„å†³ç­–è‡‚ï¼špath_id -> EnhancedDecisionArm
        self.path_arms: Dict[str, EnhancedDecisionArm] = {}
        self.convergence_threshold = MAB_CONFIG["convergence_threshold"]  # æ”¶æ•›é˜ˆå€¼
        self.min_samples = MAB_CONFIG["min_samples"]  # æœ€å°æ ·æœ¬æ•°
        
        # ğŸ”§ æ–°å¢ï¼šå·¥å…·çº§åˆ«çš„å†³ç­–è‡‚å­˜å‚¨ï¼štool_id -> EnhancedDecisionArm
        self.tool_arms: Dict[str, EnhancedDecisionArm] = {}
        self.tool_selection_history = []  # å·¥å…·é€‰æ‹©å†å²
        self.total_tool_selections = 0  # æ€»å·¥å…·é€‰æ‹©æ¬¡æ•°
        
        # ç®—æ³•é€‰æ‹©ç­–ç•¥
        self.algorithm_preferences = {
            'thompson_sampling': 0.4,
            'ucb_variant': 0.35,
            'epsilon_greedy': 0.25
        }
        
        # è·¯å¾„çº§åˆ«çš„æ€§èƒ½ç»Ÿè®¡
        self.algorithm_performance = defaultdict(lambda: {'successes': 0, 'total': 0})
        self.path_selection_history = []  # è·¯å¾„é€‰æ‹©å†å²
        self.total_path_selections = 0  # æ€»è·¯å¾„é€‰æ‹©æ¬¡æ•°
        
        # ğŸ”§ æ–°å¢ï¼šå·¥å…·çº§åˆ«çš„æ€§èƒ½ç»Ÿè®¡
        self.tool_algorithm_performance = defaultdict(lambda: {'successes': 0, 'total': 0})
        
        # ğŸ† é»„é‡‘å†³ç­–æ¨¡æ¿ç³»ç»Ÿ
        self.golden_templates: Dict[str, Dict[str, any]] = {}  # å­˜å‚¨é»„é‡‘æ¨¡æ¿
        self.golden_template_config = {
            'success_rate_threshold': 0.90,  # æˆåŠŸç‡é˜ˆå€¼90%
            'min_samples_required': 20,      # æœ€å°æ ·æœ¬æ•°20æ¬¡
            'confidence_threshold': 0.95,    # ç½®ä¿¡åº¦é˜ˆå€¼
            'stability_check_window': 10,    # ç¨³å®šæ€§æ£€æŸ¥çª—å£
            'max_golden_templates': 50       # æœ€å¤§é»„é‡‘æ¨¡æ¿æ•°é‡
        }
        self.template_usage_stats = defaultdict(int)  # é»„é‡‘æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡
        self.template_match_history = []  # æ¨¡æ¿åŒ¹é…å†å²
        
        # ğŸ”§ æ”¹è¿›æ–¹æ¡ˆï¼šé‡‡ç”¨åŠ¨æ€åˆ›å»ºç­–ç•¥ï¼Œåœ¨éœ€è¦æ—¶è‡ªåŠ¨åˆ›å»ºå†³ç­–è‡‚
        
        logger.info("ğŸ° MABConverger å·²åˆå§‹åŒ– - åŒå±‚å­¦ä¹ æ¨¡å¼ï¼šæ€ç»´è·¯å¾„ + å·¥å…·é€‰æ‹©")
        logger.info("ğŸ† é»„é‡‘å†³ç­–æ¨¡æ¿ç³»ç»Ÿå·²å¯ç”¨")
        logger.info("ğŸ”§ å·¥å…·é€‰æ‹©MABç³»ç»Ÿå·²å°±ç»ª")
    
    def _create_strategy_arm_if_missing(self, strategy_id: str, path_type: str = None) -> EnhancedDecisionArm:
        """
        åŠ¨æ€åˆ›å»ºç­–ç•¥å†³ç­–è‡‚ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        
        Args:
            strategy_id: ç­–ç•¥ID
            path_type: è·¯å¾„ç±»å‹ï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨æ¨æ–­ï¼‰
            
        Returns:
            å¯¹åº”çš„å†³ç­–è‡‚
        """
        if strategy_id not in self.path_arms:
            if path_type is None:
                # è‡ªåŠ¨æ¨æ–­è·¯å¾„ç±»å‹
                path_type = self._infer_path_type_from_strategy_id(strategy_id)
            
            self.path_arms[strategy_id] = EnhancedDecisionArm(
                path_id=strategy_id,
                option=path_type
            )
            logger.debug(f"ğŸ†• åŠ¨æ€åˆ›å»ºç­–ç•¥å†³ç­–è‡‚: {strategy_id} ({path_type})")
        
        return self.path_arms[strategy_id]
    
    def _create_tool_arm_if_missing(self, tool_id: str, tool_name: str = None) -> EnhancedDecisionArm:
        """
        åŠ¨æ€åˆ›å»ºå·¥å…·å†³ç­–è‡‚ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        
        Args:
            tool_id: å·¥å…·ID
            tool_name: å·¥å…·åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨tool_idï¼‰
            
        Returns:
            å¯¹åº”çš„å·¥å…·å†³ç­–è‡‚
        """
        if tool_id not in self.tool_arms:
            if tool_name is None:
                tool_name = tool_id  # é»˜è®¤ä½¿ç”¨tool_idä½œä¸ºå·¥å…·åç§°
            
            self.tool_arms[tool_id] = EnhancedDecisionArm(
                path_id=tool_id,
                option=tool_name
            )
            logger.debug(f"ğŸ”§ åŠ¨æ€åˆ›å»ºå·¥å…·å†³ç­–è‡‚: {tool_id} ({tool_name})")
        
        return self.tool_arms[tool_id]
    
    def select_best_path(self, paths: List[ReasoningPath], algorithm: str = 'auto') -> ReasoningPath:
        """
        é˜¶æ®µä¸‰æ ¸å¿ƒæ–¹æ³•ï¼šä»æ€ç»´è·¯å¾„åˆ—è¡¨ä¸­é€‰æ‹©æœ€ä¼˜è·¯å¾„ï¼ˆé›†æˆé»„é‡‘æ¨¡æ¿ç³»ç»Ÿï¼‰
        
        Args:
            paths: æ€ç»´è·¯å¾„åˆ—è¡¨
            algorithm: ä½¿ç”¨çš„ç®—æ³• ('thompson_sampling', 'ucb_variant', 'epsilon_greedy', 'auto')
            
        Returns:
            é€‰æ‹©çš„æœ€ä¼˜æ€ç»´è·¯å¾„
        """
        if not paths:
            raise ValueError("è·¯å¾„åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if len(paths) == 1:
            logger.info(f"ğŸ¯ åªæœ‰ä¸€ä¸ªè·¯å¾„ï¼Œç›´æ¥é€‰æ‹©: {paths[0].path_type}")
            return paths[0]
        
        self.total_path_selections += 1
        logger.info(f"ğŸ›¤ï¸ å¼€å§‹ç¬¬ {self.total_path_selections} æ¬¡è·¯å¾„é€‰æ‹©ï¼Œå€™é€‰è·¯å¾„: {len(paths)}ä¸ª")
        
        # ğŸ† é»„é‡‘æ¨¡æ¿ä¼˜å…ˆæ£€æŸ¥ï¼šåœ¨MABç®—æ³•å‰å…ˆæ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„é»„é‡‘æ¨¡æ¿
        golden_match = self._check_golden_template_match(paths)
        if golden_match:
            selected_path = golden_match['path']
            template_id = golden_match['template_id']
            match_score = golden_match['match_score']
            
            # æ›´æ–°é»„é‡‘æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡
            self.template_usage_stats[template_id] += 1
            
            # è®°å½•æ¨¡æ¿åŒ¹é…å†å²
            self.template_match_history.append({
                'template_id': template_id,
                'path_id': selected_path.path_id,
                'path_type': selected_path.path_type,
                'match_score': match_score,
                'timestamp': time.time(),
                'selection_round': self.total_path_selections
            })
            
            logger.info(f"ğŸ† é»„é‡‘æ¨¡æ¿åŒ¹é…æˆåŠŸï¼")
            logger.info(f"   æ¨¡æ¿ID: {template_id}")
            logger.info(f"   åŒ¹é…è·¯å¾„: {selected_path.path_type}")
            logger.info(f"   åŒ¹é…åˆ†æ•°: {match_score:.3f}")
            logger.info(f"   è·³è¿‡MABç®—æ³•ï¼Œç›´æ¥ä½¿ç”¨é»„é‡‘æ¨¡æ¿")
            
            return selected_path
        
        # ğŸ”§ åŠ¨æ€åˆ›å»ºç­–ç•¥ï¼šåœ¨é€‰æ‹©è·¯å¾„æ—¶ç¡®ä¿æ‰€æœ‰ç­–ç•¥å†³ç­–è‡‚éƒ½å­˜åœ¨
        available_arms = []
        strategy_to_path_mapping = {}  # ç­–ç•¥IDåˆ°è·¯å¾„å®ä¾‹çš„æ˜ å°„
        
        for path in paths:
            strategy_id = path.strategy_id
            strategy_to_path_mapping[strategy_id] = path  # è®°å½•æ˜ å°„å…³ç³»
            
            # ğŸ”§ åŠ¨æ€åˆ›å»ºï¼šç¡®ä¿ç­–ç•¥å†³ç­–è‡‚å­˜åœ¨
            arm = self._create_strategy_arm_if_missing(strategy_id, path.path_type)
            available_arms.append(arm)
            
            logger.debug(f"âœ… ç­–ç•¥å†³ç­–è‡‚å°±ç»ª: {strategy_id} ({path.path_type})")
            logger.debug(f"   å¯¹åº”å®ä¾‹: {path.instance_id}")
        
        # è‡ªåŠ¨é€‰æ‹©ç®—æ³•
        if algorithm == 'auto':
            algorithm = self._select_best_algorithm_for_paths()
        
        # æ ¹æ®é€‰æ‹©çš„ç®—æ³•è¿›è¡Œå†³ç­–
        try:
            if algorithm == 'thompson_sampling':
                best_arm = self._thompson_sampling_for_paths(available_arms)
            elif algorithm == 'ucb_variant':
                best_arm = self._ucb_variant_for_paths(available_arms)
            elif algorithm == 'epsilon_greedy':
                best_arm = self._epsilon_greedy_for_paths(available_arms)
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥ç®—æ³• {algorithm}ï¼Œä½¿ç”¨Thompsoné‡‡æ ·")
                best_arm = self._thompson_sampling_for_paths(available_arms)
            
            # æ›´æ–°ä½¿ç”¨æ—¶é—´å’Œæ¿€æ´»æ¬¡æ•°
            best_arm.last_used = time.time()
            best_arm.activation_count += 1
            
            # ğŸ¯ ä¿®å¤ï¼šåŸºäºç­–ç•¥IDæ‰¾åˆ°å¯¹åº”çš„è·¯å¾„å®ä¾‹
            selected_path = strategy_to_path_mapping.get(best_arm.path_id)
            
            if selected_path is None:
                # å…¼å®¹æ€§ï¼šå¦‚æœæ˜ å°„å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹å¼
                logger.warning(f"âš ï¸ ç­–ç•¥æ˜ å°„å¤±è´¥: {best_arm.path_id}")
                for path in paths:
                    strategy_id = getattr(path, 'strategy_id', None)
                    if strategy_id == best_arm.path_id:
                        selected_path = path
                        break
                
                if selected_path is None:
                    logger.error(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„è·¯å¾„ç­–ç•¥: {best_arm.path_id}")
                    selected_path = paths[0]  # å›é€€åˆ°ç¬¬ä¸€ä¸ªè·¯å¾„
            
            # è®°å½•é€‰æ‹©å†å²
            self.path_selection_history.append({
                'path_id': best_arm.path_id,
                'path_type': selected_path.path_type,
                'algorithm': algorithm,
                'timestamp': time.time(),
                'selection_round': self.total_path_selections
            })
            
            logger.info(f"ğŸ¯ ä½¿ç”¨ {algorithm} é€‰æ‹©è·¯å¾„: {selected_path.path_type} (ID: {best_arm.path_id})")
            return selected_path
            
        except Exception as e:
            logger.error(f"âŒ MABè·¯å¾„é€‰æ‹©ç®—æ³•æ‰§è¡Œå¤±è´¥: {e}")
            # å›é€€åˆ°éšæœºé€‰æ‹©
            selected_path = np.random.choice(paths)
            logger.info(f"ğŸ”„ å›é€€åˆ°éšæœºé€‰æ‹©è·¯å¾„: {selected_path.path_type}")
            return selected_path
    
    def select_best_tool(self, available_tools: List[str], algorithm: str = 'auto') -> str:
        """
        ğŸ”§ æ–°å¢ï¼šä»å¯ç”¨å·¥å…·åˆ—è¡¨ä¸­é€‰æ‹©æœ€ä¼˜å·¥å…·
        
        Args:
            available_tools: å¯ç”¨å·¥å…·åç§°åˆ—è¡¨
            algorithm: ä½¿ç”¨çš„ç®—æ³• ('thompson_sampling', 'ucb_variant', 'epsilon_greedy', 'auto')
            
        Returns:
            é€‰æ‹©çš„æœ€ä¼˜å·¥å…·åç§°
        """
        if not available_tools:
            raise ValueError("å·¥å…·åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if len(available_tools) == 1:
            logger.info(f"ğŸ”§ åªæœ‰ä¸€ä¸ªå·¥å…·ï¼Œç›´æ¥é€‰æ‹©: {available_tools[0]}")
            return available_tools[0]
        
        self.total_tool_selections += 1
        logger.info(f"ğŸ”§ å¼€å§‹ç¬¬ {self.total_tool_selections} æ¬¡å·¥å…·é€‰æ‹©ï¼Œå€™é€‰å·¥å…·: {len(available_tools)}ä¸ª")
        
        # ğŸ”§ åŠ¨æ€åˆ›å»ºï¼šç¡®ä¿æ‰€æœ‰å·¥å…·çš„å†³ç­–è‡‚éƒ½å­˜åœ¨
        available_arms = []
        tool_to_arm_mapping = {}  # å·¥å…·åç§°åˆ°å†³ç­–è‡‚çš„æ˜ å°„
        
        for tool_name in available_tools:
            tool_id = tool_name  # ä½¿ç”¨å·¥å…·åç§°ä½œä¸ºID
            tool_to_arm_mapping[tool_name] = tool_id
            
            # ğŸ”§ åŠ¨æ€åˆ›å»ºï¼šç¡®ä¿å·¥å…·å†³ç­–è‡‚å­˜åœ¨
            arm = self._create_tool_arm_if_missing(tool_id, tool_name)
            available_arms.append(arm)
            
            logger.debug(f"âœ… å·¥å…·å†³ç­–è‡‚å°±ç»ª: {tool_id} ({tool_name})")
        
        # è‡ªåŠ¨é€‰æ‹©ç®—æ³•
        if algorithm == 'auto':
            algorithm = self._select_best_algorithm_for_tools()
        
        # æ ¹æ®é€‰æ‹©çš„ç®—æ³•è¿›è¡Œå†³ç­–
        try:
            if algorithm == 'thompson_sampling':
                best_arm = self._thompson_sampling_for_tools(available_arms)
            elif algorithm == 'ucb_variant':
                best_arm = self._ucb_variant_for_tools(available_arms)
            elif algorithm == 'epsilon_greedy':
                best_arm = self._epsilon_greedy_for_tools(available_arms)
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥ç®—æ³• {algorithm}ï¼Œä½¿ç”¨Thompsoné‡‡æ ·")
                best_arm = self._thompson_sampling_for_tools(available_arms)
            
            # æ›´æ–°ä½¿ç”¨æ—¶é—´å’Œæ¿€æ´»æ¬¡æ•°
            best_arm.last_used = time.time()
            best_arm.activation_count += 1
            
            # ğŸ¯ æ‰¾åˆ°å¯¹åº”çš„å·¥å…·åç§°
            selected_tool = best_arm.option  # å·¥å…·åç§°å­˜å‚¨åœ¨optionå­—æ®µä¸­
            
            # è®°å½•é€‰æ‹©å†å²
            self.tool_selection_history.append({
                'tool_id': best_arm.path_id,
                'tool_name': selected_tool,
                'algorithm': algorithm,
                'timestamp': time.time(),
                'selection_round': self.total_tool_selections
            })
            
            logger.info(f"ğŸ”§ ä½¿ç”¨ {algorithm} é€‰æ‹©å·¥å…·: {selected_tool} (ID: {best_arm.path_id})")
            return selected_tool
            
        except Exception as e:
            logger.error(f"âŒ MABå·¥å…·é€‰æ‹©ç®—æ³•æ‰§è¡Œå¤±è´¥: {e}")
            # å›é€€åˆ°éšæœºé€‰æ‹©
            selected_tool = np.random.choice(available_tools)
            logger.info(f"ğŸ”„ å›é€€åˆ°éšæœºé€‰æ‹©å·¥å…·: {selected_tool}")
            return selected_tool
    
    def is_tool_cold(self, tool_name: str) -> Dict[str, any]:
        """
        ğŸ” åˆ¤æ–­å·¥å…·æ˜¯å¦å¤„äºå†·å¯åŠ¨çŠ¶æ€
        
        è¿™ä¸ªæ–¹æ³•æ˜¯MABConvergerçš„"è‡ªæˆ‘è®¤çŸ¥"èƒ½åŠ›ï¼Œå½“MainControllerè¯¢é—®æ—¶ï¼Œ
        å®ƒèƒ½æ˜ç¡®å›ç­”ï¼š"æˆ‘æ¨èçš„è¿™ä¸ªå·¥å…·ï¼Œæˆ‘è‡ªå·±ç†Ÿä¸ç†Ÿï¼Ÿ"
        
        Args:
            tool_name: å·¥å…·åç§°
            
        Returns:
            DictåŒ…å«è¯¦ç»†çš„å†·å¯åŠ¨åˆ†æç»“æœ:
            {
                'is_cold_start': bool,      # æ˜¯å¦å¤„äºå†·å¯åŠ¨çŠ¶æ€
                'cold_score': float,        # å†·å¯åŠ¨å¾—åˆ† (0-1, è¶Šé«˜è¶Š"å†·")
                'confidence': float,        # ç»éªŒå¯ä¿¡åº¦ (0-1, è¶Šé«˜è¶Šå¯ä¿¡)
                'analysis': {
                    'usage_count': int,     # ä½¿ç”¨æ¬¡æ•°
                    'reliability_score': float,  # å¯é æ€§åˆ†æ•°
                    'idle_hours': float,    # ç©ºé—²æ—¶é—´(å°æ—¶)
                    'sample_size': int      # æ ·æœ¬æ•°é‡
                },
                'recommendation': str,      # æ¨èæ¨¡å¼ ('experience'/'exploration')
                'reason': str              # åˆ¤æ–­ç†ç”±
            }
        """
        logger.debug(f"ğŸ” å¼€å§‹å†·å¯åŠ¨æ£€æµ‹: å·¥å…· '{tool_name}'")
        
        # è·å–å†·å¯åŠ¨é…ç½®
        cold_start_config = MAB_CONFIG["cold_start_threshold"]
        detection_weights = cold_start_config["detection_weights"]
        
        # è·å–å·¥å…·çš„å†³ç­–è‡‚
        tool_arm = self.tool_arms.get(tool_name)
        
        if not tool_arm:
            # å®Œå…¨æœªä½¿ç”¨çš„å·¥å…· - ç»å¯¹å†·å¯åŠ¨
            logger.debug(f"ğŸ†• å·¥å…· '{tool_name}' ä»æœªä½¿ç”¨è¿‡ï¼Œåˆ¤å®šä¸ºå†·å¯åŠ¨")
            return {
                'is_cold_start': True,
                'cold_score': 1.0,
                'confidence': 0.0,
                'analysis': {
                    'usage_count': 0,
                    'reliability_score': 0.0,
                    'idle_hours': float('inf'),
                    'sample_size': 0
                },
                'recommendation': 'exploration',
                'reason': 'å·¥å…·ä»æœªè¢«ä½¿ç”¨è¿‡ï¼Œæ— ä»»ä½•ç»éªŒæ•°æ®'
            }
        
        # è®¡ç®—å„ä¸ªå†·å¯åŠ¨å› å­
        analysis = self._calculate_cold_start_factors(tool_arm, cold_start_config)
        
        # è®¡ç®—åŠ æƒå†·å¯åŠ¨å¾—åˆ†
        cold_score = (
            analysis['usage_factor'] * detection_weights['usage_frequency'] +
            analysis['reliability_factor'] * detection_weights['reliability'] +
            analysis['recency_factor'] * detection_weights['recency'] +
            analysis['sample_factor'] * detection_weights['sample_sufficiency']
        )
        
        # åˆ¤å®šæ˜¯å¦å†·å¯åŠ¨
        exploration_threshold = cold_start_config["exploration_trigger_threshold"]
        is_cold = cold_score > exploration_threshold
        
        # ç”Ÿæˆåˆ¤æ–­ç†ç”±
        reason = self._generate_cold_start_reason(analysis, cold_score, exploration_threshold)
        
        result = {
            'is_cold_start': is_cold,
            'cold_score': round(cold_score, 3),
            'confidence': round(1.0 - cold_score, 3),
            'analysis': {
                'usage_count': analysis['usage_count'],
                'reliability_score': round(analysis['reliability_score'], 3),
                'idle_hours': round(analysis['idle_hours'], 2),
                'sample_size': analysis['sample_size']
            },
            'recommendation': 'exploration' if is_cold else 'experience',
            'reason': reason
        }
        
        logger.info(f"ğŸ” å†·å¯åŠ¨æ£€æµ‹å®Œæˆ: {tool_name} -> "
                   f"{'å†·å¯åŠ¨' if is_cold else 'ç»éªŒä¸°å¯Œ'} "
                   f"(å¾—åˆ†: {cold_score:.3f}, ç½®ä¿¡åº¦: {result['confidence']:.3f})")
        
        return result
    
    def _calculate_cold_start_factors(self, tool_arm: EnhancedDecisionArm, 
                                    cold_start_config: Dict[str, any]) -> Dict[str, any]:
        """
        è®¡ç®—å†·å¯åŠ¨å„ä¸ªå› å­
        
        Args:
            tool_arm: å·¥å…·å†³ç­–è‡‚
            cold_start_config: å†·å¯åŠ¨é…ç½®
            
        Returns:
            åŒ…å«å„ä¸ªå› å­çš„åˆ†æç»“æœ
        """
        current_time = time.time()
        
        # 1. ä½¿ç”¨é¢‘ç‡å› å­ (ä½¿ç”¨æ¬¡æ•°è¶Šå°‘ï¼Œåˆ†æ•°è¶Šé«˜)
        usage_count = tool_arm.activation_count
        min_usage = cold_start_config["min_usage_count"]
        usage_factor = max(0.0, 1.0 - usage_count / max(min_usage, 1))
        
        # 2. å¯é æ€§å› å­ (æˆåŠŸç‡ä¸ç¨³å®šæˆ–æ ·æœ¬å°‘æ—¶åˆ†æ•°é«˜)
        total_samples = tool_arm.success_count + tool_arm.failure_count
        if total_samples >= 3:
            reliability_score = tool_arm.success_rate
            # æ ·æœ¬æ•°è°ƒæ•´ï¼šæ ·æœ¬è¶Šå°‘ï¼Œå¯é æ€§è¶Šä½
            sample_adjustment = min(1.0, total_samples / 10.0)  # 10ä¸ªæ ·æœ¬è§†ä¸ºå……è¶³
            adjusted_reliability = reliability_score * sample_adjustment
        else:
            adjusted_reliability = 0.0  # æ ·æœ¬å¤ªå°‘ï¼Œä¸å¯é 
        
        min_reliability = cold_start_config["min_reliability_score"]
        reliability_factor = max(0.0, 1.0 - adjusted_reliability / max(min_reliability, 0.1))
        
        # 3. æœ€è¿‘ä½¿ç”¨å› å­ (æ—¶é—´è¶Šä¹…ï¼Œåˆ†æ•°è¶Šé«˜)
        if tool_arm.last_used > 0:
            idle_hours = (current_time - tool_arm.last_used) / 3600
        else:
            idle_hours = float('inf')
        
        max_idle = cold_start_config["max_idle_hours"]
        recency_factor = min(1.0, idle_hours / max(max_idle, 1))
        
        # 4. æ ·æœ¬å……è¶³æ€§å› å­ (æ ·æœ¬è¶Šå°‘ï¼Œåˆ†æ•°è¶Šé«˜)
        min_samples = cold_start_config["min_sample_size"]
        sample_factor = max(0.0, 1.0 - total_samples / max(min_samples, 1))
        
        return {
            'usage_count': usage_count,
            'usage_factor': usage_factor,
            'reliability_score': adjusted_reliability,
            'reliability_factor': reliability_factor,
            'idle_hours': idle_hours if idle_hours != float('inf') else -1,
            'recency_factor': recency_factor,
            'sample_size': total_samples,
            'sample_factor': sample_factor
        }
    
    def _generate_cold_start_reason(self, analysis: Dict[str, any], 
                                   cold_score: float, threshold: float) -> str:
        """
        ç”Ÿæˆå†·å¯åŠ¨åˆ¤æ–­çš„è¯¦ç»†ç†ç”±
        
        Args:
            analysis: åˆ†æç»“æœ
            cold_score: å†·å¯åŠ¨å¾—åˆ†
            threshold: åˆ¤å®šé˜ˆå€¼
            
        Returns:
            åˆ¤æ–­ç†ç”±å­—ç¬¦ä¸²
        """
        reasons = []
        
        # ä½¿ç”¨é¢‘ç‡åˆ†æ
        if analysis['usage_factor'] > 0.7:
            reasons.append(f"ä½¿ç”¨æ¬¡æ•°è¿‡å°‘({analysis['usage_count']}æ¬¡)")
        elif analysis['usage_factor'] > 0.3:
            reasons.append(f"ä½¿ç”¨ç»éªŒæœ‰é™({analysis['usage_count']}æ¬¡)")
        
        # å¯é æ€§åˆ†æ
        if analysis['reliability_factor'] > 0.6:
            reasons.append(f"æ€§èƒ½æ•°æ®ä¸å¯é (å¯é æ€§:{analysis['reliability_score']:.2f})")
        elif analysis['reliability_factor'] > 0.3:
            reasons.append(f"æ€§èƒ½æ•°æ®ä¸å¤Ÿç¨³å®š")
        
        # æœ€è¿‘ä½¿ç”¨åˆ†æ
        if analysis['idle_hours'] > 72:
            reasons.append(f"é•¿æ—¶é—´æœªä½¿ç”¨({analysis['idle_hours']:.1f}å°æ—¶)")
        elif analysis['idle_hours'] > 24:
            reasons.append(f"è¾ƒé•¿æ—¶é—´æœªä½¿ç”¨")
        
        # æ ·æœ¬æ•°åˆ†æ
        if analysis['sample_factor'] > 0.7:
            reasons.append(f"æ ·æœ¬æ•°æ®ä¸è¶³({analysis['sample_size']}ä¸ª)")
        
        if not reasons:
            if cold_score > threshold:
                reasons.append("ç»¼åˆè¯„ä¼°æ˜¾ç¤ºç¼ºä¹è¶³å¤Ÿç»éªŒ")
            else:
                reasons.append("å…·æœ‰å……è¶³çš„ä½¿ç”¨ç»éªŒå’Œå¯é æ•°æ®")
        
        # ç»„åˆç†ç”±
        if cold_score > threshold:
            return f"å†·å¯åŠ¨çŠ¶æ€: {'; '.join(reasons)} (å¾—åˆ†:{cold_score:.3f} > {threshold})"
        else:
            return f"ç»éªŒä¸°å¯Œ: {'; '.join(reasons)} (å¾—åˆ†:{cold_score:.3f} â‰¤ {threshold})"
    
    def _thompson_sampling_for_paths(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹æ€ç»´è·¯å¾„çš„Thompsoné‡‡æ ·ç®—æ³•"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„è·¯å¾„å†³ç­–è‡‚")
        
        best_arm = None
        best_score = -1
        
        logger.debug(f"ğŸ² Thompsoné‡‡æ ·è·¯å¾„é€‰æ‹©ï¼Œå€™é€‰è·¯å¾„: {len(arms)}ä¸ª")
        
        for arm in arms:
            # ä½¿ç”¨Betaåˆ†å¸ƒè¿›è¡ŒThompsoné‡‡æ ·
            alpha = arm.success_count + 1
            beta = arm.failure_count + 1
            
            # ä»Betaåˆ†å¸ƒä¸­é‡‡æ ·
            sampled_value = np.random.beta(alpha, beta)
            
            # è·¯å¾„çº§åˆ«çš„å¥–åŠ±è€ƒè™‘
            if arm.rl_reward_history:
                avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                # å°†å¥–åŠ±è°ƒæ•´åˆ°0-1èŒƒå›´
                normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                sampled_value = sampled_value * 0.8 + normalized_reward * 0.2
            
            # è·¯å¾„å¤šæ ·æ€§è€ƒè™‘ï¼šå‡å°‘è¿‡åº¦ä¾èµ–å•ä¸€è·¯å¾„
            usage_penalty = min(0.1, arm.activation_count / (self.total_path_selections + 1) * 0.2)
            sampled_value = max(0, sampled_value - usage_penalty)
            
            logger.debug(f"   è·¯å¾„ {arm.path_id}: sampled={sampled_value:.3f}, Î±={alpha}, Î²={beta}")
            
            if sampled_value > best_score:
                best_score = sampled_value
                best_arm = arm
        
        logger.debug(f"ğŸ† Thompsoné‡‡æ ·é€‰æ‹©: {best_arm.path_id} (å¾—åˆ†: {best_score:.3f})")
        return best_arm
    
    def _ucb_variant_for_paths(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹æ€ç»´è·¯å¾„çš„UCB (Upper Confidence Bound) å˜ç§ç®—æ³•"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„è·¯å¾„å†³ç­–è‡‚")
        
        total_rounds = sum(arm.activation_count for arm in arms)
        if total_rounds == 0:
            # ç¬¬ä¸€è½®éšæœºé€‰æ‹©
            selected_arm = np.random.choice(arms)
            logger.debug(f"ğŸ² UCBé¦–è½®éšæœºé€‰æ‹©è·¯å¾„: {selected_arm.path_id}")
            return selected_arm
        
        best_arm = None
        best_ucb_value = -float('inf')
        
        logger.debug(f"ğŸ“Š UCBè·¯å¾„é€‰æ‹©ï¼Œæ€»è½®æ•°: {total_rounds}")
        
        for arm in arms:
            if arm.activation_count == 0:
                # æœªå°è¯•è¿‡çš„è·¯å¾„ä¼˜å…ˆé€‰æ‹©
                logger.debug(f"ğŸ†• ä¼˜å…ˆé€‰æ‹©æœªä½¿ç”¨è·¯å¾„: {arm.path_id}")
                return arm
            
            # è®¡ç®—UCBå€¼
            confidence_bound = np.sqrt(2 * np.log(total_rounds) / arm.activation_count)
            
            # åŸºç¡€æˆåŠŸç‡
            base_value = arm.success_rate
            
            # è·¯å¾„çº§åˆ«çš„RLå¥–åŠ±è€ƒè™‘
            if arm.rl_reward_history:
                avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                base_value = base_value * 0.7 + normalized_reward * 0.3
            
            # è·¯å¾„æ¢ç´¢å¥–åŠ±ï¼šé¼“åŠ±å°è¯•ä¸åŒæ€ç»´æ–¹å¼
            exploration_bonus = confidence_bound * 1.2  # å¢å¼ºæ¢ç´¢
            ucb_value = base_value + exploration_bonus
            
            logger.debug(f"   è·¯å¾„ {arm.path_id}: UCB={ucb_value:.3f}, base={base_value:.3f}, conf={confidence_bound:.3f}")
            
            if ucb_value > best_ucb_value:
                best_ucb_value = ucb_value
                best_arm = arm
        
        logger.debug(f"ğŸ† UCBé€‰æ‹©è·¯å¾„: {best_arm.path_id} (UCBå€¼: {best_ucb_value:.3f})")
        return best_arm
    
    def _epsilon_greedy_for_paths(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹æ€ç»´è·¯å¾„çš„Epsilon-Greedyç®—æ³•"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„è·¯å¾„å†³ç­–è‡‚")
        
        # è·¯å¾„çº§åˆ«çš„åŠ¨æ€epsilonå€¼ï¼Œé¼“åŠ±æ€ç»´å¤šæ ·æ€§
        total_activations = sum(arm.activation_count for arm in arms)
        epsilon = max(0.1, 0.4 / (1 + total_activations * 0.008))  # æ¯”ä¼ ç»Ÿæ›´é«˜çš„æ¢ç´¢ç‡
        
        logger.debug(f"ğŸ¯ Epsilon-Greedyè·¯å¾„é€‰æ‹©ï¼ŒÎµ={epsilon:.3f}")
        
        # ä½¿ç”¨epsilonå†³å®šæ˜¯å¦æ¢ç´¢
        if np.random.random() < epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©è·¯å¾„ï¼Œé¼“åŠ±æ€ç»´å¤šæ ·æ€§
            selected_arm = np.random.choice(arms)
            logger.debug(f"ğŸ” æ¢ç´¢æ¨¡å¼é€‰æ‹©è·¯å¾„: {selected_arm.path_id}")
            return selected_arm
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©å½“å‰æœ€å¥½çš„è·¯å¾„
            best_arm = None
            best_score = -float('inf')
            
            for arm in arms:
                # è·¯å¾„çº§åˆ«çš„ç»¼åˆè¯„åˆ†
                score = arm.success_rate
                
                # RLå¥–åŠ±æƒé‡
                if arm.rl_reward_history:
                    avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                    normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                    score = score * 0.6 + normalized_reward * 0.4
                
                # è·¯å¾„ä½¿ç”¨é¢‘ç‡å¹³è¡¡ï¼šé¿å…è¿‡åº¦ä¾èµ–å•ä¸€æ€ç»´æ¨¡å¼
                usage_ratio = arm.activation_count / (total_activations + 1)
                if usage_ratio > 0.5:  # å¦‚æœæŸè·¯å¾„ä½¿ç”¨è¿‡äºé¢‘ç¹ï¼Œç¨å¾®é™ä½è¯„åˆ†
                    score *= 0.95
                
                logger.debug(f"   è·¯å¾„ {arm.path_id}: score={score:.3f}, usage_ratio={usage_ratio:.3f}")
                
                if score > best_score:
                    best_score = score
                    best_arm = arm
            
            logger.debug(f"ğŸ† åˆ©ç”¨æ¨¡å¼é€‰æ‹©è·¯å¾„: {best_arm.path_id} (å¾—åˆ†: {best_score:.3f})")
            return best_arm if best_arm else arms[0]
    
    def _select_best_algorithm_for_paths(self) -> str:
        """
        ä¸ºè·¯å¾„é€‰æ‹©é€‰æ‹©æœ€ä½³ç®—æ³•
        
        Returns:
            æœ€ä½³ç®—æ³•åç§°
        """
        # å¦‚æœæ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨Thompsoné‡‡æ ·è¿›è¡Œæ¢ç´¢
        if self.total_path_selections < 15:
            logger.debug("ğŸ“Š æ ·æœ¬è¾ƒå°‘ï¼Œé€‰æ‹©Thompsoné‡‡æ ·")
            return 'thompson_sampling'
        
        # è®¡ç®—è·¯å¾„çº§åˆ«çš„æ”¶æ•›æ°´å¹³
        if not self.path_arms:
            return 'thompson_sampling'
        
        arms_list = list(self.path_arms.values())
        convergence_level = self._calculate_path_convergence_level(arms_list)
        
        # è€ƒè™‘æ€ç»´å¤šæ ·æ€§ï¼šè·¯å¾„é€‰æ‹©éœ€è¦æ›´å¤šæ¢ç´¢
        if convergence_level < 0.4:
            # ä½æ”¶æ•›ï¼Œä½¿ç”¨æ¢ç´¢æ€§å¼ºçš„ç®—æ³•
            logger.debug(f"ğŸ“Š ä½æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©Thompsoné‡‡æ ·")
            return 'thompson_sampling'
        elif convergence_level < 0.7:
            # ä¸­ç­‰æ”¶æ•›ï¼Œä½¿ç”¨å¹³è¡¡çš„ç®—æ³•
            logger.debug(f"ğŸ“Š ä¸­ç­‰æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©UCB")
            return 'ucb_variant'
        else:
            # é«˜æ”¶æ•›ï¼Œä½†ä»éœ€ä¿æŒä¸€å®šæ¢ç´¢ï¼ˆæ€ç»´å¤šæ ·æ€§é‡è¦ï¼‰
            logger.debug(f"ğŸ“Š é«˜æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©Epsilon-Greedy")
            return 'epsilon_greedy'
    
    def _calculate_path_convergence_level(self, arms: List[EnhancedDecisionArm]) -> float:
        """
        è®¡ç®—è·¯å¾„çº§åˆ«çš„æ”¶æ•›æ°´å¹³
        
        Args:
            arms: è·¯å¾„å†³ç­–è‡‚åˆ—è¡¨
            
        Returns:
            æ”¶æ•›æ°´å¹³ (0.0-1.0)
        """
        if len(arms) < 2:
            return 0.0
        
        # è®¡ç®—è·¯å¾„æˆåŠŸç‡æ–¹å·®
        success_rates = []
        for arm in arms:
            total = arm.success_count + arm.failure_count
            if total > 0:
                success_rates.append(arm.success_count / total)
        
        if len(success_rates) < 2:
            return 0.0
        
        variance = np.var(success_rates)
        # å°†æ–¹å·®è½¬æ¢ä¸ºæ”¶æ•›æ°´å¹³ï¼ˆæ–¹å·®è¶Šå°ï¼Œæ”¶æ•›æ°´å¹³è¶Šé«˜ï¼‰
        # å¯¹äºæ€ç»´è·¯å¾„ï¼Œæˆ‘ä»¬å¸Œæœ›ä¿æŒä¸€å®šçš„å¤šæ ·æ€§ï¼Œæ‰€ä»¥æ”¶æ•›æ ‡å‡†ç¨å¾®å®½æ¾
        convergence_level = max(0.0, 1.0 - variance * 3.5)
        
        return convergence_level
    
    # ==================== ğŸ”§ å·¥å…·é€‰æ‹©MABç®—æ³•å®ç° ====================
    
    def _select_best_algorithm_for_tools(self) -> str:
        """
        ä¸ºå·¥å…·é€‰æ‹©é€‰æ‹©æœ€ä½³ç®—æ³•
        
        Returns:
            æœ€ä½³ç®—æ³•åç§°
        """
        # å¦‚æœæ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨Thompsoné‡‡æ ·è¿›è¡Œæ¢ç´¢
        if self.total_tool_selections < 10:
            logger.debug("ğŸ“Š å·¥å…·é€‰æ‹©æ ·æœ¬è¾ƒå°‘ï¼Œé€‰æ‹©Thompsoné‡‡æ ·")
            return 'thompson_sampling'
        
        # è®¡ç®—å·¥å…·çº§åˆ«çš„æ”¶æ•›æ°´å¹³
        if not self.tool_arms:
            return 'thompson_sampling'
        
        arms_list = list(self.tool_arms.values())
        convergence_level = self._calculate_tool_convergence_level(arms_list)
        
        # å·¥å…·é€‰æ‹©å€¾å‘äºæ›´å¿«æ”¶æ•›åˆ°æœ€ä¼˜å·¥å…·
        if convergence_level < 0.3:
            # ä½æ”¶æ•›ï¼Œä½¿ç”¨æ¢ç´¢æ€§å¼ºçš„ç®—æ³•
            logger.debug(f"ğŸ“Š å·¥å…·é€‰æ‹©ä½æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©Thompsoné‡‡æ ·")
            return 'thompson_sampling'
        elif convergence_level < 0.6:
            # ä¸­ç­‰æ”¶æ•›ï¼Œä½¿ç”¨å¹³è¡¡çš„ç®—æ³•
            logger.debug(f"ğŸ“Š å·¥å…·é€‰æ‹©ä¸­ç­‰æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©UCB")
            return 'ucb_variant'
        else:
            # é«˜æ”¶æ•›ï¼Œä½¿ç”¨åˆ©ç”¨å‹ç®—æ³•
            logger.debug(f"ğŸ“Š å·¥å…·é€‰æ‹©é«˜æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©Epsilon-Greedy")
            return 'epsilon_greedy'
    
    def _calculate_tool_convergence_level(self, arms: List[EnhancedDecisionArm]) -> float:
        """
        è®¡ç®—å·¥å…·çº§åˆ«çš„æ”¶æ•›æ°´å¹³
        
        Args:
            arms: å·¥å…·å†³ç­–è‡‚åˆ—è¡¨
            
        Returns:
            æ”¶æ•›æ°´å¹³ (0.0-1.0)
        """
        if len(arms) < 2:
            return 0.0
        
        # è®¡ç®—å·¥å…·æˆåŠŸç‡æ–¹å·®
        success_rates = []
        for arm in arms:
            total = arm.success_count + arm.failure_count
            if total > 0:
                success_rates.append(arm.success_count / total)
        
        if len(success_rates) < 2:
            return 0.0
        
        variance = np.var(success_rates)
        # å·¥å…·é€‰æ‹©å¯ä»¥æ›´å¿«æ”¶æ•›ï¼Œæ”¶æ•›æ ‡å‡†ç›¸å¯¹ä¸¥æ ¼
        convergence_level = max(0.0, 1.0 - variance * 2.5)
        
        return convergence_level
    
    def _thompson_sampling_for_tools(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹å·¥å…·é€‰æ‹©çš„Thompsoné‡‡æ ·ç®—æ³•"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„å·¥å…·å†³ç­–è‡‚")
        
        best_arm = None
        best_score = -1
        
        logger.debug(f"ğŸ”§ Thompsoné‡‡æ ·å·¥å…·é€‰æ‹©ï¼Œå€™é€‰å·¥å…·: {len(arms)}ä¸ª")
        
        for arm in arms:
            # ä½¿ç”¨Betaåˆ†å¸ƒè¿›è¡ŒThompsoné‡‡æ ·
            alpha = arm.success_count + 1
            beta = arm.failure_count + 1
            
            # ä»Betaåˆ†å¸ƒä¸­é‡‡æ ·
            sampled_value = np.random.beta(alpha, beta)
            
            # å·¥å…·çº§åˆ«çš„å¥–åŠ±è€ƒè™‘
            if arm.rl_reward_history:
                avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                # å°†å¥–åŠ±è°ƒæ•´åˆ°0-1èŒƒå›´
                normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                sampled_value = sampled_value * 0.7 + normalized_reward * 0.3
            
            logger.debug(f"   å·¥å…· {arm.path_id}: sampled={sampled_value:.3f}, Î±={alpha}, Î²={beta}")
            
            if sampled_value > best_score:
                best_score = sampled_value
                best_arm = arm
        
        logger.debug(f"ğŸ† Thompsoné‡‡æ ·é€‰æ‹©å·¥å…·: {best_arm.path_id} (å¾—åˆ†: {best_score:.3f})")
        return best_arm
    
    def _ucb_variant_for_tools(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹å·¥å…·é€‰æ‹©çš„UCB (Upper Confidence Bound) å˜ç§ç®—æ³•"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„å·¥å…·å†³ç­–è‡‚")
        
        total_rounds = sum(arm.activation_count for arm in arms)
        if total_rounds == 0:
            # ç¬¬ä¸€è½®éšæœºé€‰æ‹©
            selected_arm = np.random.choice(arms)
            logger.debug(f"ğŸ”§ UCBé¦–è½®éšæœºé€‰æ‹©å·¥å…·: {selected_arm.path_id}")
            return selected_arm
        
        best_arm = None
        best_ucb_value = -float('inf')
        
        logger.debug(f"ğŸ“Š UCBå·¥å…·é€‰æ‹©ï¼Œæ€»è½®æ•°: {total_rounds}")
        
        for arm in arms:
            if arm.activation_count == 0:
                # æœªå°è¯•è¿‡çš„å·¥å…·ä¼˜å…ˆé€‰æ‹©
                logger.debug(f"ğŸ†• ä¼˜å…ˆé€‰æ‹©æœªä½¿ç”¨å·¥å…·: {arm.path_id}")
                return arm
            
            # è®¡ç®—UCBå€¼
            confidence_bound = np.sqrt(2 * np.log(total_rounds) / arm.activation_count)
            
            # åŸºç¡€æˆåŠŸç‡
            base_value = arm.success_rate
            
            # å·¥å…·çº§åˆ«çš„RLå¥–åŠ±è€ƒè™‘
            if arm.rl_reward_history:
                avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                base_value = base_value * 0.6 + normalized_reward * 0.4
            
            # å·¥å…·æ¢ç´¢å¥–åŠ±
            exploration_bonus = confidence_bound * 1.0  # æ ‡å‡†æ¢ç´¢
            ucb_value = base_value + exploration_bonus
            
            logger.debug(f"   å·¥å…· {arm.path_id}: UCB={ucb_value:.3f}, base={base_value:.3f}, conf={confidence_bound:.3f}")
            
            if ucb_value > best_ucb_value:
                best_ucb_value = ucb_value
                best_arm = arm
        
        logger.debug(f"ğŸ† UCBé€‰æ‹©å·¥å…·: {best_arm.path_id} (UCBå€¼: {best_ucb_value:.3f})")
        return best_arm
    
    def _epsilon_greedy_for_tools(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹å·¥å…·é€‰æ‹©çš„Epsilon-Greedyç®—æ³•"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„å·¥å…·å†³ç­–è‡‚")
        
        # å·¥å…·çº§åˆ«çš„åŠ¨æ€epsilonå€¼
        total_activations = sum(arm.activation_count for arm in arms)
        epsilon = max(0.05, 0.3 / (1 + total_activations * 0.01))  # æ¯”è·¯å¾„é€‰æ‹©æ›´ä½çš„æ¢ç´¢ç‡
        
        logger.debug(f"ğŸ”§ Epsilon-Greedyå·¥å…·é€‰æ‹©ï¼ŒÎµ={epsilon:.3f}")
        
        # ä½¿ç”¨epsilonå†³å®šæ˜¯å¦æ¢ç´¢
        if np.random.random() < epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©å·¥å…·
            selected_arm = np.random.choice(arms)
            logger.debug(f"ğŸ” æ¢ç´¢æ¨¡å¼é€‰æ‹©å·¥å…·: {selected_arm.path_id}")
            return selected_arm
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©å½“å‰æœ€å¥½çš„å·¥å…·
            best_arm = None
            best_score = -float('inf')
            
            for arm in arms:
                # å·¥å…·çº§åˆ«çš„ç»¼åˆè¯„åˆ†
                score = arm.success_rate
                
                # RLå¥–åŠ±æƒé‡
                if arm.rl_reward_history:
                    avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                    normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                    score = score * 0.5 + normalized_reward * 0.5
                
                logger.debug(f"   å·¥å…· {arm.path_id}: score={score:.3f}")
                
                if score > best_score:
                    best_score = score
                    best_arm = arm
            
            logger.debug(f"ğŸ† åˆ©ç”¨æ¨¡å¼é€‰æ‹©å·¥å…·: {best_arm.path_id} (å¾—åˆ†: {best_score:.3f})")
            return best_arm if best_arm else arms[0]
    
    # ==================== ğŸ“Š æ›´æ–°æ€§èƒ½åé¦ˆæ–¹æ³• ====================
    
    def update_path_performance(self, path_id: str, success: bool, reward: float = 0.0):
        """
        ğŸ”§ åŒå±‚å­¦ä¹ ï¼šæ›´æ–°è·¯å¾„æˆ–å·¥å…·çš„æ€§èƒ½åé¦ˆ - é€šç”¨æ€§åé¦ˆæ›´æ–°æ–¹æ³•
        
        Args:
            path_id: è·¯å¾„IDæˆ–å·¥å…·IDï¼ˆç”±è°ƒç”¨æ–¹å†³å®šæ˜¯è·¯å¾„è¿˜æ˜¯å·¥å…·ï¼‰
            success: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
            reward: RLå¥–åŠ±å€¼
        """
        # ğŸ¯ æ™ºèƒ½è¯†åˆ«ï¼šæ£€æŸ¥æ˜¯è·¯å¾„åé¦ˆè¿˜æ˜¯å·¥å…·åé¦ˆ
        if path_id in self.path_arms:
            # è·¯å¾„åé¦ˆå¤„ç†
            target_arm = self.path_arms[path_id]
            
            # æ›´æ–°è·¯å¾„ç®—æ³•æ€§èƒ½ç»Ÿè®¡
            if self.path_selection_history:
                last_selection = self.path_selection_history[-1]
                if last_selection['path_id'] == path_id:
                    algorithm = last_selection['algorithm']
                    self.algorithm_performance[algorithm]['total'] += 1
                    if success:
                        self.algorithm_performance[algorithm]['successes'] += 1
                        
        elif path_id in self.tool_arms:
            # å·¥å…·åé¦ˆå¤„ç†
            target_arm = self.tool_arms[path_id]
            
            # æ›´æ–°å·¥å…·ç®—æ³•æ€§èƒ½ç»Ÿè®¡
            if self.tool_selection_history:
                last_selection = self.tool_selection_history[-1]
                if last_selection['tool_id'] == path_id:
                    algorithm = last_selection['algorithm']
                    self.tool_algorithm_performance[algorithm]['total'] += 1
                    if success:
                        self.tool_algorithm_performance[algorithm]['successes'] += 1
                        
        else:
            # åŠ¨æ€åˆ›å»ºå†³ç­–è‡‚ï¼ˆé»˜è®¤ä½œä¸ºè·¯å¾„å¤„ç†ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
            target_arm = self._create_strategy_arm_if_missing(path_id)
            logger.debug(f"ğŸ”§ ä¸ºæœªçŸ¥ID {path_id} åˆ›å»ºè·¯å¾„å†³ç­–è‡‚ï¼ˆå‘åå…¼å®¹ï¼‰")
        
        # ä½¿ç”¨å¢å¼ºçš„æ€§èƒ½æ›´æ–°æ–¹æ³•
        target_arm.update_performance(success, reward)
        
        # è®°å½•æ›´æ–°æ—¥å¿—
        arm_type = "å·¥å…·" if path_id in self.tool_arms else "è·¯å¾„"
        logger.info(f"ğŸ“Š æ›´æ–°{arm_type}æ€§èƒ½: {path_id} -> æˆåŠŸç‡:{target_arm.success_rate:.3f}, å¥–åŠ±:{reward:.3f}")
        logger.debug(f"   è¯¦ç»†: æˆåŠŸ{target_arm.success_count}æ¬¡, å¤±è´¥{target_arm.failure_count}æ¬¡, æ¿€æ´»{target_arm.activation_count}æ¬¡")
        
        # ğŸ† é»„é‡‘æ¨¡æ¿è¯†åˆ«é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦ç¬¦åˆé»„é‡‘æ¨¡æ¿æ¡ä»¶ï¼ˆä»…å¯¹è·¯å¾„åº”ç”¨ï¼‰
        if path_id in self.path_arms:
            self._check_and_promote_to_golden_template(path_id, target_arm)
    
    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆæ ‡è®°ä¸ºè¿‡æ—¶ï¼‰
    def update_arm_performance(self, dimension_name: str, option: str, 
                             success: bool, reward: float = 0.0):
        """
        æ›´æ–°å†³ç­–è‡‚çš„æ€§èƒ½ - å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ update_path_performance
        
        Args:
            dimension_name: ç»´åº¦åç§°
            option: é€‰é¡¹åç§°
            success: æ˜¯å¦æˆåŠŸ
            reward: RLå¥–åŠ±å€¼
        """
        logger.warning("âš ï¸ update_arm_performance å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ update_path_performance")
        path_id = f"{dimension_name}_{option}"  # ä¸´æ—¶è½¬æ¢
        self.update_path_performance(path_id, success, reward)
    
    def check_path_convergence(self) -> bool:
        """
        æ£€æŸ¥è·¯å¾„é€‰æ‹©æ˜¯å¦æ”¶æ•›
        
        Returns:
            æ˜¯å¦æ”¶æ•›
        """
        if len(self.path_arms) < 2:
            return False
            
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬
        total_samples = sum(arm.success_count + arm.failure_count for arm in self.path_arms.values())
        if total_samples < self.min_samples:
            return False
            
        # è®¡ç®—è·¯å¾„æˆåŠŸç‡æ–¹å·®ï¼Œåˆ¤æ–­æ˜¯å¦æ”¶æ•›
        success_rates = []
        for arm in self.path_arms.values():
            total = arm.success_count + arm.failure_count
            if total > 0:
                success_rates.append(arm.success_count / total)
                
        if len(success_rates) < 2:
            return False
            
        variance = np.var(success_rates)
        # å¯¹äºæ€ç»´è·¯å¾„ï¼Œä½¿ç”¨ç¨å¾®å®½æ¾çš„æ”¶æ•›æ ‡å‡†ï¼Œä¿æŒå¤šæ ·æ€§
        adjusted_threshold = self.convergence_threshold * 1.2
        is_converged = variance < adjusted_threshold
        
        if is_converged:
            logger.info(f"âœ… è·¯å¾„é€‰æ‹©å·²æ”¶æ•› (æ–¹å·®:{variance:.4f}, é˜ˆå€¼:{adjusted_threshold:.4f})")
        
        return is_converged
    
    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆæ ‡è®°ä¸ºè¿‡æ—¶ï¼‰
    def check_convergence(self, dimension_name: str) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šç»´åº¦æ˜¯å¦æ”¶æ•› - å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ check_path_convergence
        """
        logger.warning("âš ï¸ check_convergence å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ check_path_convergence")
        return self.check_path_convergence()
    
    def get_path_statistics(self) -> Dict[str, Dict[str, any]]:
        """
        è·å–æ‰€æœ‰è·¯å¾„çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«é»„é‡‘æ¨¡æ¿çŠ¶æ€ï¼‰
        
        Returns:
            è·¯å¾„ç»Ÿè®¡æ•°æ®
        """
        statistics = {}
        
        for path_id, arm in self.path_arms.items():
            # æ£€æŸ¥æ˜¯å¦ä¸ºé»„é‡‘æ¨¡æ¿
            is_golden_template = path_id in self.golden_templates
            golden_template_info = None
            
            if is_golden_template:
                template_data = self.golden_templates[path_id]
                golden_template_info = {
                    'created_timestamp': template_data['created_timestamp'],
                    'last_updated': template_data['last_updated'],
                    'stability_score': template_data['stability_score'],
                    'usage_as_template': self.template_usage_stats.get(path_id, 0),
                    'promotion_reason': template_data['promotion_reason']
                }
            
            # è®¡ç®—è·¯å¾„ç‰¹å®šçš„ç»Ÿè®¡
            statistics[path_id] = {
                'path_type': arm.option,  # è·¯å¾„ç±»å‹
                'path_id': path_id,
                'activation_count': arm.activation_count,
                'success_count': arm.success_count,
                'failure_count': arm.failure_count,
                'success_rate': arm.success_rate,
                'total_reward': arm.total_reward,
                'average_reward': sum(arm.rl_reward_history) / len(arm.rl_reward_history) if arm.rl_reward_history else 0.0,
                'last_used': arm.last_used,
                'recent_trend': self._calculate_recent_trend(arm),
                'consecutive_successes': self._calculate_consecutive_successes(arm),
                'usage_ratio': arm.activation_count / max(self.total_path_selections, 1),
                
                # ğŸ† é»„é‡‘æ¨¡æ¿ç›¸å…³ä¿¡æ¯
                'is_golden_template': is_golden_template,
                'golden_template_info': golden_template_info,
                'meets_golden_criteria': self._check_golden_criteria(arm),
                'stability_score': self._calculate_stability_score(arm) if arm.activation_count >= 10 else 0.0
            }
        
        return statistics
    
    def _check_golden_criteria(self, arm: EnhancedDecisionArm) -> bool:
        """
        æ£€æŸ¥è·¯å¾„æ˜¯å¦ç¬¦åˆé»„é‡‘æ¨¡æ¿çš„åŸºæœ¬æ¡ä»¶ï¼ˆä¸åŒ…æ‹¬ç¨³å®šæ€§æ£€æŸ¥ï¼‰
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            æ˜¯å¦ç¬¦åˆåŸºæœ¬æ¡ä»¶
        """
        config = self.golden_template_config
        return (arm.success_rate >= config['success_rate_threshold'] and 
                arm.activation_count >= config['min_samples_required'])
    
    def get_system_path_summary(self) -> Dict[str, any]:
        """
        è·å–è·¯å¾„é€‰æ‹©ç³»ç»Ÿçš„æ•´ä½“æ‘˜è¦
        
        Returns:
            ç³»ç»Ÿæ‘˜è¦æ•°æ®
        """
        if not self.path_arms:
            return {
                'total_paths': 0,
                'total_selections': self.total_path_selections,
                'is_converged': False,
                'convergence_level': 0.0,
                'most_used_path': None,
                'best_performing_path': None
            }
        
        # æœ€å¸¸ç”¨è·¯å¾„
        most_used_arm = max(self.path_arms.values(), key=lambda a: a.activation_count)
        
        # æœ€ä½³æ€§èƒ½è·¯å¾„
        best_performing_arm = max(self.path_arms.values(), key=lambda a: a.success_rate)
        
        # ç®—æ³•æ€§èƒ½ç»Ÿè®¡
        algorithm_stats = {}
        for algo, stats in self.algorithm_performance.items():
            if stats['total'] > 0:
                algorithm_stats[algo] = {
                    'success_rate': stats['successes'] / stats['total'],
                    'total_uses': stats['total']
                }
        
        return {
            'total_paths': len(self.path_arms),
            'total_selections': self.total_path_selections,
            'is_converged': self.check_path_convergence(),
            'convergence_level': self._calculate_path_convergence_level(list(self.path_arms.values())),
            'most_used_path': {
                'path_id': most_used_arm.path_id,
                'path_type': most_used_arm.option,
                'usage_count': most_used_arm.activation_count
            },
            'best_performing_path': {
                'path_id': best_performing_arm.path_id,
                'path_type': best_performing_arm.option,
                'success_rate': best_performing_arm.success_rate
            },
            'algorithm_performance': algorithm_stats,
            'total_samples': sum(arm.success_count + arm.failure_count for arm in self.path_arms.values())
        }
    
    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆæ ‡è®°ä¸ºè¿‡æ—¶ï¼‰
    def get_dimension_statistics(self) -> Dict[str, Dict[str, any]]:
        """
        è·å–æ‰€æœ‰ç»´åº¦çš„ç»Ÿè®¡ä¿¡æ¯ - å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ get_path_statistics
        """
        logger.warning("âš ï¸ get_dimension_statistics å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ get_path_statistics")
        return self.get_path_statistics()
    
    def get_path_details(self, path_id: str = None) -> Dict[str, any]:
        """
        è·å–æŒ‡å®šè·¯å¾„çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            path_id: è·¯å¾„IDï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›æ‰€æœ‰è·¯å¾„çš„è¯¦ç»†ä¿¡æ¯
            
        Returns:
            è·¯å¾„è¯¦ç»†ä¿¡æ¯
        """
        if path_id is not None:
            if path_id not in self.path_arms:
                logger.warning(f"âš ï¸ è·¯å¾„ {path_id} ä¸å­˜åœ¨")
                return {}
            
            arm = self.path_arms[path_id]
            return {
                'path_id': path_id,
                'path_type': arm.option,
                'success_count': arm.success_count,
                'failure_count': arm.failure_count,
                'success_rate': arm.success_rate,
                'total_reward': arm.total_reward,
                'average_reward': sum(arm.rl_reward_history) / len(arm.rl_reward_history) if arm.rl_reward_history else 0.0,
                'activation_count': arm.activation_count,
                'last_used': arm.last_used,
                'recent_trend': self._calculate_recent_trend(arm),
                'consecutive_successes': self._calculate_consecutive_successes(arm),
                'rl_reward_history': arm.rl_reward_history.copy(),
                'recent_results': arm.recent_results.copy()
            }
        else:
            # è¿”å›æ‰€æœ‰è·¯å¾„çš„è¯¦ç»†ä¿¡æ¯
            all_details = {}
            for pid, arm in self.path_arms.items():
                all_details[pid] = self.get_path_details(pid)
            
            # æŒ‰æˆåŠŸç‡æ’åº
            sorted_paths = sorted(all_details.items(), 
                                key=lambda x: x[1]['success_rate'], 
                                reverse=True)
            return dict(sorted_paths)
    
    def get_selection_history(self, limit: int = 10) -> List[Dict[str, any]]:
        """
        è·å–è·¯å¾„é€‰æ‹©å†å²
        
        Args:
            limit: è¿”å›çš„å†å²è®°å½•æ•°é‡é™åˆ¶
            
        Returns:
            é€‰æ‹©å†å²åˆ—è¡¨
        """
        return self.path_selection_history[-limit:] if self.path_selection_history else []
    
    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆæ ‡è®°ä¸ºè¿‡æ—¶ï¼‰
    def get_arm_details(self, dimension_name: str) -> List[Dict[str, any]]:
        """
        è·å–æŒ‡å®šç»´åº¦çš„æ‰€æœ‰å†³ç­–è‡‚è¯¦ç»†ä¿¡æ¯ - å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ get_path_details
        """
        logger.warning("âš ï¸ get_arm_details å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ get_path_details")
        return list(self.get_path_details().values())
    
    def reset_path(self, path_id: str):
        """
        é‡ç½®æŒ‡å®šè·¯å¾„çš„æ‰€æœ‰æ•°æ®
        
        Args:
            path_id: è·¯å¾„ID
        """
        if path_id in self.path_arms:
            del self.path_arms[path_id]
            logger.info(f"ğŸ”„ è·¯å¾„ {path_id} å·²é‡ç½®")
        
        # æ¸…ç†é€‰æ‹©å†å²ä¸­çš„ç›¸å…³è®°å½•
        self.path_selection_history = [
            record for record in self.path_selection_history 
            if record['path_id'] != path_id
        ]
    
    def reset_all_paths(self):
        """
        é‡ç½®æ‰€æœ‰è·¯å¾„æ•°æ®ï¼Œå®Œå…¨æ¸…ç©ºå­¦ä¹ å†å²
        """
        self.path_arms.clear()
        self.path_selection_history.clear()
        self.total_path_selections = 0
        self.algorithm_performance.clear()
        logger.info("ğŸ”„ æ‰€æœ‰è·¯å¾„æ•°æ®å·²é‡ç½®")
    
    def get_system_status(self) -> Dict[str, any]:
        """
        è·å–MABè·¯å¾„é€‰æ‹©ç³»ç»Ÿçš„æ•´ä½“çŠ¶æ€
        
        Returns:
            ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
        """
        total_paths = len(self.path_arms)
        is_converged = self.check_path_convergence()
        
        # è®¡ç®—æ´»è·ƒè·¯å¾„æ•°ï¼ˆæœ€è¿‘ä½¿ç”¨è¿‡çš„ï¼‰
        current_time = time.time()
        active_paths = sum(
            1 for arm in self.path_arms.values() 
            if arm.last_used > 0 and (current_time - arm.last_used) < 3600  # 1å°æ—¶å†…ä½¿ç”¨è¿‡
        )
        
        # æœ€å—æ¬¢è¿çš„è·¯å¾„ç±»å‹
        path_type_usage = {}
        for arm in self.path_arms.values():
            path_type = arm.option
            path_type_usage[path_type] = path_type_usage.get(path_type, 0) + arm.activation_count
        
        most_popular_type = max(path_type_usage.items(), key=lambda x: x[1])[0] if path_type_usage else None
        
        # è·å–é»„é‡‘æ¨¡æ¿ç»Ÿè®¡
        golden_stats = self.get_golden_template_stats()
        
        return {
            'mode': 'path_selection',  # æ–°å¢ï¼šæ ‡è¯†å½“å‰ä¸ºè·¯å¾„é€‰æ‹©æ¨¡å¼
            'total_paths': total_paths,
            'active_paths': active_paths,
            'total_selections': self.total_path_selections,
            'is_converged': is_converged,
            'convergence_level': self._calculate_path_convergence_level(list(self.path_arms.values())) if self.path_arms else 0.0,
            'convergence_threshold': self.convergence_threshold,
            'min_samples': self.min_samples,
            'most_popular_path_type': most_popular_type,
            'path_type_distribution': path_type_usage,
            'algorithm_performance': dict(self.algorithm_performance),
            
            # ğŸ† é»„é‡‘æ¨¡æ¿ç³»ç»ŸçŠ¶æ€
            'golden_template_system': {
                'enabled': True,
                'total_templates': golden_stats['total_templates'],
                'avg_success_rate': golden_stats['avg_success_rate'],
                'total_usage_count': golden_stats['total_usage_count'],
                'most_used_template': golden_stats['most_used_template'],
                'match_history_count': golden_stats['match_history_count'],
                'config': self.golden_template_config
            }
        }
    
    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆæ ‡è®°ä¸ºè¿‡æ—¶ï¼‰
    def reset_dimension(self, dimension_name: str):
        """
        é‡ç½®æŒ‡å®šç»´åº¦çš„æ‰€æœ‰æ•°æ® - å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ reset_path æˆ– reset_all_paths
        """
        logger.warning("âš ï¸ reset_dimension å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ reset_path æˆ– reset_all_paths")
        # ä¸æ‰§è¡Œä»»ä½•æ“ä½œï¼Œé¿å…æ„å¤–æ¸…é™¤è·¯å¾„æ•°æ®
    
    # ==================== ğŸ† é»„é‡‘å†³ç­–æ¨¡æ¿ç³»ç»Ÿå®ç° ====================
    
    def _check_golden_template_match(self, paths: List[ReasoningPath]) -> Optional[Dict[str, any]]:
        """
        æ£€æŸ¥å½“å‰è·¯å¾„åˆ—è¡¨æ˜¯å¦ä¸å·²æœ‰é»„é‡‘æ¨¡æ¿åŒ¹é… - ğŸ¯ ä¿®å¤ç‰ˆï¼šåŸºäºç­–ç•¥IDåŒ¹é…
        
        Args:
            paths: å€™é€‰æ€ç»´è·¯å¾„åˆ—è¡¨
            
        Returns:
            åŒ¹é…ç»“æœå­—å…¸ï¼ŒåŒ…å«åŒ¹é…çš„æ¨¡æ¿å’Œè·¯å¾„ä¿¡æ¯ï¼Œå¦‚æœæ— åŒ¹é…åˆ™è¿”å›None
        """
        if not self.golden_templates:
            return None
        
        best_match = None
        best_score = 0.0
        match_threshold = 0.85  # åŒ¹é…é˜ˆå€¼
        
        logger.debug(f"ğŸ† æ£€æŸ¥ {len(self.golden_templates)} ä¸ªé»„é‡‘æ¨¡æ¿")
        
        for template_id, template_data in self.golden_templates.items():
            template_path_type = template_data['path_type']
            
            # ğŸ¯ æ ¹æºä¿®å¤ï¼šç›´æ¥ä½¿ç”¨è·¯å¾„çš„ç­–ç•¥IDï¼Œæ— éœ€æ¨å¯¼
            for path in paths:
                # ç›´æ¥ä½¿ç”¨è·¯å¾„çš„ç­–ç•¥ID
                path_strategy_id = path.strategy_id
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ï¼šç­–ç•¥IDåŒ¹é…æˆ–è·¯å¾„ç±»å‹åŒ¹é…
                is_strategy_match = (template_id == path_strategy_id)
                is_type_match = (template_path_type == path.path_type)
                
                if is_strategy_match or is_type_match:
                    # è®¡ç®—åŒ¹é…åˆ†æ•°
                    match_score = self._calculate_template_match_score(template_data, path)
                    
                    # ç­–ç•¥IDåŒ¹é…ç»™é¢å¤–åˆ†æ•°
                    if is_strategy_match:
                        match_score += 0.1  # ç­–ç•¥IDåŒ¹é…å¥–åŠ±
                    
                    logger.debug(f"   æ¨¡æ¿ {template_id} vs è·¯å¾„ç­–ç•¥ {path_strategy_id}: åŒ¹é…åˆ†æ•° {match_score:.3f}")
                    logger.debug(f"      ç­–ç•¥åŒ¹é…: {is_strategy_match}, ç±»å‹åŒ¹é…: {is_type_match}")
                    
                    if match_score > match_threshold and match_score > best_score:
                        best_match = {
                            'template_id': template_id,
                            'path': path,
                            'match_score': match_score,
                            'template_data': template_data,
                            'strategy_match': is_strategy_match
                        }
                        best_score = match_score
        
        if best_match:
            match_type = "ç­–ç•¥ID" if best_match['strategy_match'] else "è·¯å¾„ç±»å‹"
            logger.debug(f"ğŸ† æ‰¾åˆ°æœ€ä½³åŒ¹é…: æ¨¡æ¿ {best_match['template_id']} (åˆ†æ•°: {best_score:.3f}, åŒ¹é…ç±»å‹: {match_type})")
        else:
            logger.debug("ğŸ† æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„é»„é‡‘æ¨¡æ¿åŒ¹é…")
        
        return best_match
    
    def _calculate_template_match_score(self, template_data: Dict[str, any], path: ReasoningPath) -> float:
        """
        è®¡ç®—æ¨¡æ¿ä¸è·¯å¾„çš„åŒ¹é…åˆ†æ•° - ğŸ¯ ä¿®å¤ç‰ˆï¼šåŸºäºç­–ç•¥IDåŒ¹é…
        
        Args:
            template_data: é»„é‡‘æ¨¡æ¿æ•°æ®
            path: å€™é€‰è·¯å¾„
            
        Returns:
            åŒ¹é…åˆ†æ•° (0.0-1.0)
        """
        score = 0.0
        
        # ğŸ¯ æ ¹æºä¿®å¤ï¼šç›´æ¥ä½¿ç”¨è·¯å¾„çš„ç­–ç•¥ID
        path_strategy_id = path.strategy_id
        
        # 1. ç­–ç•¥IDå®Œå…¨åŒ¹é… (åŸºç¡€åˆ†æ•°60%)
        template_strategy_id = template_data.get('strategy_id', template_data.get('path_id', ''))
        if template_strategy_id == path_strategy_id:
            score += 0.6
        # 1b. è·¯å¾„ç±»å‹åŒ¹é…ä½œä¸ºå¤‡é€‰ (åŸºç¡€åˆ†æ•°40%)
        elif template_data['path_type'] == path.path_type:
            score += 0.4
        
        # 2. æè¿°ç›¸ä¼¼æ€§ (é¢å¤–20%)
        desc_similarity = self._calculate_description_similarity(
            template_data.get('description', ''), path.description
        )
        score += desc_similarity * 0.2
        
        # 3. å†å²æ€§èƒ½å¥–åŠ± (é¢å¤–20%)
        performance_bonus = min(template_data['success_rate'] - 0.8, 0.2) * 1.0  # è¶…è¿‡80%çš„éƒ¨åˆ†è½¬æ¢ä¸ºå¥–åŠ±
        score += performance_bonus
        
        logger.debug(f"ğŸ¯ åŒ¹é…åˆ†æ•°è¯¦æƒ…:")
        logger.debug(f"   æ¨¡æ¿ç­–ç•¥ID: {template_strategy_id}")
        logger.debug(f"   è·¯å¾„ç­–ç•¥ID: {path_strategy_id}")
        logger.debug(f"   ç­–ç•¥åŒ¹é…: {template_strategy_id == path_strategy_id}")
        logger.debug(f"   æè¿°ç›¸ä¼¼æ€§: {desc_similarity:.3f}")
        logger.debug(f"   æ€§èƒ½å¥–åŠ±: {performance_bonus:.3f}")
        logger.debug(f"   æ€»åˆ†: {score:.3f}")
        
        return min(score, 1.0)
    
    def _calculate_description_similarity(self, desc1: str, desc2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæè¿°æ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆç®€å•å®ç°ï¼‰
        
        Args:
            desc1: æè¿°æ–‡æœ¬1
            desc2: æè¿°æ–‡æœ¬2
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0.0-1.0)
        """
        if not desc1 or not desc2:
            return 0.0
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…ç®—æ³•
        words1 = set(desc1.lower().split())
        words2 = set(desc2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _check_and_promote_to_golden_template(self, path_id: str, arm: EnhancedDecisionArm):
        """
        æ£€æŸ¥è·¯å¾„æ˜¯å¦ç¬¦åˆé»„é‡‘æ¨¡æ¿æ¡ä»¶ï¼Œå¦‚æœç¬¦åˆåˆ™æå‡ä¸ºé»„é‡‘æ¨¡æ¿
        
        Args:
            path_id: è·¯å¾„ID
            arm: å†³ç­–è‡‚å¯¹è±¡
        """
        config = self.golden_template_config
        
        # æ£€æŸ¥åŸºæœ¬æ¡ä»¶
        if (arm.success_rate >= config['success_rate_threshold'] and 
            arm.activation_count >= config['min_samples_required']):
            
            # æ£€æŸ¥ç¨³å®šæ€§ï¼ˆæœ€è¿‘Næ¬¡çš„è¡¨ç°ï¼‰
            if self._check_path_stability(arm):
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯é»„é‡‘æ¨¡æ¿
                if path_id not in self.golden_templates:
                    self._promote_to_golden_template(path_id, arm)
                else:
                    # æ›´æ–°å·²æœ‰é»„é‡‘æ¨¡æ¿
                    self._update_golden_template(path_id, arm)
    
    def _check_path_stability(self, arm: EnhancedDecisionArm) -> bool:
        """
        æ£€æŸ¥è·¯å¾„çš„ç¨³å®šæ€§ï¼ˆæœ€è¿‘è¡¨ç°æ˜¯å¦æŒç»­è‰¯å¥½ï¼‰
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            æ˜¯å¦ç¨³å®š
        """
        window_size = self.golden_template_config['stability_check_window']
        
        # è·å–æœ€è¿‘çš„ç»“æœ
        recent_results = arm.recent_results[-window_size:] if len(arm.recent_results) >= window_size else arm.recent_results
        
        if len(recent_results) < window_size:
            return False  # æ ·æœ¬ä¸è¶³
        
        # è®¡ç®—æœ€è¿‘çª—å£çš„æˆåŠŸç‡
        recent_successes = sum(1 for result in recent_results if result)
        recent_success_rate = recent_successes / len(recent_results)
        
        # ç¨³å®šæ€§è¦æ±‚ï¼šæœ€è¿‘è¡¨ç°ä¸ä½äºæ•´ä½“è¡¨ç°çš„95%
        stability_threshold = arm.success_rate * 0.95
        
        return recent_success_rate >= stability_threshold
    
    def _promote_to_golden_template(self, strategy_id: str, arm: EnhancedDecisionArm):
        """
        å°†ç­–ç•¥æå‡ä¸ºé»„é‡‘æ¨¡æ¿ - ğŸ¯ ä¿®å¤ç‰ˆï¼šåŸºäºç­–ç•¥ID
        
        Args:
            strategy_id: ç­–ç•¥IDï¼ˆè€Œéå®ä¾‹IDï¼‰
            arm: å†³ç­–è‡‚å¯¹è±¡
        """
        # æ£€æŸ¥æ¨¡æ¿æ•°é‡é™åˆ¶
        if len(self.golden_templates) >= self.golden_template_config['max_golden_templates']:
            # ç§»é™¤è¡¨ç°æœ€å·®çš„æ¨¡æ¿
            self._remove_worst_golden_template()
        
        # ğŸ¯ ä¿®å¤ï¼šåŸºäºç­–ç•¥IDåˆ›å»ºé»„é‡‘æ¨¡æ¿
        template_data = {
            'strategy_id': strategy_id,        # ç­–ç•¥IDï¼ˆç”¨äºåŒ¹é…ï¼‰
            'path_id': strategy_id,           # å…¼å®¹æ€§å­—æ®µ
            'path_type': arm.option,          # è·¯å¾„ç±»å‹
            'description': getattr(arm, 'description', ''),
            'success_rate': arm.success_rate,
            'total_activations': arm.activation_count,
            'average_reward': sum(arm.rl_reward_history) / len(arm.rl_reward_history) if arm.rl_reward_history else 0.0,
            'created_timestamp': time.time(),
            'last_updated': time.time(),
            'promotion_reason': 'high_performance',
            'stability_score': self._calculate_stability_score(arm),
            'usage_count': 0  # ä½œä¸ºæ¨¡æ¿è¢«ä½¿ç”¨çš„æ¬¡æ•°
        }
        
        # ä½¿ç”¨ç­–ç•¥IDä½œä¸ºæ¨¡æ¿é”®
        self.golden_templates[strategy_id] = template_data
        
        logger.info(f"ğŸ† æ–°é»„é‡‘æ¨¡æ¿è¯ç”Ÿï¼")
        logger.info(f"   ç­–ç•¥ID: {strategy_id}")
        logger.info(f"   è·¯å¾„ç±»å‹: {arm.option}")
        logger.info(f"   æˆåŠŸç‡: {arm.success_rate:.1%}")
        logger.info(f"   æ¿€æ´»æ¬¡æ•°: {arm.activation_count}")
        avg_rl_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history) if arm.rl_reward_history else 0.0
        logger.info(f"   å¹³å‡å¥–åŠ±: {avg_rl_reward:.3f}")
        logger.info(f"   å½“å‰é»„é‡‘æ¨¡æ¿æ€»æ•°: {len(self.golden_templates)}")
    
    def _update_golden_template(self, strategy_id: str, arm: EnhancedDecisionArm):
        """
        æ›´æ–°å·²æœ‰çš„é»„é‡‘æ¨¡æ¿æ•°æ® - ğŸ¯ ä¿®å¤ç‰ˆï¼šåŸºäºç­–ç•¥ID
        
        Args:
            strategy_id: ç­–ç•¥IDï¼ˆè€Œéå®ä¾‹IDï¼‰
            arm: å†³ç­–è‡‚å¯¹è±¡
        """
        if strategy_id in self.golden_templates:
            template = self.golden_templates[strategy_id]
            template.update({
                'success_rate': arm.success_rate,
                'total_activations': arm.activation_count,
                'average_reward': sum(arm.rl_reward_history) / len(arm.rl_reward_history) if arm.rl_reward_history else 0.0,
                'last_updated': time.time(),
                'stability_score': self._calculate_stability_score(arm)
            })
            
            logger.debug(f"ğŸ† æ›´æ–°é»„é‡‘æ¨¡æ¿: {strategy_id} -> æˆåŠŸç‡:{arm.success_rate:.1%}")
    
    def _calculate_stability_score(self, arm: EnhancedDecisionArm) -> float:
        """
        è®¡ç®—è·¯å¾„çš„ç¨³å®šæ€§åˆ†æ•°
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            ç¨³å®šæ€§åˆ†æ•° (0.0-1.0)
        """
        if arm.activation_count < 10:
            return 0.0
        
        # è®¡ç®—æˆåŠŸç‡çš„æ–¹å·®ï¼ˆè¶Šå°è¶Šç¨³å®šï¼‰
        recent_results = arm.recent_results[-20:] if len(arm.recent_results) >= 20 else arm.recent_results
        
        if len(recent_results) < 5:
            return 0.5  # æ ·æœ¬ä¸è¶³ï¼Œç»™ä¸­ç­‰åˆ†æ•°
        
        # è®¡ç®—æ»‘åŠ¨çª—å£æˆåŠŸç‡çš„æ–¹å·®
        window_size = 5
        success_rates = []
        
        for i in range(len(recent_results) - window_size + 1):
            window = recent_results[i:i + window_size]
            window_success_rate = sum(window) / len(window)
            success_rates.append(window_success_rate)
        
        if len(success_rates) < 2:
            return 0.5
        
        # æ–¹å·®è¶Šå°ï¼Œç¨³å®šæ€§è¶Šé«˜
        variance = np.var(success_rates)
        stability_score = max(0.0, 1.0 - variance * 4)  # å°†æ–¹å·®è½¬æ¢ä¸ºç¨³å®šæ€§åˆ†æ•°
        
        return stability_score
    
    def _remove_worst_golden_template(self):
        """
        ç§»é™¤è¡¨ç°æœ€å·®çš„é»„é‡‘æ¨¡æ¿
        """
        if not self.golden_templates:
            return
        
        # æŒ‰ç»¼åˆåˆ†æ•°æ’åºï¼Œç§»é™¤æœ€å·®çš„
        worst_template_id = min(self.golden_templates.keys(), 
                               key=lambda tid: self._calculate_template_quality_score(self.golden_templates[tid]))
        
        removed_template = self.golden_templates.pop(worst_template_id)
        
        logger.info(f"ğŸ—‘ï¸ ç§»é™¤è¡¨ç°è¾ƒå·®çš„é»„é‡‘æ¨¡æ¿: {worst_template_id}")
        logger.info(f"   åŸå› : ä¸ºæ–°æ¨¡æ¿è…¾å‡ºç©ºé—´")
        logger.info(f"   è¢«ç§»é™¤æ¨¡æ¿æˆåŠŸç‡: {removed_template['success_rate']:.1%}")
    
    def _calculate_template_quality_score(self, template_data: Dict[str, any]) -> float:
        """
        è®¡ç®—æ¨¡æ¿çš„è´¨é‡åˆ†æ•°
        
        Args:
            template_data: æ¨¡æ¿æ•°æ®
            
        Returns:
            è´¨é‡åˆ†æ•°
        """
        # ç»¼åˆè€ƒè™‘æˆåŠŸç‡ã€ä½¿ç”¨æ¬¡æ•°ã€ç¨³å®šæ€§ç­‰å› ç´ 
        success_score = template_data['success_rate'] * 0.4
        usage_score = min(template_data.get('usage_count', 0) / 10, 1.0) * 0.3  # ä½¿ç”¨æ¬¡æ•°æ ‡å‡†åŒ–
        stability_score = template_data.get('stability_score', 0.5) * 0.2
        recency_score = self._calculate_recency_score(template_data) * 0.1
        
        return success_score + usage_score + stability_score + recency_score
    
    def _calculate_recency_score(self, template_data: Dict[str, any]) -> float:
        """
        è®¡ç®—æ¨¡æ¿çš„æ–°è¿‘æ€§åˆ†æ•°
        
        Args:
            template_data: æ¨¡æ¿æ•°æ®
            
        Returns:
            æ–°è¿‘æ€§åˆ†æ•° (0.0-1.0)
        """
        current_time = time.time()
        last_updated = template_data.get('last_updated', template_data.get('created_timestamp', current_time))
        
        # è®¡ç®—è·ç¦»ä¸Šæ¬¡æ›´æ–°çš„æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        hours_since_update = (current_time - last_updated) / 3600
        
        # 24å°æ—¶å†…æ›´æ–°å¾—æ»¡åˆ†ï¼Œè¶…è¿‡7å¤©å¼€å§‹è¡°å‡
        if hours_since_update <= 24:
            return 1.0
        elif hours_since_update <= 168:  # 7å¤©
            return 1.0 - (hours_since_update - 24) / 144  # çº¿æ€§è¡°å‡
        else:
            return 0.0
    
    # ==================== ğŸ† é»„é‡‘æ¨¡æ¿ç®¡ç†æ¥å£ ====================
    
    def get_golden_templates(self) -> Dict[str, Dict[str, any]]:
        """
        è·å–æ‰€æœ‰é»„é‡‘æ¨¡æ¿
        
        Returns:
            é»„é‡‘æ¨¡æ¿å­—å…¸
        """
        return self.golden_templates.copy()
    
    def get_golden_template_stats(self) -> Dict[str, any]:
        """
        è·å–é»„é‡‘æ¨¡æ¿ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.golden_templates:
            return {
                'total_templates': 0,
                'avg_success_rate': 0.0,
                'total_usage_count': 0,
                'most_used_template': None,
                'template_usage_stats': {},
                'match_history_count': len(self.template_match_history) if hasattr(self, 'template_match_history') else 0
            }
        
        success_rates = [t['success_rate'] for t in self.golden_templates.values()]
        usage_counts = [self.template_usage_stats.get(tid, 0) for tid in self.golden_templates.keys()]
        
        most_used_template_id = max(self.template_usage_stats.keys(), 
                                   key=self.template_usage_stats.get) if self.template_usage_stats else None
        
        return {
            'total_templates': len(self.golden_templates),
            'avg_success_rate': sum(success_rates) / len(success_rates),
            'total_usage_count': sum(usage_counts),
            'most_used_template': {
                'template_id': most_used_template_id,
                'usage_count': self.template_usage_stats.get(most_used_template_id, 0),
                'template_data': self.golden_templates.get(most_used_template_id)
            } if most_used_template_id else None,
            'template_usage_stats': dict(self.template_usage_stats),
            'match_history_count': len(self.template_match_history)
        }
    
    def remove_golden_template(self, template_id: str) -> bool:
        """
        æ‰‹åŠ¨ç§»é™¤æŒ‡å®šçš„é»„é‡‘æ¨¡æ¿
        
        Args:
            template_id: æ¨¡æ¿ID
            
        Returns:
            æ˜¯å¦æˆåŠŸç§»é™¤
        """
        if template_id in self.golden_templates:
            removed_template = self.golden_templates.pop(template_id)
            logger.info(f"ğŸ—‘ï¸ æ‰‹åŠ¨ç§»é™¤é»„é‡‘æ¨¡æ¿: {template_id}")
            logger.info(f"   æ¨¡æ¿ç±»å‹: {removed_template['path_type']}")
            return True
        else:
            logger.warning(f"âš ï¸ é»„é‡‘æ¨¡æ¿ {template_id} ä¸å­˜åœ¨")
            return False
    
    def clear_golden_templates(self):
        """
        æ¸…ç©ºæ‰€æœ‰é»„é‡‘æ¨¡æ¿
        """
        count = len(self.golden_templates)
        self.golden_templates.clear()
        self.template_usage_stats.clear()
        self.template_match_history.clear()
        
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç©ºæ‰€æœ‰é»„é‡‘æ¨¡æ¿ (å…± {count} ä¸ª)")
    
    def export_golden_templates(self) -> str:
        """
        å¯¼å‡ºé»„é‡‘æ¨¡æ¿æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
        
        Returns:
            JSONå­—ç¬¦ä¸²
        """
        import json
        export_data = {
            'golden_templates': self.golden_templates,
            'template_usage_stats': dict(self.template_usage_stats),
            'template_match_history': self.template_match_history[-50:],  # åªå¯¼å‡ºæœ€è¿‘50æ¡åŒ¹é…å†å²
            'export_timestamp': time.time(),
            'config': self.golden_template_config
        }
        
        return json.dumps(export_data, indent=2, ensure_ascii=False)
    
    def import_golden_templates(self, json_data: str) -> bool:
        """
        å¯¼å…¥é»„é‡‘æ¨¡æ¿æ•°æ®
        
        Args:
            json_data: JSONå­—ç¬¦ä¸²
            
        Returns:
            æ˜¯å¦æˆåŠŸå¯¼å…¥
        """
        try:
            import json
            data = json.loads(json_data)
            
            # éªŒè¯æ•°æ®æ ¼å¼
            if 'golden_templates' not in data:
                logger.error("âŒ å¯¼å…¥æ•°æ®æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘golden_templateså­—æ®µ")
                return False
            
            # å¯¼å…¥æ¨¡æ¿
            imported_count = 0
            for template_id, template_data in data['golden_templates'].items():
                if len(self.golden_templates) < self.golden_template_config['max_golden_templates']:
                    self.golden_templates[template_id] = template_data
                    imported_count += 1
                else:
                    break
            
            # å¯¼å…¥ä½¿ç”¨ç»Ÿè®¡
            if 'template_usage_stats' in data:
                for template_id, count in data['template_usage_stats'].items():
                    if template_id in self.golden_templates:
                        self.template_usage_stats[template_id] = count
            
            logger.info(f"âœ… æˆåŠŸå¯¼å…¥ {imported_count} ä¸ªé»„é‡‘æ¨¡æ¿")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥é»„é‡‘æ¨¡æ¿å¤±è´¥: {e}")
            return False
    
    # ==================== ğŸ† é»„é‡‘æ¨¡æ¿ä½¿ç”¨ç¤ºä¾‹ ====================
    
    def demo_golden_template_workflow(self):
        """
        æ¼”ç¤ºé»„é‡‘æ¨¡æ¿ç³»ç»Ÿçš„å®Œæ•´å·¥ä½œæµç¨‹
        
        è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–¹æ³•ï¼Œå±•ç¤ºäº†é»„é‡‘æ¨¡æ¿ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½
        """
        logger.info("ğŸ† å¼€å§‹é»„é‡‘æ¨¡æ¿ç³»ç»Ÿæ¼”ç¤º")
        
        # 1. æ˜¾ç¤ºå½“å‰çŠ¶æ€
        stats = self.get_golden_template_stats()
        logger.info(f"å½“å‰é»„é‡‘æ¨¡æ¿æ•°é‡: {stats['total_templates']}")
        
        # 2. æ˜¾ç¤ºé…ç½®
        config = self.golden_template_config
        logger.info(f"é»„é‡‘æ¨¡æ¿é…ç½®:")
        logger.info(f"  - æˆåŠŸç‡é˜ˆå€¼: {config['success_rate_threshold']:.1%}")
        logger.info(f"  - æœ€å°æ ·æœ¬æ•°: {config['min_samples_required']}")
        logger.info(f"  - æœ€å¤§æ¨¡æ¿æ•°: {config['max_golden_templates']}")
        
        # 3. æ˜¾ç¤ºç°æœ‰é»„é‡‘æ¨¡æ¿
        if self.golden_templates:
            logger.info("ğŸ† ç°æœ‰é»„é‡‘æ¨¡æ¿:")
            for template_id, template_data in self.golden_templates.items():
                logger.info(f"  - {template_id}: {template_data['path_type']} "
                           f"(æˆåŠŸç‡: {template_data['success_rate']:.1%}, "
                           f"ä½¿ç”¨æ¬¡æ•°: {self.template_usage_stats.get(template_id, 0)})")
        else:
            logger.info("ğŸ“ æš‚æ— é»„é‡‘æ¨¡æ¿")
        
        # 4. æ˜¾ç¤ºå€™é€‰è·¯å¾„
        candidate_paths = []
        for path_id, arm in self.path_arms.items():
            if self._check_golden_criteria(arm) and path_id not in self.golden_templates:
                candidate_paths.append((path_id, arm))
        
        if candidate_paths:
            logger.info("â­ ç¬¦åˆé»„é‡‘æ¨¡æ¿æ¡ä»¶çš„å€™é€‰è·¯å¾„:")
            for path_id, arm in candidate_paths:
                stability = self._calculate_stability_score(arm)
                logger.info(f"  - {path_id}: {arm.option} "
                           f"(æˆåŠŸç‡: {arm.success_rate:.1%}, "
                           f"æ ·æœ¬: {arm.activation_count}, "
                           f"ç¨³å®šæ€§: {stability:.2f})")
        else:
            logger.info("ğŸ“ æš‚æ— ç¬¦åˆæ¡ä»¶çš„å€™é€‰è·¯å¾„")
        
        logger.info("ğŸ† é»„é‡‘æ¨¡æ¿ç³»ç»Ÿæ¼”ç¤ºå®Œæˆ")
    
    # ==================== ğŸ’¡ Aha-Momentå†³ç­–æ”¯æŒç³»ç»Ÿ ====================
    
    def get_path_confidence(self, strategy_id: str) -> float:
        """
        è·å–æŒ‡å®šç­–ç•¥çš„ç½®ä¿¡åº¦åˆ†æ•°
        
        Args:
            strategy_id: ç­–ç•¥IDï¼ˆæ³¨æ„ï¼šè¿™é‡Œåº”è¯¥ä¼ é€’strategy_idè€Œä¸æ˜¯path_idå®ä¾‹IDï¼‰
            
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•° (0.0-1.0)ï¼Œ1.0è¡¨ç¤ºéå¸¸æœ‰ä¿¡å¿ƒï¼Œ0.0è¡¨ç¤ºå®Œå…¨æ²¡æœ‰ä¿¡å¿ƒ
        """
        # ğŸ”§ åŠ¨æ€åˆ›å»ºï¼šå¦‚æœç­–ç•¥ä¸å­˜åœ¨ï¼Œåˆ™åŠ¨æ€åˆ›å»ºï¼ˆç½®ä¿¡åº¦ä¸ºæœ€ä½ï¼‰
        arm = self._create_strategy_arm_if_missing(strategy_id)
        
        # å¦‚æœæ ·æœ¬æ•°ä¸è¶³ï¼Œç½®ä¿¡åº¦è¾ƒä½
        if arm.activation_count < 5:
            base_confidence = 0.2  # åŸºç¡€ä¿¡å¿ƒå¾ˆä½
        elif arm.activation_count < 10:
            base_confidence = 0.4
        elif arm.activation_count < 20:
            base_confidence = 0.6
        else:
            base_confidence = 0.8  # å……è¶³æ ·æœ¬çš„åŸºç¡€ä¿¡å¿ƒ
        
        # åŸºäºæˆåŠŸç‡è°ƒæ•´ç½®ä¿¡åº¦
        success_factor = arm.success_rate
        
        # åŸºäºç¨³å®šæ€§è°ƒæ•´ç½®ä¿¡åº¦
        stability_factor = self._calculate_stability_score(arm) if arm.activation_count >= 10 else 0.5
        
        # åŸºäºæœ€è¿‘è¡¨ç°è°ƒæ•´ç½®ä¿¡åº¦
        recent_performance_factor = self._calculate_recent_performance_factor(arm)
        
        # ç»¼åˆè®¡ç®—ç½®ä¿¡åº¦
        confidence = (
            base_confidence * 0.3 +          # æ ·æœ¬é‡è´¡çŒ®30%
            success_factor * 0.4 +           # æˆåŠŸç‡è´¡çŒ®40%
            stability_factor * 0.2 +         # ç¨³å®šæ€§è´¡çŒ®20%
            recent_performance_factor * 0.1  # æœ€è¿‘è¡¨ç°è´¡çŒ®10%
        )
        
        return min(max(confidence, 0.0), 1.0)  # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
    
    def _calculate_recent_performance_factor(self, arm: EnhancedDecisionArm) -> float:
        """
        è®¡ç®—æœ€è¿‘è¡¨ç°å› å­
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            æœ€è¿‘è¡¨ç°å› å­ (0.0-1.0)
        """
        if not arm.recent_results or len(arm.recent_results) < 3:
            return 0.5  # é»˜è®¤ä¸­ç­‰
        
        # è®¡ç®—æœ€è¿‘5æ¬¡çš„æˆåŠŸç‡
        recent_window = arm.recent_results[-5:]
        recent_success_rate = sum(recent_window) / len(recent_window)
        
        return recent_success_rate
    
    def get_all_paths_confidence(self) -> Dict[str, float]:
        """
        è·å–æ‰€æœ‰è·¯å¾„çš„ç½®ä¿¡åº¦
        
        Returns:
            è·¯å¾„IDåˆ°ç½®ä¿¡åº¦çš„æ˜ å°„
        """
        confidence_map = {}
        for path_id in self.path_arms.keys():
            confidence_map[path_id] = self.get_path_confidence(path_id)
        
        return confidence_map
    
    def check_low_confidence_scenario(self, threshold: float = 0.3) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å¤„äºä½ç½®ä¿¡åº¦åœºæ™¯ï¼ˆæ‰€æœ‰è·¯å¾„è¡¨ç°éƒ½å¾ˆå·®ï¼‰
        
        Args:
            threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¤ä¸ºæ˜¯ä½ç½®ä¿¡åº¦
            
        Returns:
            æ˜¯å¦æ‰€æœ‰è·¯å¾„éƒ½å¤„äºä½ç½®ä¿¡åº¦çŠ¶æ€
        """
        if not self.path_arms:
            return True  # æ²¡æœ‰è·¯å¾„æ•°æ®ï¼Œè®¤ä¸ºæ˜¯ä½ç½®ä¿¡åº¦åœºæ™¯
        
        confidence_scores = self.get_all_paths_confidence()
        
        if not confidence_scores:
            return True
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è·¯å¾„çš„ç½®ä¿¡åº¦éƒ½ä½äºé˜ˆå€¼
        max_confidence = max(confidence_scores.values())
        avg_confidence = sum(confidence_scores.values()) / len(confidence_scores)
        
        logger.debug(f"ğŸ’¡ ç½®ä¿¡åº¦æ£€æŸ¥: æœ€é«˜ç½®ä¿¡åº¦={max_confidence:.3f}, å¹³å‡ç½®ä¿¡åº¦={avg_confidence:.3f}, é˜ˆå€¼={threshold}")
        
        # å¦‚æœæœ€é«˜ç½®ä¿¡åº¦éƒ½ä½äºé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºéœ€è¦ç»•é“æ€è€ƒ
        return max_confidence < threshold
    
    def get_confidence_analysis(self) -> Dict[str, any]:
        """
        è·å–ç½®ä¿¡åº¦åˆ†ææŠ¥å‘Š
        
        Returns:
            ç½®ä¿¡åº¦åˆ†ææ•°æ®
        """
        confidence_scores = self.get_all_paths_confidence()
        
        if not confidence_scores:
            return {
                'total_paths': 0,
                'max_confidence': 0.0,
                'min_confidence': 0.0,
                'avg_confidence': 0.0,
                'low_confidence_paths': 0,
                'high_confidence_paths': 0,
                'confidence_distribution': {}
            }
        
        values = list(confidence_scores.values())
        low_confidence_count = sum(1 for conf in values if conf < 0.3)
        high_confidence_count = sum(1 for conf in values if conf > 0.7)
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒç»Ÿè®¡
        distribution = {
            'very_low (0.0-0.2)': sum(1 for conf in values if 0.0 <= conf < 0.2),
            'low (0.2-0.4)': sum(1 for conf in values if 0.2 <= conf < 0.4),
            'medium (0.4-0.6)': sum(1 for conf in values if 0.4 <= conf < 0.6),
            'high (0.6-0.8)': sum(1 for conf in values if 0.6 <= conf < 0.8),
            'very_high (0.8-1.0)': sum(1 for conf in values if 0.8 <= conf <= 1.0)
        }
        
        return {
            'total_paths': len(confidence_scores),
            'max_confidence': max(values),
            'min_confidence': min(values),
            'avg_confidence': sum(values) / len(values),
            'low_confidence_paths': low_confidence_count,
            'high_confidence_paths': high_confidence_count,
            'confidence_distribution': distribution,
            'detailed_scores': confidence_scores
        }
    
    # ==================== ğŸ”§ è¾…åŠ©è®¡ç®—æ–¹æ³• ====================
    
    def _calculate_recent_trend(self, arm: EnhancedDecisionArm) -> str:
        """
        è®¡ç®—è·¯å¾„çš„æœ€è¿‘æ€§èƒ½è¶‹åŠ¿
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            è¶‹åŠ¿å­—ç¬¦ä¸²: 'improving', 'declining', 'stable', 'insufficient_data'
        """
        if len(arm.recent_results) < 4:
            return 'insufficient_data'
        
        # å–æœ€è¿‘çš„ç»“æœï¼Œåˆ†ä¸ºä¸¤åŠè¿›è¡Œæ¯”è¾ƒ
        recent = arm.recent_results[-10:] if len(arm.recent_results) >= 10 else arm.recent_results
        mid_point = len(recent) // 2
        
        if mid_point < 2:
            return 'insufficient_data'
        
        # è®¡ç®—å‰åŠæ®µå’ŒååŠæ®µçš„æˆåŠŸç‡
        earlier_half = recent[:mid_point]
        later_half = recent[mid_point:]
        
        earlier_rate = sum(earlier_half) / len(earlier_half)
        later_rate = sum(later_half) / len(later_half)
        
        # åˆ¤æ–­è¶‹åŠ¿
        if later_rate > earlier_rate + 0.1:  # 10%çš„æ”¹å–„è§†ä¸ºimproving
            return 'improving'
        elif later_rate < earlier_rate - 0.1:  # 10%çš„ä¸‹é™è§†ä¸ºdeclining
            return 'declining'
        else:
            return 'stable'
    
    def _calculate_consecutive_successes(self, arm: EnhancedDecisionArm) -> int:
        """
        è®¡ç®—è¿ç»­æˆåŠŸæ¬¡æ•°
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            è¿ç»­æˆåŠŸæ¬¡æ•°
        """
        if not arm.recent_results:
            return 0
        
        consecutive_count = 0
        # ä»æœ€è¿‘çš„ç»“æœå¼€å§‹å¾€å‰æ•°
        for result in reversed(arm.recent_results):
            if result:  # å¦‚æœæ˜¯æˆåŠŸ
                consecutive_count += 1
            else:  # å¦‚æœå¤±è´¥äº†ï¼Œå°±åœæ­¢è®¡æ•°
                break
        
        return consecutive_count
    
    # ==================== ğŸ¯ æ ¹æºä¿®å¤å®Œæˆï¼šç§»é™¤å¤æ‚è§£æé€»è¾‘ ====================
    # æ³¨æ„ï¼š_resolve_strategy_id æ–¹æ³•å·²ç§»é™¤ï¼Œå› ä¸ºæ•°æ®æºå¤´ç°åœ¨ç›´æ¥æä¾›æ­£ç¡®çš„ç­–ç•¥ID
    
    def _infer_path_type_from_strategy_id(self, strategy_id: str) -> str:
        """
        ä»ç­–ç•¥IDæ¨æ–­è·¯å¾„ç±»å‹
        
        Args:
            strategy_id: ç­–ç•¥ID
            
        Returns:
            æ¨æ–­çš„è·¯å¾„ç±»å‹
        """
        # ç­–ç•¥IDåˆ°è·¯å¾„ç±»å‹çš„æ˜ å°„è¡¨
        strategy_to_type_mapping = {
            'systematic_analytical': 'ç³»ç»Ÿåˆ†æå‹',
            'creative_innovative': 'åˆ›æ–°çªç ´å‹',
            'critical_questioning': 'æ‰¹åˆ¤è´¨ç–‘å‹',
            'practical_pragmatic': 'å®ç”¨åŠ¡å®å‹',
            'holistic_comprehensive': 'æ•´ä½“ç»¼åˆå‹',
            'exploratory_investigative': 'æ¢ç´¢è°ƒç ”å‹',
            'collaborative_consultative': 'åä½œå’¨è¯¢å‹',
            'adaptive_flexible': 'é€‚åº”çµæ´»å‹',
            
            # å…¼å®¹æ€§æ˜ å°„ï¼ˆä¸­æ–‡è·¯å¾„ç±»å‹ï¼‰
            'ç³»ç»Ÿåˆ†æ': 'ç³»ç»Ÿåˆ†æå‹',
            'åˆ›æ–°çªç ´': 'åˆ›æ–°çªç ´å‹',
            'æ‰¹åˆ¤è´¨ç–‘': 'æ‰¹åˆ¤è´¨ç–‘å‹',
            'å®ç”¨åŠ¡å®': 'å®ç”¨åŠ¡å®å‹',
            'æ•´ä½“ç»¼åˆ': 'æ•´ä½“ç»¼åˆå‹',
            'æ¢ç´¢è°ƒç ”': 'æ¢ç´¢è°ƒç ”å‹',
            'åä½œå’¨è¯¢': 'åä½œå’¨è¯¢å‹',
            'é€‚åº”çµæ´»': 'é€‚åº”çµæ´»å‹'
        }
        
        # ç›´æ¥åŒ¹é…
        if strategy_id in strategy_to_type_mapping:
            return strategy_to_type_mapping[strategy_id]
        
        # æ¨¡ç³ŠåŒ¹é…
        strategy_lower = strategy_id.lower()
        for key, value in strategy_to_type_mapping.items():
            if key.lower() in strategy_lower or strategy_lower in key.lower():
                logger.debug(f"ğŸ” æ¨¡ç³ŠåŒ¹é…ç­–ç•¥ç±»å‹: {strategy_id} -> {value}")
                return value
        
        # åŸºäºå…³é”®è¯æ¨æ–­
        if 'systematic' in strategy_lower or 'analytical' in strategy_lower or 'ç³»ç»Ÿ' in strategy_id:
            return 'ç³»ç»Ÿåˆ†æå‹'
        elif 'creative' in strategy_lower or 'innovative' in strategy_lower or 'åˆ›æ–°' in strategy_id:
            return 'åˆ›æ–°çªç ´å‹'
        elif 'critical' in strategy_lower or 'questioning' in strategy_lower or 'æ‰¹åˆ¤' in strategy_id:
            return 'æ‰¹åˆ¤è´¨ç–‘å‹'
        elif 'practical' in strategy_lower or 'pragmatic' in strategy_lower or 'å®ç”¨' in strategy_id:
            return 'å®ç”¨åŠ¡å®å‹'
        elif 'holistic' in strategy_lower or 'comprehensive' in strategy_lower or 'æ•´ä½“' in strategy_id:
            return 'æ•´ä½“ç»¼åˆå‹'
        elif 'exploratory' in strategy_lower or 'investigative' in strategy_lower or 'æ¢ç´¢' in strategy_id:
            return 'æ¢ç´¢è°ƒç ”å‹'
        elif 'collaborative' in strategy_lower or 'consultative' in strategy_lower or 'åä½œ' in strategy_id:
            return 'åä½œå’¨è¯¢å‹'
        elif 'adaptive' in strategy_lower or 'flexible' in strategy_lower or 'é€‚åº”' in strategy_id:
            return 'é€‚åº”çµæ´»å‹'
        
        # é»˜è®¤è¿”å›
        logger.debug(f"âš ï¸ æ— æ³•æ¨æ–­è·¯å¾„ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤: {strategy_id} -> é€šç”¨æ–¹æ³•å‹")
        return 'é€šç”¨æ–¹æ³•å‹'
