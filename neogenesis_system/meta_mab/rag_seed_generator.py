
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
RAGç§å­ç”Ÿæˆå™¨ - åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ™ºèƒ½æ€ç»´ç§å­åˆ›å»ºå™¨
RAG Seed Generator - Intelligent thinking seed creator based on Retrieval-Augmented Generation

æ ¸å¿ƒèŒè´£ï¼š
1. ç†è§£é—®é¢˜å¹¶æ„æ€æœç´¢ç­–ç•¥ï¼šåˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œç”Ÿæˆç²¾å‡†æœç´¢å…³é”®è¯
2. æ‰§è¡Œç½‘ç»œæœç´¢ï¼šè°ƒç”¨WebSearchClientè·å–å®æ—¶ä¿¡æ¯
3. ç»¼åˆä¿¡æ¯ç”Ÿæˆç§å­ï¼šç»“åˆç”¨æˆ·é—®é¢˜å’Œæœç´¢ç»“æœï¼Œç”ŸæˆåŸºäºäº‹å®çš„æ€ç»´ç§å­

æŠ€æœ¯ç‰¹è‰²ï¼š
- åŸºäºAnthropicçš„contextual retrievalç†å¿µ
- å¤šæºä¿¡æ¯éªŒè¯å’Œäº¤å‰å¼•ç”¨
- ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä¿¡æ¯æ•´åˆ
- å®æ—¶ä¿¡æ¯è·å–èƒ½åŠ›
"""

import time
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

from .utils.search_client import WebSearchClient, SearchResult, SearchResponse
# from .utils.client_adapter import DeepSeekClientAdapter  # ä¸å†éœ€è¦ï¼Œä½¿ç”¨ä¾èµ–æ³¨å…¥
from .utils.common_utils import parse_json_response
from config import PROMPT_TEMPLATES, RAG_CONFIG

logger = logging.getLogger(__name__)


@dataclass
class RAGSearchStrategy:
    """RAGæœç´¢ç­–ç•¥æ•°æ®ç»“æ„"""
    primary_keywords: List[str]      # ä¸»è¦å…³é”®è¯
    secondary_keywords: List[str]    # æ¬¡è¦å…³é”®è¯  
    search_intent: str              # æœç´¢æ„å›¾
    domain_focus: str               # é¢†åŸŸèšç„¦
    information_types: List[str]    # éœ€è¦çš„ä¿¡æ¯ç±»å‹
    search_depth: str               # æœç´¢æ·±åº¦ (shallow/medium/deep)


@dataclass
class RAGInformationSynthesis:
    """RAGä¿¡æ¯ç»¼åˆç»“æœ"""
    contextual_seed: str            # ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ€ç»´ç§å­
    information_sources: List[str]  # ä¿¡æ¯æ¥æº
    confidence_score: float         # ä¿¡æ¯å¯ä¿¡åº¦
    key_insights: List[str]         # å…³é”®æ´å¯Ÿ
    knowledge_gaps: List[str]       # çŸ¥è¯†ç¼ºå£
    verification_status: str        # éªŒè¯çŠ¶æ€


class RAGSeedGenerator:
    """
    RAGç§å­ç”Ÿæˆå™¨ - ä¸“é—¨è´Ÿè´£åŸºäºå®æ—¶ä¿¡æ¯æ£€ç´¢çš„æ€ç»´ç§å­ç”Ÿæˆ
    
    è®¾è®¡ç†å¿µï¼š
    - é‡‡ç”¨"ç†è§£-æœç´¢-ç»¼åˆ"ä¸‰é˜¶æ®µæµç¨‹
    - å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä¿¡æ¯æ£€ç´¢
    - å¤šæºä¿¡æ¯éªŒè¯å’Œäº¤å‰å¼•ç”¨
    - ç”Ÿæˆå…·æœ‰ä¸°å¯Œä¸Šä¸‹æ–‡çš„æ€ç»´ç§å­
    """
    
    def __init__(self, api_key: str = "", search_engine: str = "duckduckgo", 
                 web_search_client=None, llm_client=None):
        """
        åˆå§‹åŒ–RAGç§å­ç”Ÿæˆå™¨
        
        Args:
            api_key: LLM APIå¯†é’¥ï¼ˆå‘åå…¼å®¹ï¼‰
            search_engine: æœç´¢å¼•æ“ç±»å‹ï¼ˆå‘åå…¼å®¹ï¼‰
            web_search_client: å…±äº«çš„Webæœç´¢å®¢æˆ·ç«¯ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
            llm_client: å…±äº«çš„LLMå®¢æˆ·ç«¯ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.api_key = api_key
        self.search_engine = search_engine
        
        # ğŸ”§ ä¾èµ–æ³¨å…¥ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„æœç´¢å®¢æˆ·ç«¯
        if web_search_client:
            self.web_search_client = web_search_client
            logger.info("ğŸ” RAGç§å­ç”Ÿæˆå™¨ä½¿ç”¨å…±äº«æœç´¢å®¢æˆ·ç«¯")
        else:
            # å‘åå…¼å®¹ï¼šåˆ›å»ºè‡ªå·±çš„æœç´¢å®¢æˆ·ç«¯
            self.web_search_client = WebSearchClient(
                search_engine=search_engine, 
                max_results=RAG_CONFIG.get("max_search_results", 8)
            )
            logger.info("ğŸ” RAGç§å­ç”Ÿæˆå™¨åˆ›å»ºç‹¬ç«‹æœç´¢å®¢æˆ·ç«¯")
        
        # ğŸ”§ ä¾èµ–æ³¨å…¥ï¼šä½¿ç”¨ä¼ å…¥çš„LLMå®¢æˆ·ç«¯ï¼ˆçº¯ä¾èµ–æ³¨å…¥æ¨¡å¼ï¼‰
        self.llm_client = llm_client
        if self.llm_client:
            logger.info("ğŸ§  RAGç§å­ç”Ÿæˆå™¨ä½¿ç”¨å…±äº«LLMå®¢æˆ·ç«¯")
        else:
            logger.warning("âš ï¸ æœªæä¾›LLMå®¢æˆ·ç«¯ï¼ŒRAGå°†è¿è¡Œåœ¨ä»…æœç´¢æ¨¡å¼")
            logger.info("ğŸ’¡ è¯·ç¡®ä¿ä»ä¸Šå±‚ï¼ˆMainControllerï¼‰ä¼ å…¥æœ‰æ•ˆçš„llm_client")
        
        # æ€§èƒ½ç»Ÿè®¡å’Œç¼“å­˜
        self.performance_stats = {
            'total_generations': 0,
            'successful_generations': 0,
            'avg_generation_time': 0.0,
            'search_success_rate': 0.0,
            'synthesis_success_rate': 0.0
        }
        
        # æœç´¢ç­–ç•¥ç¼“å­˜
        self.strategy_cache = {}  # æŸ¥è¯¢æ¨¡å¼ -> æœç´¢ç­–ç•¥
        self.information_cache = {}  # å…³é”®è¯ -> æœç´¢ç»“æœ
        self.synthesis_cache = {}  # æŸ¥è¯¢+ä¿¡æ¯å“ˆå¸Œ -> ç»¼åˆç»“æœ
        
        # RAGè´¨é‡è·Ÿè¸ª
        self.rag_quality_metrics = {
            'information_diversity': defaultdict(float),  # ä¿¡æ¯å¤šæ ·æ€§
            'source_reliability': defaultdict(float),    # æ¥æºå¯é æ€§
            'contextual_relevance': defaultdict(float),  # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
            'factual_accuracy': defaultdict(float)       # äº‹å®å‡†ç¡®æ€§
        }
        
        logger.info("ğŸš€ RAGç§å­ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ğŸ” æœç´¢å¼•æ“: {search_engine}")
        logger.info(f"   ğŸ§  AIåˆ†æ: {'å¯ç”¨' if self.llm_client else 'ç¦ç”¨'}")
    
    def generate_rag_seed(self, user_query: str, execution_context: Optional[Dict] = None) -> str:
        """
        ç”ŸæˆåŸºäºRAGçš„æ€ç»´ç§å­ - ä¸‰é˜¶æ®µæ ¸å¿ƒæµç¨‹
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            åŸºäºå®æ—¶ä¿¡æ¯çš„æ€ç»´ç§å­
        """
        start_time = time.time()
        self.performance_stats['total_generations'] += 1
        
        logger.info(f"ğŸ¯ å¼€å§‹RAGç§å­ç”Ÿæˆ: {user_query[:50]}...")
        
        try:
            # é˜¶æ®µä¸€ï¼šç†è§£é—®é¢˜å¹¶æ„æ€æœç´¢ç­–ç•¥
            search_strategy = self._analyze_and_plan_search(user_query, execution_context)
            logger.info(f"ğŸ“‹ æœç´¢ç­–ç•¥: {search_strategy.search_intent}")
            
            # é˜¶æ®µäºŒï¼šæ‰§è¡Œç½‘ç»œæœç´¢
            search_results = self._execute_web_search(search_strategy)
            logger.info(f"ğŸ” æœç´¢å®Œæˆ: è·å– {len(search_results)} æ¡ç»“æœ")
            
            # é˜¶æ®µä¸‰ï¼šç»¼åˆä¿¡æ¯å¹¶ç”Ÿæˆç§å­
            synthesis_result = self._synthesize_information(
                user_query, search_strategy, search_results, execution_context
            )
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            generation_time = time.time() - start_time
            self.performance_stats['successful_generations'] += 1
            self._update_performance_stats(generation_time)
            
            logger.info(f"âœ… RAGç§å­ç”ŸæˆæˆåŠŸ (è€—æ—¶: {generation_time:.2f}s)")
            logger.info(f"   ğŸ“Š ä¿¡æ¯å¯ä¿¡åº¦: {synthesis_result.confidence_score:.2f}")
            logger.info(f"   ğŸ“š ä¿¡æ¯æºæ•°é‡: {len(synthesis_result.information_sources)}")
            
            return synthesis_result.contextual_seed
            
        except Exception as e:
            logger.error(f"âŒ RAGç§å­ç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€åˆ°åŸºç¡€åˆ†ææ¨¡å¼
            return self._generate_fallback_seed(user_query, execution_context)
    
    def _analyze_and_plan_search(self, user_query: str, execution_context: Optional[Dict]) -> RAGSearchStrategy:
        """
        é˜¶æ®µä¸€ï¼šåˆ†æç”¨æˆ·é—®é¢˜å¹¶è§„åˆ’æœç´¢ç­–ç•¥
        
        æ ¸å¿ƒä»»åŠ¡ï¼š
        1. ç†è§£ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ„å›¾
        2. è¯†åˆ«å…³é”®æ¦‚å¿µå’Œå®ä½“
        3. ç¡®å®šæœç´¢é¢†åŸŸå’Œä¿¡æ¯ç±»å‹
        4. ç”Ÿæˆå¤šå±‚æ¬¡æœç´¢å…³é”®è¯
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æœç´¢ç­–ç•¥å¯¹è±¡
        """
        # æ£€æŸ¥ç­–ç•¥ç¼“å­˜
        cache_key = f"{user_query}_{hash(str(execution_context))}"
        if cache_key in self.strategy_cache:
            logger.debug("ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„æœç´¢ç­–ç•¥")
            return self.strategy_cache[cache_key]
        
        if self.llm_client:
            try:
                return self._llm_based_search_planning(user_query, execution_context)
            except Exception as e:
                logger.warning(f"âš ï¸ LLMæœç´¢è§„åˆ’å¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•: {e}")
        
        # å¯å‘å¼æœç´¢ç­–ç•¥ç”Ÿæˆ
        return self._heuristic_search_planning(user_query, execution_context)
    
    def _llm_based_search_planning(self, user_query: str, execution_context: Optional[Dict]) -> RAGSearchStrategy:
        """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æœç´¢ç­–ç•¥è§„åˆ’"""
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{user_query}_{hash(str(execution_context))}"
        if cache_key in self.strategy_cache:
            logger.debug("ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„LLMæœç´¢ç­–ç•¥")
            return self.strategy_cache[cache_key]
        
        context_info = ""
        if execution_context:
            context_items = [f"- {k}: {v}" for k, v in execution_context.items()]
            context_info = f"\n\nğŸ“‹ **æ‰§è¡Œä¸Šä¸‹æ–‡**:\n" + "\n".join(context_items)
        
        planning_prompt = f"""
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æ£€ç´¢ç­–ç•¥å¸ˆï¼Œè¯·ä¸ºä»¥ä¸‹ç”¨æˆ·é—®é¢˜åˆ¶å®šç²¾å‡†çš„æœç´¢ç­–ç•¥ã€‚

ğŸ¯ **ç”¨æˆ·é—®é¢˜**: {user_query}
{context_info}

ğŸ“ **ä»»åŠ¡è¦æ±‚**:
1. æ·±åº¦ç†è§£ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ„å›¾å’Œä¿¡æ¯éœ€æ±‚
2. è¯†åˆ«å…³é”®æ¦‚å¿µã€å®ä½“å’ŒæŠ€æœ¯æœ¯è¯­
3. ç¡®å®šæœ€ä½³æœç´¢é¢†åŸŸå’Œä¿¡æ¯ç±»å‹
4. ç”Ÿæˆå¤šå±‚æ¬¡ã€å¤šè§’åº¦çš„æœç´¢å…³é”®è¯ç»„åˆ
5. è¯„ä¼°æœç´¢æ·±åº¦éœ€æ±‚

ğŸ” **è¾“å‡ºæ ¼å¼** (ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼):
```json
{{
    "search_intent": "ç”¨æˆ·æœç´¢çš„æ ¸å¿ƒæ„å›¾æè¿°",
    "domain_focus": "ä¸»è¦é¢†åŸŸï¼ˆå¦‚ï¼šæŠ€æœ¯ã€å•†ä¸šã€å­¦æœ¯ã€æ–°é—»ç­‰ï¼‰",
    "primary_keywords": ["ä¸»è¦å…³é”®è¯1", "ä¸»è¦å…³é”®è¯2", "ä¸»è¦å…³é”®è¯3"],
    "secondary_keywords": ["è¡¥å……å…³é”®è¯1", "è¡¥å……å…³é”®è¯2"],
    "information_types": ["éœ€è¦çš„ä¿¡æ¯ç±»å‹ï¼Œå¦‚ï¼šå®šä¹‰ã€æ•™ç¨‹ã€æ¡ˆä¾‹ã€ç»Ÿè®¡æ•°æ®"],
    "search_depth": "shallow/medium/deepï¼ˆæœç´¢æ·±åº¦ï¼‰"
}}
```

è¯·åŸºäºé—®é¢˜çš„å¤æ‚æ€§å’Œæ—¶æ•ˆæ€§éœ€æ±‚ï¼Œåˆ¶å®šæœ€ä¼˜çš„æœç´¢ç­–ç•¥ã€‚
"""
        
        llm_response = self.llm_client.call_api(planning_prompt, temperature=0.3)
        strategy_data = parse_json_response(llm_response)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„æœç´¢ç­–ç•¥å­—æ®µ
        required_fields = ['search_intent', 'domain_focus', 'primary_keywords']
        if strategy_data and all(field in strategy_data for field in required_fields):
            strategy = RAGSearchStrategy(
                primary_keywords=strategy_data.get('primary_keywords', []),
                secondary_keywords=strategy_data.get('secondary_keywords', []),
                search_intent=strategy_data.get('search_intent', ''),
                domain_focus=strategy_data.get('domain_focus', 'general'),
                information_types=strategy_data.get('information_types', []),
                search_depth=strategy_data.get('search_depth', 'medium')
            )
            
            # ç¼“å­˜ç­–ç•¥
            self.strategy_cache[cache_key] = strategy
            
            return strategy
        else:
            raise ValueError("LLMæœç´¢ç­–ç•¥è§£æå¤±è´¥")
    
    def _heuristic_search_planning(self, user_query: str, execution_context: Optional[Dict]) -> RAGSearchStrategy:
        """åŸºäºå¯å‘å¼è§„åˆ™çš„æœç´¢ç­–ç•¥ç”Ÿæˆ"""
        
        # åŸºç¡€å…³é”®è¯æå–
        import re
        words = re.findall(r'\b\w+\b', user_query.lower())
        
        # æŠ€æœ¯æœ¯è¯­è¯†åˆ«ï¼ˆåŒ…å«ä¸­è‹±æ–‡ï¼‰
        tech_terms = [
            'api', 'algorithm', 'database', 'system', 'architecture', 'optimization',
            'machine learning', 'ml', 'ai', 'artificial intelligence', 'deep learning',
            'network', 'crawler', 'data analysis', 'real-time', 'performance',
            'rag', 'retrieval', 'generation', 'llm', 'transformer',
            # ä¸­æ–‡æŠ€æœ¯æœ¯è¯­
            'æœºå™¨å­¦ä¹ ', 'ç®—æ³•', 'æ¶æ„', 'æ•°æ®åº“', 'ç³»ç»Ÿ', 'ä¼˜åŒ–', 'äººå·¥æ™ºèƒ½',
            'æ·±åº¦å­¦ä¹ ', 'ç½‘ç»œ', 'æ€§èƒ½', 'åˆ†å¸ƒå¼', 'å®æ—¶', 'é«˜æ€§èƒ½'
        ]
        
        primary_keywords = []
        secondary_keywords = []
        
        # è¯†åˆ«æŠ€æœ¯å…³é”®è¯
        for term in tech_terms:
            if term in user_query.lower():
                primary_keywords.append(term)
        
        # æ·»åŠ åŸå§‹æŸ¥è¯¢çš„ä¸»è¦è¯æ±‡
        important_words = [w for w in words if len(w) > 3][:5]
        primary_keywords.extend(important_words)
        
        # å»é‡
        primary_keywords = list(set(primary_keywords))[:6]
        
        # ç¡®å®šé¢†åŸŸå’Œæ„å›¾
        if any(term in user_query.lower() for term in ['how', 'what', 'why', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ']):
            search_intent = "å¯»æ‰¾è§£é‡Šæˆ–æŒ‡å¯¼ä¿¡æ¯"
            information_types = ["æ•™ç¨‹", "å®šä¹‰", "æŒ‡å—"]
        elif any(term in user_query.lower() for term in ['best', 'compare', 'æœ€å¥½', 'æ¯”è¾ƒ']):
            search_intent = "å¯»æ‰¾æ¯”è¾ƒå’Œæ¨èä¿¡æ¯"
            information_types = ["æ¯”è¾ƒ", "è¯„æµ‹", "æ¨è"]
        else:
            search_intent = "å¯»æ‰¾ç›¸å…³äº‹å®ä¿¡æ¯"
            information_types = ["äº‹å®", "æ•°æ®", "æ¡ˆä¾‹"]
        
        # åˆ¤æ–­æœç´¢æ·±åº¦
        if len(user_query) > 100 or any(term in user_query.lower() for term in ['complex', 'advanced', 'å¤æ‚', 'é«˜çº§']):
            search_depth = "deep"
        elif len(user_query) < 30:
            search_depth = "shallow"
        else:
            search_depth = "medium"
        
        return RAGSearchStrategy(
            primary_keywords=primary_keywords,
            secondary_keywords=secondary_keywords,
            search_intent=search_intent,
            domain_focus="æŠ€æœ¯" if any(t in primary_keywords for t in tech_terms) else "é€šç”¨",
            information_types=information_types,
            search_depth=search_depth
        )
    
    def _execute_web_search(self, strategy: RAGSearchStrategy) -> List[SearchResult]:
        """
        é˜¶æ®µäºŒï¼šæ‰§è¡Œç½‘ç»œæœç´¢ï¼ˆæ”¯æŒå¹¶è¡Œä¼˜åŒ–ï¼‰
        
        æ ¸å¿ƒä»»åŠ¡ï¼š
        1. åŸºäºæœç´¢ç­–ç•¥æ‰§è¡Œå¤šè½®æœç´¢
        2. ç»„åˆä¸åŒå…³é”®è¯è¿›è¡Œå…¨é¢æ£€ç´¢
        3. è¿‡æ»¤å’Œå»é‡æœç´¢ç»“æœ
        4. æŒ‰ç›¸å…³æ€§æ’åº
        
        ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼š
        - æ”¯æŒå¹¶è¡Œæœç´¢ï¼Œå¤§å¹…ç¼©çŸ­æ€»è€—æ—¶
        - æ™ºèƒ½é…ç½®å¹¶å‘æ•°é‡
        - å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œé™çº§
        
        Args:
            strategy: æœç´¢ç­–ç•¥
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        all_results = []
        search_queries = []
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        # ä¸»è¦å…³é”®è¯ç»„åˆ
        for keyword in strategy.primary_keywords[:3]:  # é™åˆ¶ä¸»è¦æœç´¢æ¬¡æ•°
            search_queries.append(keyword)
        
        # ç»„åˆæŸ¥è¯¢ï¼ˆä¸»è¦+æ¬¡è¦å…³é”®è¯ï¼‰
        if strategy.secondary_keywords:
            for primary in strategy.primary_keywords[:2]:
                for secondary in strategy.secondary_keywords[:2]:
                    search_queries.append(f"{primary} {secondary}")
        
        # é™åˆ¶æ€»æœç´¢æ¬¡æ•°
        search_queries = search_queries[:5]
        
        logger.info(f"ğŸ” æ‰§è¡Œ {len(search_queries)} è½®æœç´¢æŸ¥è¯¢")
        
        # ğŸš€ å†³å®šä½¿ç”¨å¹¶è¡Œæœç´¢è¿˜æ˜¯ä¸²è¡Œæœç´¢
        enable_parallel = RAG_CONFIG.get("enable_parallel_search", True)
        max_workers = RAG_CONFIG.get("max_search_workers", 3)
        
        if enable_parallel and len(search_queries) > 1:
            logger.info(f"âš¡ å¯ç”¨å¹¶è¡Œæœç´¢æ¨¡å¼ - æœ€å¤§å¹¶å‘æ•°: {max_workers}")
            all_results = self._execute_parallel_search(search_queries, max_workers)
        else:
            logger.info("ğŸ“Š ä½¿ç”¨ä¼ ç»Ÿä¸²è¡Œæœç´¢æ¨¡å¼")
            all_results = self._execute_serial_search(search_queries)
        
        # å»é‡å’Œè¿‡æ»¤
        unique_results = self._filter_and_deduplicate_results(all_results, strategy)
        
        logger.info(f"ğŸ¯ æœç´¢å®Œæˆ: {len(all_results)} -> {len(unique_results)} (å»é‡å)")
        return unique_results
    
    def _execute_parallel_search(self, search_queries: List[str], max_workers: int) -> List[SearchResult]:
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæœç´¢æŸ¥è¯¢
        
        Args:
            search_queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        all_results = []
        start_time = time.time()
        
        # åˆ›å»ºæœç´¢ä»»åŠ¡
        def search_single_query(query: str) -> Tuple[str, List[SearchResult]]:
            """æ‰§è¡Œå•ä¸ªæœç´¢æŸ¥è¯¢"""
            try:
                logger.debug(f"   ğŸ” å¹¶è¡Œæœç´¢: {query}")
                response = self.web_search_client.search(query)
                
                if response and response.results:
                    logger.debug(f"   âœ… æœç´¢æˆåŠŸ: {query} -> {len(response.results)} æ¡ç»“æœ")
                    return query, response.results
                else:
                    logger.debug(f"   ğŸ” æœç´¢æ— ç»“æœ: {query}")
                    return query, []
                    
            except Exception as e:
                logger.warning(f"âš ï¸ æœç´¢å¤±è´¥ '{query}': {e}")
                return query, []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œæœç´¢
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æœç´¢ä»»åŠ¡
            future_to_query = {
                executor.submit(search_single_query, query): query 
                for query in search_queries
            }
            
            # æ”¶é›†ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_query):
                query = future_to_query[future]
                try:
                    query_name, results = future.result()
                    all_results.extend(results)
                    completed_count += 1
                    
                    logger.debug(f"âœ… æœç´¢å®Œæˆ ({completed_count}/{len(search_queries)}): {query_name}")
                    
                except Exception as e:
                    logger.error(f"âŒ æœç´¢ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {query} - {e}")
        
        duration = time.time() - start_time
        logger.info(f"ğŸ¯ å¹¶è¡Œæœç´¢å®Œæˆ - è€—æ—¶: {duration:.2f}s, è·å¾— {len(all_results)} æ¡ç»“æœ")
        
        return all_results
    
    def _execute_serial_search(self, search_queries: List[str]) -> List[SearchResult]:
        """
        ä¸²è¡Œæ‰§è¡Œæœç´¢æŸ¥è¯¢ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
        
        Args:
            search_queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        all_results = []
        
        for query in search_queries:
            try:
                logger.debug(f"   æœç´¢: {query}")
                response = self.web_search_client.search(query)
                
                if response and response.results:
                    all_results.extend(response.results)
                    logger.debug(f"   è·å¾— {len(response.results)} æ¡ç»“æœ")
                else:
                    logger.debug(f"   æœç´¢æ— ç»“æœ: {query}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ æœç´¢å¤±è´¥ '{query}': {e}")
                continue
        
        return all_results
    
    def _filter_and_deduplicate_results(self, results: List[SearchResult], 
                                       strategy: RAGSearchStrategy) -> List[SearchResult]:
        """è¿‡æ»¤å’Œå»é‡æœç´¢ç»“æœ"""
        if not results:
            return []
        
        # æŒ‰URLå»é‡
        seen_urls = set()
        unique_results = []
        
        for result in results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                unique_results.append(result)
        
        # æŒ‰ç›¸å…³æ€§æ’åºï¼ˆåŸºäºæ ‡é¢˜å’Œæ‘˜è¦ä¸­å…³é”®è¯åŒ¹é…åº¦ï¼‰
        def relevance_score(result: SearchResult) -> float:
            score = 0.0
            text = f"{result.title} {result.snippet}".lower()
            
            # ä¸»è¦å…³é”®è¯æƒé‡æ›´é«˜
            for keyword in strategy.primary_keywords:
                if keyword.lower() in text:
                    score += 2.0
            
            # æ¬¡è¦å…³é”®è¯æƒé‡è¾ƒä½
            for keyword in strategy.secondary_keywords:
                if keyword.lower() in text:
                    score += 1.0
            
            return score
        
        # æ’åºå¹¶è¿”å›å‰Nä¸ªç»“æœ
        unique_results.sort(key=relevance_score, reverse=True)
        return unique_results[:8]  # è¿”å›æœ€ç›¸å…³çš„8ä¸ªç»“æœ
    
    def _synthesize_information(self, user_query: str, strategy: RAGSearchStrategy, 
                               search_results: List[SearchResult], 
                               execution_context: Optional[Dict]) -> RAGInformationSynthesis:
        """
        é˜¶æ®µä¸‰ï¼šç»¼åˆä¿¡æ¯å¹¶ç”Ÿæˆæ€ç»´ç§å­
        
        æ ¸å¿ƒä»»åŠ¡ï¼š
        1. åˆ†ææ‰€æœ‰æœç´¢ç»“æœçš„å†…å®¹
        2. è¯†åˆ«å…³é”®ä¿¡æ¯å’Œæ´å¯Ÿ
        3. éªŒè¯ä¿¡æ¯ä¸€è‡´æ€§å’Œå¯é æ€§
        4. ç”Ÿæˆä¸Šä¸‹æ–‡ä¸°å¯Œçš„æ€ç»´ç§å­
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            strategy: æœç´¢ç­–ç•¥
            search_results: æœç´¢ç»“æœ
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            ä¿¡æ¯ç»¼åˆç»“æœ
        """
        if not search_results:
            logger.warning("âš ï¸ æ— æœç´¢ç»“æœï¼Œç”ŸæˆåŸºç¡€ç§å­")
            return RAGInformationSynthesis(
                contextual_seed=f"åŸºäºç”¨æˆ·é—®é¢˜'{user_query}'çš„åŸºç¡€åˆ†æã€‚ç”±äºç¼ºä¹å®æ—¶ä¿¡æ¯ï¼Œå»ºè®®è¿›ä¸€æ­¥è°ƒç ”ç›¸å…³èµ„æ–™ã€‚",
                information_sources=[],
                confidence_score=0.3,
                key_insights=["éœ€è¦æ›´å¤šä¿¡æ¯"],
                knowledge_gaps=["å®æ—¶æ•°æ®ç¼ºå¤±"],
                verification_status="insufficient_data"
            )
        
        if self.llm_client:
            try:
                return self._llm_based_synthesis(user_query, strategy, search_results, execution_context)
            except Exception as e:
                logger.warning(f"âš ï¸ LLMä¿¡æ¯ç»¼åˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ³•: {e}")
        
        # åŸºç¡€ä¿¡æ¯ç»¼åˆ
        return self._basic_information_synthesis(user_query, strategy, search_results)
    
    def _llm_based_synthesis(self, user_query: str, strategy: RAGSearchStrategy,
                            search_results: List[SearchResult], 
                            execution_context: Optional[Dict]) -> RAGInformationSynthesis:
        """ä½¿ç”¨LLMè¿›è¡Œé«˜çº§ä¿¡æ¯ç»¼åˆ"""
        
        # æ„å»ºæœç´¢ç»“æœæ‘˜è¦
        results_summary = []
        for i, result in enumerate(search_results[:6], 1):  # é™åˆ¶ç»“æœæ•°é‡é¿å…tokenè¶…é™
            results_summary.append(f"""
**æ¥æº {i}**: {result.title}
- URL: {result.url}  
- æ‘˜è¦: {result.snippet}
""")
        
        context_info = ""
        if execution_context:
            context_items = [f"- {k}: {v}" for k, v in execution_context.items()]
            context_info = f"\n\nğŸ“‹ **æ‰§è¡Œä¸Šä¸‹æ–‡**:\n" + "\n".join(context_items)
        
        synthesis_prompt = f"""
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯åˆ†æå¸ˆï¼Œè¯·åŸºäºç”¨æˆ·é—®é¢˜å’Œæœç´¢åˆ°çš„å®æ—¶ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªå…¨é¢ã€å®¢è§‚ã€åŸºäºäº‹å®çš„æ€ç»´ç§å­ã€‚

ğŸ¯ **ç”¨æˆ·é—®é¢˜**: {user_query}

ğŸ” **æœç´¢ç­–ç•¥**: {strategy.search_intent}
**å…³æ³¨é¢†åŸŸ**: {strategy.domain_focus}

ğŸ“š **æœç´¢ç»“æœ**:
{"".join(results_summary)}
{context_info}

ğŸ“ **ç»¼åˆè¦æ±‚**:
1. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**: å……åˆ†ç†è§£ç”¨æˆ·é—®é¢˜çš„èƒŒæ™¯å’Œæ„å›¾
2. **äº‹å®åŸºç¡€**: åŸºäºæœç´¢ç»“æœçš„çœŸå®ä¿¡æ¯ï¼Œé¿å…çŒœæµ‹
3. **ä¿¡æ¯æ•´åˆ**: å°†å¤šä¸ªæ¥æºçš„ä¿¡æ¯è¿›è¡Œæœ‰æœºæ•´åˆ
4. **å…³é”®æ´å¯Ÿ**: æå–æœ€é‡è¦çš„è§‚ç‚¹å’Œå‘ç°
5. **çŸ¥è¯†ç¼ºå£**: è¯†åˆ«ä¿¡æ¯ä¸è¶³æˆ–éœ€è¦è¿›ä¸€æ­¥éªŒè¯çš„é¢†åŸŸ
6. **å®ç”¨å¯¼å‘**: ç”Ÿæˆå¯¹åç»­å†³ç­–æœ‰å¸®åŠ©çš„æ€è€ƒæ–¹å‘

ğŸ¯ **è¾“å‡ºæ ¼å¼** (ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼):
```json
{{
    "contextual_seed": "åŸºäºå®æ—¶ä¿¡æ¯çš„å…¨é¢æ€ç»´ç§å­ï¼ˆ200-400å­—ï¼‰",
    "key_insights": ["å…³é”®æ´å¯Ÿ1", "å…³é”®æ´å¯Ÿ2", "å…³é”®æ´å¯Ÿ3"],
    "knowledge_gaps": ["éœ€è¦è¿›ä¸€æ­¥äº†è§£çš„æ–¹é¢1", "éœ€è¦è¿›ä¸€æ­¥äº†è§£çš„æ–¹é¢2"],
    "confidence_score": 0.85,
    "information_sources": ["å¯é æ¥æº1", "å¯é æ¥æº2"],
    "verification_status": "verified/partially_verified/needs_verification"
}}
```

è¯·ç¡®ä¿ç”Ÿæˆçš„æ€ç»´ç§å­å…·æœ‰ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œèƒ½å¤Ÿä¸ºåç»­çš„æ€ç»´è·¯å¾„é€‰æ‹©æä¾›åšå®çš„äº‹å®åŸºç¡€ã€‚
"""
        
        llm_response = self.llm_client.call_api(synthesis_prompt, temperature=0.4)
        synthesis_data = parse_json_response(llm_response)
        
        if synthesis_data:
            return RAGInformationSynthesis(
                contextual_seed=synthesis_data.get('contextual_seed', ''),
                information_sources=synthesis_data.get('information_sources', []),
                confidence_score=float(synthesis_data.get('confidence_score', 0.5)),
                key_insights=synthesis_data.get('key_insights', []),
                knowledge_gaps=synthesis_data.get('knowledge_gaps', []),
                verification_status=synthesis_data.get('verification_status', 'unknown')
            )
        else:
            raise ValueError("LLMä¿¡æ¯ç»¼åˆè§£æå¤±è´¥")
    
    def _basic_information_synthesis(self, user_query: str, strategy: RAGSearchStrategy,
                                   search_results: List[SearchResult]) -> RAGInformationSynthesis:
        """åŸºç¡€ä¿¡æ¯ç»¼åˆæ–¹æ³•"""
        
        # æå–å…³é”®ä¿¡æ¯
        all_snippets = [result.snippet for result in search_results if result.snippet]
        all_titles = [result.title for result in search_results if result.title]
        
        # ç”ŸæˆåŸºç¡€ç§å­
        seed_parts = [
            f"åŸºäºå¯¹'{user_query}'çš„æœç´¢è°ƒç ”ï¼Œ",
            f"ä»{len(search_results)}ä¸ªä¿¡æ¯æºè·å¾—ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š",
        ]
        
        # æ·»åŠ ä¸»è¦ä¿¡æ¯ç‚¹
        for i, snippet in enumerate(all_snippets[:3], 1):
            seed_parts.append(f"{i}. {snippet[:100]}...")
        
        # æ·»åŠ ç»“è®º
        seed_parts.append(f"è¿™äº›ä¿¡æ¯è¡¨æ˜{strategy.search_intent.lower()}çš„é‡è¦æ€§ï¼Œ")
        seed_parts.append("å»ºè®®åœ¨åˆ¶å®šè§£å†³æ–¹æ¡ˆæ—¶å……åˆ†è€ƒè™‘è¿™äº›å®æ—¶ä¿¡æ¯ã€‚")
        
        contextual_seed = " ".join(seed_parts)
        
        return RAGInformationSynthesis(
            contextual_seed=contextual_seed,
            information_sources=[result.url for result in search_results[:3]],
            confidence_score=0.6,
            key_insights=all_titles[:3],
            knowledge_gaps=["éœ€è¦æ›´è¯¦ç»†çš„åˆ†æ"],
            verification_status="basic_verified"
        )
    
    def _generate_fallback_seed(self, user_query: str, execution_context: Optional[Dict]) -> str:
        """ç”Ÿæˆå›é€€æ€ç»´ç§å­"""
        logger.info("ğŸ”„ ä½¿ç”¨å›é€€æ¨¡å¼ç”Ÿæˆæ€ç»´ç§å­")
        
        fallback_seed = f"""
åŸºäºå¯¹é—®é¢˜'{user_query}'çš„åˆ†æï¼Œè¿™æ˜¯ä¸€ä¸ªéœ€è¦ç»¼åˆè€ƒè™‘çš„é—®é¢˜ã€‚
è™½ç„¶å½“å‰æ— æ³•è·å–å®æ—¶ä¿¡æ¯ï¼Œä½†å¯ä»¥ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œæ€è€ƒï¼š
1. é—®é¢˜çš„æ ¸å¿ƒè¦æ±‚å’Œçº¦æŸæ¡ä»¶
2. å¯èƒ½çš„è§£å†³æ–¹æ¡ˆå’Œå®ç°è·¯å¾„  
3. éœ€è¦è€ƒè™‘çš„é£é™©å’ŒæŒ‘æˆ˜
4. ç›¸å…³çš„æœ€ä½³å®è·µå’Œç»éªŒ
å»ºè®®åœ¨å…·ä½“å®æ–½å‰è·å–æœ€æ–°çš„ç›¸å…³ä¿¡æ¯å’Œæ•°æ®ã€‚
"""
        return fallback_seed.strip()
    
    def _update_performance_stats(self, generation_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        total = self.performance_stats['total_generations']
        current_avg = self.performance_stats['avg_generation_time']
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡æ—¶é—´
        if total == 1:
            self.performance_stats['avg_generation_time'] = generation_time
        else:
            self.performance_stats['avg_generation_time'] = (
                current_avg * (total - 1) + generation_time
            ) / total
        
        # æ›´æ–°æˆåŠŸç‡
        success_count = self.performance_stats['successful_generations']
        self.performance_stats['search_success_rate'] = success_count / total
    
    def get_rag_performance_stats(self) -> Dict[str, Any]:
        """è·å–RAGæ€§èƒ½ç»Ÿè®¡"""
        return {
            'component': 'RAG_Seed_Generator',
            'performance_stats': self.performance_stats.copy(),
            'cache_stats': {
                'strategy_cache_size': len(self.strategy_cache),
                'information_cache_size': len(self.information_cache),
                'synthesis_cache_size': len(self.synthesis_cache)
            },
            'quality_metrics': {
                'information_diversity': dict(self.rag_quality_metrics['information_diversity']),
                'source_reliability': dict(self.rag_quality_metrics['source_reliability']),
                'contextual_relevance': dict(self.rag_quality_metrics['contextual_relevance'])
            }
        }
    
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.strategy_cache.clear()
        self.information_cache.clear() 
        self.synthesis_cache.clear()
        logger.info("ğŸ§¹ RAGç§å­ç”Ÿæˆå™¨ç¼“å­˜å·²æ¸…ç©º")
