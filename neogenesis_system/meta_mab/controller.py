#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¸»æ§åˆ¶å™¨ - åè°ƒå„ä¸ªç»„ä»¶çš„å·¥ä½œ
Main Controller - coordinates the work of all components
"""

import time
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass

from .reasoner import PriorReasoner
from .path_generator import PathGenerator
from .mab_converger import MABConverger
from .data_structures import DecisionResult, SystemStatus
from .state_manager import StateManager, TaskPhase, GoalStatus
# ğŸ—‘ï¸ å·²ç§»é™¤ï¼šä¸å†ç›´æ¥å¯¼å…¥æœç´¢å®¢æˆ·ç«¯ï¼Œæ‰€æœ‰æœç´¢åŠŸèƒ½é€šè¿‡ToolRegistryè¿›è¡Œ
# from .utils.search_client import WebSearchClient, IdeaVerificationSearchClient, SearchResponse
from .utils.performance_optimizer import PerformanceOptimizer
from .utils.shutdown_manager import shutdown_neogenesis_system, register_for_shutdown

# ğŸ”§ æ–°å¢ï¼šå¯¼å…¥ç»Ÿä¸€å·¥å…·æŠ½è±¡æ¥å£
from .utils.tool_abstraction import (
    ToolRegistry, 
    global_tool_registry,
    register_tool,
    get_tool,
    execute_tool,
    search_tools,
    ToolCategory,
    ToolResult
)
from .utils.search_tools import (
    WebSearchTool,
    IdeaVerificationTool,
    create_and_register_search_tools
)

from config import SYSTEM_LIMITS, FEATURE_FLAGS, PROMPT_TEMPLATES, PERFORMANCE_CONFIG

logger = logging.getLogger(__name__)




@dataclass
class MainController:
    """ä¸»æ§åˆ¶å™¨ - åè°ƒå„ä¸ªç»„ä»¶çš„å·¥ä½œ"""
    
    def __init__(self, api_key: str = "", config=None):
        self.api_key = api_key
        self.config = config
        
        # ğŸ—ï¸ å¤šLLMæ”¯æŒï¼šä½¿ç”¨ç»Ÿä¸€çš„LLMç®¡ç†å™¨
        from .llm_manager import LLMManager
        
        logger.info(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–LLMç®¡ç†å™¨...")
        
        # å¦‚æœæä¾›äº†APIå¯†é’¥ï¼Œè®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå‘åå…¼å®¹ï¼‰
        if api_key and api_key.strip():
            import os
            os.environ.setdefault("DEEPSEEK_API_KEY", api_key.strip())
            logger.info(f"ğŸ”‘ APIå¯†é’¥å·²è®¾ç½®ä¸ºDEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
        
        # åˆ›å»ºLLMç®¡ç†å™¨
        try:
            self.llm_manager = LLMManager()
            self.llm_client = self.llm_manager  # å‘åå…¼å®¹
            
            status = self.llm_manager.get_provider_status()
            logger.info("ğŸ§  LLMç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            logger.info(f"   æ€»æä¾›å•†: {status['total_providers']}")
            logger.info(f"   å¥åº·æä¾›å•†: {status['healthy_providers']}")
            logger.info(f"   åˆå§‹åŒ–çŠ¶æ€: {'âœ…' if status['initialized'] else 'âŒ'}")
            
        except Exception as e:
            logger.error(f"âŒ LLMç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(f"   è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")
            
            # å›é€€åˆ°å•ä¸€å®¢æˆ·ç«¯æ¨¡å¼
            logger.warning("ğŸ”„ å›é€€åˆ°å•ä¸€DeepSeekå®¢æˆ·ç«¯æ¨¡å¼")
            self.llm_manager = None
            self.llm_client = self._create_fallback_client(api_key)
        
        # ğŸ”§ åˆå§‹åŒ–å·¥å…·æ³¨å†Œè¡¨ç³»ç»Ÿ
        self._initialize_tool_registry()
        
        # ğŸ—‘ï¸ å·²ç§»é™¤ï¼šä¸å†éœ€è¦ç›´æ¥çš„æœç´¢å®¢æˆ·ç«¯ï¼Œæ‰€æœ‰æœç´¢åŠŸèƒ½é€šè¿‡ToolRegistryè¿›è¡Œ
        
        # å®Œæˆå‰©ä½™çš„åˆå§‹åŒ–å·¥ä½œ
        self._complete_initialization()
    
    def _create_fallback_client(self, api_key: str):
        """åˆ›å»ºå›é€€å®¢æˆ·ç«¯"""
        if api_key and api_key.strip():
            try:
                from .utils.client_adapter import DeepSeekClientAdapter
                return DeepSeekClientAdapter(api_key.strip())
            except Exception as e:
                logger.error(f"âŒ å›é€€å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {e}")
                return None
        return None
    
    def _initialize_tool_registry(self):
        """åˆå§‹åŒ–å·¥å…·æ³¨å†Œè¡¨ç³»ç»Ÿ"""
        try:
            # ä½¿ç”¨å…¨å±€å·¥å…·æ³¨å†Œè¡¨
            self.tool_registry = global_tool_registry
            
            # åˆ›å»ºå¹¶æ³¨å†Œæœç´¢å·¥å…·
            tools = create_and_register_search_tools()
            
            logger.info(f"ğŸ”§ å·¥å…·æ³¨å†Œè¡¨åˆå§‹åŒ–å®Œæˆï¼Œå·²æ³¨å†Œ {len(tools)} ä¸ªå·¥å…·")
            for tool_name in tools:
                logger.debug(f"   - {tool_name}")
                
        except Exception as e:
            logger.error(f"âŒ å·¥å…·æ³¨å†Œè¡¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.tool_registry = None
    
    def _execute_llm_with_tools(self, prompt: str, context: Optional[Dict] = None, 
                               available_tools: Optional[List[str]] = None,
                               max_tool_calls: int = 3) -> Dict[str, Any]:
        """
        å¢å¼ºçš„LLMæ‰§è¡Œæ–¹æ³• - æ”¯æŒå·¥å…·è°ƒç”¨
        
        è¿™ä¸ªæ–¹æ³•å…è®¸LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ™ºèƒ½åœ°è°ƒç”¨å·¥å…·æ¥è·å–ä¿¡æ¯æˆ–æ‰§è¡Œæ“ä½œã€‚
        å·¥å…·è°ƒç”¨ç»“æœä¼šè¢«èå…¥åˆ°LLMçš„æ€è€ƒè¿‡ç¨‹ä¸­ï¼Œå®ç°çœŸæ­£çš„å·¥å…·å¢å¼ºæ¨ç†ã€‚
        
        Args:
            prompt: LLMæç¤ºè¯
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ 
            available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰å·²æ³¨å†Œå·¥å…·ï¼‰
            max_tool_calls: æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°
            
        Returns:
            åŒ…å«LLMå“åº”å’Œå·¥å…·è°ƒç”¨ç»“æœçš„å­—å…¸
        """
        start_time = time.time()
        
        # å‡†å¤‡å·¥å…·ä¿¡æ¯
        tool_descriptions = self._prepare_tool_descriptions(available_tools)
        
        # æ„å»ºå¢å¼ºçš„æç¤ºè¯
        enhanced_prompt = self._build_tool_enhanced_prompt(prompt, tool_descriptions, context)
        
        # æ‰§è¡Œç»“æœ
        result = {
            'llm_response': '',
            'tool_calls': [],
            'tool_results': {},
            'execution_time': 0.0,
            'success': True,
            'error_message': '',
            'context_updates': {}
        }
        
        try:
            # åˆå§‹LLMè°ƒç”¨
            logger.debug(f"ğŸ§  æ‰§è¡Œå·¥å…·å¢å¼ºLLMæ¨ç†ï¼Œæœ€å¤§å·¥å…·è°ƒç”¨: {max_tool_calls}")
            
            if self.llm_manager:
                llm_result = self.llm_manager.chat_completion(enhanced_prompt)
                if llm_result.success:
                    llm_response = llm_result.content
                else:
                    raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {llm_result.error_message}")
            elif self.llm_client:
                # æ£€æŸ¥å®¢æˆ·ç«¯ç±»å‹å¹¶è°ƒç”¨æ­£ç¡®çš„æ–¹æ³•
                if hasattr(self.llm_client, 'call_api'):
                    llm_response = self.llm_client.call_api(enhanced_prompt)
                elif hasattr(self.llm_client, 'chat_completion'):
                    llm_result = self.llm_client.chat_completion(enhanced_prompt)
                    if llm_result.success:
                        llm_response = llm_result.content
                    else:
                        raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {llm_result.error_message}")
                else:
                    raise Exception("LLMå®¢æˆ·ç«¯ä¸æ”¯æŒæ‰€éœ€çš„æ–¹æ³•")
            else:
                raise Exception("æ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯")
            
            result['llm_response'] = llm_response
            
            # è§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_calls_made = 0
            current_response = llm_response
            
            while tool_calls_made < max_tool_calls:
                # æ£€æµ‹å·¥å…·è°ƒç”¨æ„å›¾
                tool_call_request = self._detect_tool_call_intent(current_response)
                
                if not tool_call_request:
                    break
                
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_result = self._execute_detected_tool_call(tool_call_request)
                
                if tool_result:
                    result['tool_calls'].append(tool_call_request)
                    result['tool_results'][tool_call_request['tool_name']] = tool_result
                    
                    # å°†å·¥å…·ç»“æœèå…¥åˆ°ä¸‹ä¸€æ¬¡LLMè°ƒç”¨ä¸­
                    followup_prompt = self._build_tool_followup_prompt(
                        original_prompt=prompt,
                        previous_response=current_response,
                        tool_call=tool_call_request,
                        tool_result=tool_result
                    )
                    
                    # ä¸‹ä¸€æ¬¡LLMè°ƒç”¨
                    if self.llm_manager:
                        llm_result = self.llm_manager.chat_completion(followup_prompt)
                        if llm_result.success:
                            current_response = llm_result.content
                        else:
                            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {llm_result.error_message}")
                    elif self.llm_client:
                        # æ£€æŸ¥å®¢æˆ·ç«¯ç±»å‹å¹¶è°ƒç”¨æ­£ç¡®çš„æ–¹æ³•
                        if hasattr(self.llm_client, 'call_api'):
                            current_response = self.llm_client.call_api(followup_prompt)
                        elif hasattr(self.llm_client, 'chat_completion'):
                            llm_result = self.llm_client.chat_completion(followup_prompt)
                            if llm_result.success:
                                current_response = llm_result.content
                            else:
                                raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {llm_result.error_message}")
                        else:
                            raise Exception("LLMå®¢æˆ·ç«¯ä¸æ”¯æŒæ‰€éœ€çš„æ–¹æ³•")
                    
                    result['llm_response'] = current_response  # æ›´æ–°æœ€ç»ˆå“åº”
                    tool_calls_made += 1
                    
                    logger.debug(f"ğŸ”§ å·¥å…·è°ƒç”¨ {tool_calls_made}: {tool_call_request['tool_name']}")
                else:
                    break
            
            result['execution_time'] = time.time() - start_time
            logger.debug(f"âœ… å·¥å…·å¢å¼ºLLMæ‰§è¡Œå®Œæˆï¼Œè°ƒç”¨äº† {tool_calls_made} ä¸ªå·¥å…·")
            
        except Exception as e:
            result['success'] = False
            result['error_message'] = str(e)
            result['execution_time'] = time.time() - start_time
            logger.error(f"âŒ å·¥å…·å¢å¼ºLLMæ‰§è¡Œå¤±è´¥: {e}")
        
        return result
    
    def _execute_llm_with_mab_tools(self, prompt: str, context: Optional[Dict] = None, 
                                   max_tool_calls: int = 3) -> Dict[str, Any]:
        """
        ğŸ—ï¸ MABä¸»å¯¼çš„ä»£ç†æ‰§è¡Œæ–¹æ³• - é›†æˆStateManagerç»Ÿä¸€çŠ¶æ€ç®¡ç†
        
        æ–°çš„ä»£ç†æ‰§è¡Œæµç¨‹ï¼š
        1. åˆå§‹åŒ–StateManagerçŠ¶æ€
        2. å®šä¹‰å½“å‰çŠ¶æ€(State): ä»StateManagerè·å–ç»Ÿä¸€çŠ¶æ€
        3. è·å–å€™é€‰è¡ŒåŠ¨(Actions): ä»å·¥å…·æ³¨å†Œè¡¨è·å–æ‰€æœ‰å¯ç”¨å·¥å…·
        4. MABè¿›è¡Œå†³ç­–: è°ƒç”¨MABConvergeré€‰æ‹©æœ€ä¼˜å·¥å…·
        5. LLMç”Ÿæˆå‚æ•°: æ ¹æ®é€‰æ‹©ç»“æœç”Ÿæˆå‚æ•°æˆ–ç›´æ¥å›ç­”
        6. æ›´æ–°StateManagerçŠ¶æ€
        
        Args:
            prompt: LLMæç¤ºè¯
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            max_tool_calls: æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°
            
        Returns:
            åŒ…å«LLMå“åº”ã€å·¥å…·é€‰æ‹©å’Œæ‰§è¡Œç»“æœçš„å­—å…¸
        """
        start_time = time.time()
        logger.info("ğŸ¯ å¯åŠ¨MABä¸»å¯¼çš„ä»£ç†æ‰§è¡Œæµç¨‹ï¼ˆStateManageré›†æˆï¼‰")
        
        # ğŸ—ï¸ åˆå§‹åŒ–StateManagerçŠ¶æ€
        goal_id = self.state_manager.add_user_goal(prompt, self._infer_task_type(prompt))
        turn_id = self.state_manager.start_conversation_turn(prompt)
        self.state_manager.current_phase = TaskPhase.ANALYSIS
        
        # æ‰§è¡Œç»“æœ
        result = {
            'llm_response': '',
            'tool_calls': [],
            'tool_results': {},
            'mab_decisions': [],  # æ–°å¢ï¼šMABå†³ç­–å†å²
            'execution_time': 0.0,
            'success': True,
            'error_message': '',
            'context_updates': {},
            'mab_no_tool_decisions': 0,  # MABå†³å®šä¸ä½¿ç”¨å·¥å…·çš„æ¬¡æ•°
            # ğŸ—ï¸ StateManagerä¿¡æ¯
            'goal_id': goal_id,
            'turn_id': turn_id,
            'session_id': self.state_manager.session_id
        }
        
        try:
            current_response = ""
            tool_calls_made = 0
            
            # ä»£ç†å¾ªç¯ï¼šæ¯è½®è¿›è¡ŒMABå†³ç­–
            while tool_calls_made < max_tool_calls:
                logger.info(f"ğŸ”„ ä»£ç†å¾ªç¯ç¬¬ {tool_calls_made + 1} è½®")
                
                # æ­¥éª¤1: å®šä¹‰å½“å‰çŠ¶æ€(State)
                current_state = self._define_current_state(prompt, context, current_response, result)
                logger.debug(f"ğŸ“Š å½“å‰çŠ¶æ€å·²å®šä¹‰: {len(current_state)} ä¸ªçŠ¶æ€ç»´åº¦")
                
                # æ­¥éª¤2: è·å–å€™é€‰è¡ŒåŠ¨(Actions)
                available_tools = self._get_candidate_actions()
                logger.debug(f"ğŸ”§ è·å–å€™é€‰å·¥å…·: {len(available_tools)} ä¸ª")
                
                # æ­¥éª¤3: MABè¿›è¡Œå†³ç­–
                chosen_tool = self._mab_decide_action(current_state, available_tools)
                
                # è®°å½•MABå†³ç­–
                mab_decision = {
                    'round': tool_calls_made + 1,
                    'state_hash': hash(str(current_state)),
                    'available_tools': available_tools.copy(),
                    'chosen_tool': chosen_tool,
                    'timestamp': time.time()
                }
                result['mab_decisions'].append(mab_decision)
                
                # ğŸ—ï¸ æ›´æ–°StateManagerçš„MABå†³ç­–è®°å½•
                self.state_manager.add_mab_decision_to_turn(turn_id, mab_decision)
                
                # æ­¥éª¤4: LLMç”Ÿæˆå‚æ•°
                if chosen_tool is None:
                    # MABå†³å®šä¸ä½¿ç”¨å·¥å…·ï¼Œè®©LLMç›´æ¥å›ç­”
                    logger.info("ğŸš« MABå†³å®šä¸ä½¿ç”¨å·¥å…·ï¼ŒLLMç›´æ¥ç”Ÿæˆå›ç­”")
                    result['mab_no_tool_decisions'] += 1
                    
                    final_response = self._llm_generate_direct_answer(prompt, context, current_response)
                    result['llm_response'] = final_response
                    break
                else:
                    # MABé€‰æ‹©äº†å·¥å…·ï¼Œè®©LLMä¸ºè¯¥å·¥å…·ç”Ÿæˆå‚æ•°
                    logger.info(f"ğŸ¯ MABé€‰æ‹©å·¥å…·: {chosen_tool}")
                    
                    tool_execution_result = self._llm_generate_tool_parameters_and_execute(
                        chosen_tool, prompt, context, current_response
                    )
                    
                    if tool_execution_result['success']:
                        # å·¥å…·æ‰§è¡ŒæˆåŠŸ
                        result['tool_calls'].append(tool_execution_result['tool_call'])
                        result['tool_results'][chosen_tool] = tool_execution_result['tool_result']
                        current_response = tool_execution_result['llm_response']
                        result['llm_response'] = current_response
                        
                        # ğŸ—ï¸ æ›´æ–°StateManagerï¼šè®°å½•å·¥å…·è°ƒç”¨å’Œç»“æœ
                        self.state_manager.add_tool_call_to_turn(turn_id, tool_execution_result['tool_call'], 
                                                               tool_execution_result['tool_result'])
                        
                        # ğŸ—ï¸ æ·»åŠ ä¸­é—´ç»“æœåˆ°StateManager
                        self.state_manager.add_intermediate_result(
                            source=chosen_tool,
                            content=tool_execution_result['tool_result'],
                            quality_score=tool_execution_result['reward'],
                            relevance_score=min(1.0, tool_execution_result['reward'] + 0.2)
                        )
                        
                        # å‘MABæä¾›å·¥å…·æ‰§è¡Œåé¦ˆ
                        self._provide_mab_feedback(chosen_tool, True, tool_execution_result['reward'])
                        
                        tool_calls_made += 1
                        logger.info(f"âœ… å·¥å…· {chosen_tool} æ‰§è¡ŒæˆåŠŸï¼Œç»§ç»­ä¸‹ä¸€è½®")
                    else:
                        # å·¥å…·æ‰§è¡Œå¤±è´¥
                        logger.warning(f"âŒ å·¥å…· {chosen_tool} æ‰§è¡Œå¤±è´¥: {tool_execution_result['error']}")
                        
                        # å‘MABæä¾›è´Ÿé¢åé¦ˆ
                        self._provide_mab_feedback(chosen_tool, False, -0.5)
                        
                        # è®©LLMç”Ÿæˆæœ€ç»ˆå›ç­”
                        final_response = self._llm_generate_direct_answer(prompt, context, current_response)
                        result['llm_response'] = final_response
                        break
            
            # å¦‚æœè¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”
            if tool_calls_made >= max_tool_calls and not result['llm_response']:
                logger.info(f"ğŸ›‘ è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•° {max_tool_calls}ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”")
                final_response = self._llm_generate_direct_answer(prompt, context, current_response)
                result['llm_response'] = final_response
            
            result['execution_time'] = time.time() - start_time
            
            # ğŸ—ï¸ å®ŒæˆStateManagerçŠ¶æ€æ›´æ–°
            self.state_manager.complete_conversation_turn(turn_id, result['llm_response'], 
                                                        result['success'], result.get('error_message', ''))
            
            # ğŸ—ï¸ æ›´æ–°ç›®æ ‡è¿›åº¦
            goal_progress = min(1.0, 0.8 if result['success'] else 0.3)  # ç®€åŒ–çš„è¿›åº¦è®¡ç®—
            goal_status = GoalStatus.ACHIEVED if result['success'] else GoalStatus.PARTIALLY_ACHIEVED
            self.state_manager.update_goal_progress(goal_id, goal_progress, goal_status)
            
            logger.info(f"âœ… MABä¸»å¯¼ä»£ç†æ‰§è¡Œå®Œæˆï¼Œå·¥å…·è°ƒç”¨: {tool_calls_made}æ¬¡, MABæ‹’ç»å·¥å…·: {result['mab_no_tool_decisions']}æ¬¡")
            logger.info(f"ğŸ—ï¸ StateManagerçŠ¶æ€å·²æ›´æ–°ï¼Œç›®æ ‡è¿›åº¦: {goal_progress:.2%}")
            
        except Exception as e:
            result['success'] = False
            result['error_message'] = str(e)
            result['execution_time'] = time.time() - start_time
            
            # ğŸ—ï¸ é”™è¯¯æƒ…å†µä¸‹çš„StateManagerçŠ¶æ€æ›´æ–°
            self.state_manager.complete_conversation_turn(turn_id, "", False, str(e))
            self.state_manager.update_goal_progress(goal_id, 0.1, GoalStatus.FAILED)
            
            logger.error(f"âŒ MABä¸»å¯¼ä»£ç†æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(f"   è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")
        
        return result
    
    # ==================== ğŸ—ï¸ StateManagerè®¿é—®æ–¹æ³• ====================
    
    def get_state_manager_summary(self) -> Dict[str, Any]:
        """è·å–StateManagerçŠ¶æ€æ‘˜è¦"""
        if self.state_manager:
            return self.state_manager.get_summary()
        return {'error': 'StateManageræœªåˆå§‹åŒ–'}
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """è·å–å¯¹è¯å†å²"""
        if self.state_manager:
            return [turn.to_dict() if hasattr(turn, 'to_dict') else turn.__dict__ 
                   for turn in self.state_manager.conversation_history]
        return []
    
    def get_current_goals(self) -> List[Dict[str, Any]]:
        """è·å–å½“å‰ç›®æ ‡"""
        if self.state_manager:
            return [goal.__dict__ for goal in self.state_manager.user_goals]
        return []
    
    def get_intermediate_results(self) -> List[Dict[str, Any]]:
        """è·å–ä¸­é—´ç»“æœ"""
        if self.state_manager:
            return [result.__dict__ for result in self.state_manager.intermediate_results]
        return []
    
    def get_rl_features(self) -> Dict[str, float]:
        """è·å–RLç®—æ³•ç‰¹å¾"""
        if self.state_manager:
            return self.state_manager.get_state_features_for_rl()
        return {}
    
    def export_state_for_analysis(self) -> Dict[str, Any]:
        """å¯¼å‡ºå®Œæ•´çŠ¶æ€ç”¨äºåˆ†æ"""
        if self.state_manager:
            state_dict = self.state_manager.to_dict()
            state_dict['mab_feedback_stats'] = self.get_mab_feedback_statistics()
            return state_dict
        return {'error': 'StateManageræœªåˆå§‹åŒ–'}
    
    def reset_state_manager(self, new_session_id: Optional[str] = None):
        """é‡ç½®StateManagerï¼ˆç”¨äºæ–°ä¼šè¯ï¼‰"""
        if self.state_manager:
            old_session_id = self.state_manager.session_id
            self.state_manager = StateManager(new_session_id)
            logger.info(f"ğŸ—ï¸ StateManagerå·²é‡ç½®: {old_session_id} -> {self.state_manager.session_id}")
        else:
            logger.warning("âš ï¸ StateManageræœªåˆå§‹åŒ–ï¼Œæ— æ³•é‡ç½®")
    
    # ==================== ğŸ”§ MABä¸»å¯¼ä»£ç†æ‰§è¡Œçš„è¾…åŠ©æ–¹æ³• ====================
    
    def _define_current_state(self, prompt: str, context: Optional[Dict], 
                            current_response: str, result: Dict) -> Dict[str, Any]:
        """
        ğŸ—ï¸ æ­¥éª¤1: ä»StateManagerè·å–å½“å‰çŠ¶æ€ - æ”¯æŒç»Ÿä¸€çŠ¶æ€ç®¡ç†
        
        Args:
            prompt: åŸå§‹æç¤ºè¯
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            current_response: å½“å‰LLMå“åº”
            result: å½“å‰æ‰§è¡Œç»“æœ
            
        Returns:
            åŒ…å«æ‰€æœ‰ç›¸å…³çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
        """
        # ğŸ—ï¸ ä½¿ç”¨StateManagerè·å–å®Œæ•´çŠ¶æ€
        state_manager_state = self.state_manager.get_current_state()
        
        # ğŸ”§ è¡¥å……MABå†³ç­–æ‰€éœ€çš„å®æ—¶ä¿¡æ¯
        enhanced_state = state_manager_state.copy()
        enhanced_state.update({
            # å®æ—¶ä»»åŠ¡ä¿¡æ¯
            'original_prompt': prompt,
            'prompt_length': len(prompt) if prompt else 0,
            'prompt_complexity': self._estimate_prompt_complexity(prompt),
            
            # å®æ—¶å¯¹è¯çŠ¶æ€
            'has_current_response': bool(current_response),
            'current_response_length': len(current_response) if current_response else 0,
            'current_tool_calls': len(result.get('tool_calls', [])),
            
            # å®æ—¶ä¸Šä¸‹æ–‡ä¿¡æ¯
            'context_provided': context is not None,
            'context_keys': list(context.keys()) if context else [],
            'context_size': len(str(context)) if context else 0,
            
            # å®æ—¶å†³ç­–ä¿¡æ¯
            'session_mab_decisions': len(result.get('mab_decisions', [])),
            'session_tools_used': [call.get('tool_name') for call in result.get('tool_calls', [])],
            'last_tool_used': result.get('tool_calls', [{}])[-1].get('tool_name') if result.get('tool_calls') else None,
            
            # å®æ—¶æ€§èƒ½æŒ‡æ ‡
            'execution_time_so_far': result.get('execution_time', 0.0),
            
            # ä»»åŠ¡ç±»å‹æ¨æ–­
            'task_type': self._infer_task_type(prompt),
            'requires_search': self._requires_search_analysis(prompt),
            'requires_analysis': self._requires_analysis_capability(prompt),
            
            # RLç‰¹å¾
            'rl_features': self.state_manager.get_state_features_for_rl()
        })
        
        logger.debug(f"ğŸ—ï¸ çŠ¶æ€è·å–å®Œæˆ: ä»»åŠ¡ç±»å‹={enhanced_state['task_type']}, å¤æ‚åº¦={enhanced_state['prompt_complexity']}")
        logger.debug(f"ğŸ—ï¸ StateManagerçŠ¶æ€: è½®æ¬¡={enhanced_state['conversation']['total_turns']}, ç›®æ ‡è¿›åº¦={enhanced_state['current_goal']['progress']:.2%}")
        
        return enhanced_state
    
    def _get_candidate_actions(self) -> List[str]:
        """
        æ­¥éª¤2: è·å–å€™é€‰è¡ŒåŠ¨(Actions) - ä»å·¥å…·æ³¨å†Œè¡¨è·å–æ‰€æœ‰å¯ç”¨å·¥å…·
        
        Returns:
            å¯ç”¨å·¥å…·åç§°åˆ—è¡¨ï¼ŒåŒ…å«ç‰¹æ®Šçš„"no_tool"é€‰é¡¹
        """
        available_tools = []
        
        if self.tool_registry:
            # ä»å·¥å…·æ³¨å†Œè¡¨è·å–æ‰€æœ‰å·²æ³¨å†Œå·¥å…·
            for tool in self.tool_registry:
                if tool.name and hasattr(tool, 'is_enabled') and tool.is_enabled():
                    available_tools.append(tool.name)
                elif tool.name:  # å¦‚æœæ²¡æœ‰is_enabledæ–¹æ³•ï¼Œé»˜è®¤è®¤ä¸ºå¯ç”¨
                    available_tools.append(tool.name)
        
        # å§‹ç»ˆåŒ…å«"ä¸ä½¿ç”¨å·¥å…·"é€‰é¡¹
        available_tools.append("no_tool")
        
        logger.debug(f"ğŸ”§ å€™é€‰å·¥å…·: {available_tools}")
        return available_tools
    
    def _mab_decide_action(self, current_state: Dict[str, Any], available_tools: List[str]) -> Optional[str]:
        """
        ğŸŒŸ æ­¥éª¤3: æ··åˆå†³ç­–æ ¸å¿ƒ - MABç»éªŒåˆ¤æ–­ + LLMæ™ºèƒ½æ¢ç´¢
        
        è¿™æ˜¯æ•´ä¸ª"æ··åˆåŠ¨åŠ›"æ¨¡å¼çš„å¿ƒè„éƒ¨åˆ†ã€‚å†³ç­–æµç¨‹ï¼š
        1. MABæä¾›åˆå§‹å»ºè®®
        2. æ£€æŸ¥å»ºè®®å·¥å…·çš„ç»éªŒæ°´å¹³
        3. æ ¹æ®ç»éªŒæ°´å¹³æ™ºèƒ½åˆ‡æ¢å†³ç­–æ¨¡å¼ï¼š
           - ç»éªŒä¸°å¯Œ â†’ ä¿¡ä»»MABï¼Œé‡‡ç”¨ç»éªŒæ¨¡å¼
           - ç»éªŒä¸è¶³ â†’ åˆ‡æ¢LLMï¼Œé‡‡ç”¨æ¢ç´¢æ¨¡å¼
        
        Args:
            current_state: å½“å‰çŠ¶æ€ä¿¡æ¯
            available_tools: å€™é€‰å·¥å…·åˆ—è¡¨
            
        Returns:
            é€‰æ‹©çš„å·¥å…·åç§°ï¼Œæˆ–Noneè¡¨ç¤ºä¸ä½¿ç”¨å·¥å…·
        """
        if not self.mab_converger:
            logger.warning("âš ï¸ MABConvergeræœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°LLMæ¢ç´¢æ¨¡å¼")
            return self._llm_explore_tool_selection(current_state, available_tools)
        
        try:
            # ============= é˜¶æ®µ1: è¿‡æ»¤å’Œå‡†å¤‡å€™é€‰å·¥å…· =============
            # è¿‡æ»¤æ‰"no_tool"é€‰é¡¹ï¼ŒMABåªåœ¨çœŸå®å·¥å…·ä¸­é€‰æ‹©
            real_tools = [tool for tool in available_tools if tool != "no_tool"]
            
            if not real_tools:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„çœŸå®å·¥å…·ï¼Œè¿”å›no_tool")
                return None
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ï¼ˆåŸºç¡€æ¦‚ç‡åˆ¤æ–­ï¼‰
            no_tool_probability = self._calculate_no_tool_probability(current_state)
            import random
            if random.random() < no_tool_probability:
                logger.info(f"ğŸ² åŸºäºçŠ¶æ€åˆ†æï¼Œ{no_tool_probability:.2%}æ¦‚ç‡é€‰æ‹©ä¸ä½¿ç”¨å·¥å…·")
                return None
            
            logger.info(f"ğŸŒŸ å¼€å§‹æ··åˆå†³ç­–æµç¨‹ï¼Œå€™é€‰å·¥å…·: {len(real_tools)}ä¸ª")
            logger.debug(f"   å€™é€‰å·¥å…·åˆ—è¡¨: {', '.join(real_tools)}")
            
            # ============= é˜¶æ®µ2: MABåˆæ­¥å»ºè®® =============
            # è°ƒç”¨MABè·å–åˆæ­¥å»ºè®®ï¼ˆä½†ä¸ä¸€å®šé‡‡çº³ï¼‰
            mab_recommendation = self.mab_converger.select_best_tool(real_tools)
            logger.info(f"ğŸ¯ MABåˆæ­¥å»ºè®®: {mab_recommendation}")
            
            # ============= é˜¶æ®µ3: ç»éªŒæ°´å¹³æ£€æŸ¥ =============
            # ğŸ” å…³é”®æ­¥éª¤ï¼šæ£€æŸ¥MABæ¨èå·¥å…·çš„ç»éªŒæ°´å¹³
            cold_analysis = self.mab_converger.is_tool_cold(mab_recommendation)
            
            logger.info(f"ğŸ” ç»éªŒæ£€æŸ¥ç»“æœ:")
            logger.info(f"   å·¥å…·: {mab_recommendation}")
            logger.info(f"   çŠ¶æ€: {'ğŸ§Š å†·å¯åŠ¨' if cold_analysis['is_cold_start'] else 'ğŸ”¥ ç»éªŒä¸°å¯Œ'}")
            logger.info(f"   ç½®ä¿¡åº¦: {cold_analysis['confidence']:.2%}")
            logger.info(f"   ç†ç”±: {cold_analysis['reason']}")
            
            # ============= é˜¶æ®µ4: æ™ºèƒ½æ¨¡å¼åˆ‡æ¢ =============
            if cold_analysis['is_cold_start']:
                # ğŸ§  å·¥å…·ç»éªŒä¸è¶³ â†’ åˆ‡æ¢åˆ°LLMæ¢ç´¢æ¨¡å¼
                logger.info("ğŸ§  åˆ‡æ¢åˆ°æ¢ç´¢æ¨¡å¼ (LLMä¸»å¯¼å†³ç­–)")
                logger.info(f"   åŸå› : MABæ¨èçš„å·¥å…· '{mab_recommendation}' å¤„äºå†·å¯åŠ¨çŠ¶æ€")
                
                # è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½å·¥å…·é€‰æ‹©ï¼ˆä½¿ç”¨ä¸“ç”¨çš„å†·å¯åŠ¨æ–¹æ³•ï¼‰
                llm_choice = self._llm_guided_tool_selection_for_cold_start(
                    current_state, real_tools, mab_recommendation, cold_analysis
                )
                
                # è®°å½•æ¢ç´¢æ¨¡å¼å†³ç­–
                self._record_decision_mode('exploration', {
                    'mab_suggestion': mab_recommendation,
                    'llm_choice': llm_choice,
                    'cold_analysis': cold_analysis,
                    'reason': f"MABæ¨èå·¥å…·å†·å¯åŠ¨ï¼Œç½®ä¿¡åº¦ä»…{cold_analysis['confidence']:.1%}"
                })
                
                logger.info(f"ğŸ§  LLMæ¢ç´¢é€‰æ‹©: {llm_choice}")
                return llm_choice
                
            else:
                # ğŸ“Š å·¥å…·ç»éªŒä¸°å¯Œ â†’ ä¿¡ä»»MABå»ºè®®ï¼Œé‡‡ç”¨ç»éªŒæ¨¡å¼
                logger.info("ğŸ“Š ä½¿ç”¨ç»éªŒæ¨¡å¼ (MABä¸»å¯¼å†³ç­–)")
                logger.info(f"   åŸå› : å·¥å…· '{mab_recommendation}' å…·æœ‰ä¸°å¯Œç»éªŒ (ç½®ä¿¡åº¦: {cold_analysis['confidence']:.1%})")
                
                # è®°å½•ç»éªŒæ¨¡å¼å†³ç­–
                self._record_decision_mode('experience', {
                    'mab_choice': mab_recommendation,
                    'cold_analysis': cold_analysis,
                    'reason': f"ä¿¡ä»»MABç»éªŒï¼Œå·¥å…·ä½¿ç”¨{cold_analysis['analysis']['usage_count']}æ¬¡"
                })
                
                logger.info(f"ğŸ“Š MABç»éªŒé€‰æ‹©: {mab_recommendation}")
                return mab_recommendation
            
        except Exception as e:
            logger.error(f"âŒ æ··åˆå†³ç­–æµç¨‹å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºäºè§„åˆ™çš„é€‰æ‹©
            logger.info("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿè§„åˆ™é€‰æ‹©")
            return self._fallback_tool_selection(current_state, available_tools)
    
    def _llm_explore_tool_selection(self, current_state: Dict[str, Any], 
                                   available_tools: List[str], 
                                   mab_suggestion: str = None,
                                   cold_analysis: Dict[str, any] = None) -> Optional[str]:
        """
        ğŸ§  LLMä¸»å¯¼çš„æ¢ç´¢æ€§å·¥å…·é€‰æ‹© - å…¼å®¹æ€§åŒ…è£…æ–¹æ³•
        
        è¿™æ˜¯ä¸€ä¸ªå…¼å®¹æ€§åŒ…è£…æ–¹æ³•ï¼Œç°åœ¨ç›´æ¥è°ƒç”¨æ›´å¼ºå¤§çš„
        _llm_guided_tool_selection_for_cold_start æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¤ç”¨äº†æ¨¡å¼ä¸€çš„æ ¸å¿ƒé€»è¾‘ã€‚
        
        Args:
            current_state: å½“å‰çŠ¶æ€ä¿¡æ¯
            available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            mab_suggestion: MABçš„å»ºè®®ï¼ˆå¯é€‰ï¼Œç”¨äºå‚è€ƒï¼‰
            cold_analysis: å†·å¯åŠ¨åˆ†æç»“æœï¼ˆå¯é€‰ï¼‰
            
        Returns:
            LLMé€‰æ‹©çš„å·¥å…·åç§°
        """
        logger.debug("ğŸ”„ è°ƒç”¨å…¼å®¹æ€§åŒ…è£…æ–¹æ³•ï¼Œè½¬å‘åˆ°ä¸“ç”¨å†·å¯åŠ¨å·¥å…·é€‰æ‹©")
        
        # ç›´æ¥è°ƒç”¨ä¸“ç”¨çš„å†·å¯åŠ¨å·¥å…·é€‰æ‹©æ–¹æ³•
        return self._llm_guided_tool_selection_for_cold_start(
            current_state, available_tools, mab_suggestion, cold_analysis
        )
    
    # ==================== ğŸ—‘ï¸ å·²å¼ƒç”¨çš„è¾…åŠ©æ–¹æ³•ï¼ˆä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰ ====================
    # ä»¥ä¸‹æ–¹æ³•å·²è¢« _llm_guided_tool_selection_for_cold_start ä¸­çš„æ›´å¥½å®ç°æ›¿ä»£
    
    def _build_exploration_prompt(self, current_state: Dict[str, Any], 
                                 available_tools: List[str],
                                 mab_suggestion: str = None,
                                 cold_analysis: Dict[str, any] = None) -> str:
        """âš ï¸ å·²å¼ƒç”¨ï¼šè¯·ä½¿ç”¨ _llm_guided_tool_selection_for_cold_start ä¸­çš„å®ç°"""
        logger.warning("âš ï¸ _build_exploration_prompt å·²å¼ƒç”¨ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ _llm_guided_tool_selection_for_cold_start")
        
        # ç®€åŒ–å®ç°ï¼Œå›é€€åˆ°åŸºæœ¬æç¤ºè¯
        user_query = current_state.get('user_query', 'ç”¨æˆ·æŸ¥è¯¢ä¿¡æ¯')
        tool_list = "\n".join([f"- {tool}" for tool in available_tools])
        
        return f"""è¯·ä»ä»¥ä¸‹å·¥å…·ä¸­é€‰æ‹©æœ€é€‚åˆå¤„ç†æŸ¥è¯¢çš„å·¥å…·ï¼š

æŸ¥è¯¢: {user_query}

å¯ç”¨å·¥å…·:
{tool_list}

é€‰æ‹©å·¥å…·: [å·¥å…·åç§°]
"""
    
    def _format_tools_for_llm(self, available_tools: List[str]) -> str:
        """âš ï¸ å·²å¼ƒç”¨ï¼šè¯·ä½¿ç”¨ _llm_guided_tool_selection_for_cold_start ä¸­çš„å®ç°"""
        logger.warning("âš ï¸ _format_tools_for_llm å·²å¼ƒç”¨ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ _llm_guided_tool_selection_for_cold_start")
        
        # ç®€åŒ–å®ç°
        if not self.tool_registry:
            return "\n".join([f"- {tool}" for tool in available_tools])
        
        formatted_descriptions = []
        for tool_name in available_tools:
            try:
                tool = self.tool_registry.get_tool(tool_name)
                if tool:
                    formatted_descriptions.append(f"- {tool_name}: {tool.description}")
                else:
                    formatted_descriptions.append(f"- {tool_name}: å·¥å…·æè¿°æœªæ‰¾åˆ°")
            except Exception:
                formatted_descriptions.append(f"- {tool_name}: å·¥å…·ä¿¡æ¯è·å–å¤±è´¥")
        
        return "\n".join(formatted_descriptions)
    
    def _parse_llm_tool_choice(self, llm_response: str, available_tools: List[str]) -> str:
        """âš ï¸ å·²å¼ƒç”¨ï¼šè¯·ä½¿ç”¨ _llm_guided_tool_selection_for_cold_start ä¸­çš„å®ç°"""
        logger.warning("âš ï¸ _parse_llm_tool_choice å·²å¼ƒç”¨ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ _llm_guided_tool_selection_for_cold_start")
        
        # ç®€åŒ–å®ç°ï¼ŒåªåšåŸºæœ¬è§£æ
        for line in llm_response.split('\n'):
            if 'é€‰æ‹©å·¥å…·:' in line or 'é€‰æ‹©å·¥å…·ï¼š' in line:
                tool_name = line.split(':', 1)[-1].split('ï¼š', 1)[-1].strip().strip('[]')
                if tool_name in available_tools:
                    return tool_name
        
        # å›é€€åˆ°ç¬¬ä¸€ä¸ªå·¥å…·
        return available_tools[0] if available_tools else None
    
    def _record_decision_mode(self, mode: str, decision_data: Dict[str, any]):
        """
        è®°å½•å†³ç­–æ¨¡å¼å’Œç›¸å…³æ•°æ®
        
        Args:
            mode: å†³ç­–æ¨¡å¼ ('experience' æˆ– 'exploration')
            decision_data: å†³ç­–ç›¸å…³æ•°æ®
        """
        record = {
            'timestamp': time.time(),
            'mode': mode,
            'data': decision_data
        }
        
        # å¦‚æœæœ‰çŠ¶æ€ç®¡ç†å™¨ï¼Œè®°å½•åˆ°å¯¹è¯å†å²ä¸­
        if hasattr(self, 'state_manager') and self.state_manager:
            # å¯ä»¥æ‰©å±•StateManageræ¥æ”¯æŒå†³ç­–æ¨¡å¼è®°å½•
            logger.debug(f"ğŸ“Š è®°å½•å†³ç­–æ¨¡å¼: {mode}")
        
        # ç®€å•çš„å†…å­˜è®°å½•ï¼ˆå¯ä»¥åç»­æ‰©å±•åˆ°æŒä¹…åŒ–ï¼‰
        if not hasattr(self, 'decision_history'):
            self.decision_history = []
        
        self.decision_history.append(record)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.decision_history) > 100:
            self.decision_history = self.decision_history[-50:]
        
        logger.info(f"ğŸ“Š å†³ç­–æ¨¡å¼è®°å½•: {mode} - {decision_data.get('reason', 'æ— ç†ç”±')}")
    
    def _llm_guided_tool_selection_for_cold_start(self, current_state: Dict[str, Any], 
                                                 available_tools: List[str],
                                                 mab_suggestion: str = None,
                                                 cold_analysis: Dict[str, any] = None) -> Optional[str]:
        """
        ğŸ§  LLMä¸»å¯¼çš„å†·å¯åŠ¨å·¥å…·é€‰æ‹© - å¤ç”¨æ¨¡å¼ä¸€çš„æ ¸å¿ƒé€»è¾‘
        
        å½“MABæ¨èçš„å·¥å…·å¤„äºå†·å¯åŠ¨çŠ¶æ€æ—¶ï¼Œè¿™ä¸ªæ–¹æ³•æä¾›äº†ä¸€ä¸ªç‹¬ç«‹çš„ã€å¯å¤ç”¨çš„
        åŠŸèƒ½å—æ¥è®©LLMæ ¹æ®è¯­ä¹‰é€‰æ‹©å·¥å…·ã€‚å®ƒå¤ç”¨äº†æ¨¡å¼ä¸€çš„æ ¸å¿ƒé€»è¾‘ï¼Œç¡®ä¿è¡Œä¸ºä¸€è‡´æ€§ã€‚
        
        å¤ç”¨çš„ç°æœ‰å‡½æ•°ï¼š
        - _prepare_tool_descriptions: å‡†å¤‡å·¥å…·æè¿°
        - _build_tool_enhanced_prompt: æ„å»ºå·¥å…·å¢å¼ºæç¤ºè¯
        - _detect_tool_call_intent: æ£€æµ‹å·¥å…·è°ƒç”¨æ„å›¾
        
        Args:
            current_state: å½“å‰çŠ¶æ€ä¿¡æ¯
            available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            mab_suggestion: MABçš„å»ºè®®ï¼ˆç”¨äºå‚è€ƒå¯¹æ¯”ï¼‰
            cold_analysis: å†·å¯åŠ¨åˆ†æç»“æœï¼ˆç”¨äºæç¤ºè¯ä¼˜åŒ–ï¼‰
            
        Returns:
            LLMé€‰æ‹©çš„å·¥å…·åç§°ï¼Œæˆ–Noneè¡¨ç¤ºä¸ä½¿ç”¨å·¥å…·
        """
        logger.info("ğŸ§  å¼€å§‹LLMä¸»å¯¼çš„å†·å¯åŠ¨å·¥å…·é€‰æ‹©")
        
        try:
            # ========== æ­¥éª¤1: å¤ç”¨æ¨¡å¼ä¸€ - å‡†å¤‡å·¥å…·æè¿° ==========
            tool_descriptions = self._prepare_tool_descriptions(available_tools)
            logger.debug(f"âœ… å·¥å…·æè¿°å‡†å¤‡å®Œæˆ: {len(available_tools)}ä¸ªå·¥å…·")
            
            # ========== æ­¥éª¤2: æ„å»ºå†·å¯åŠ¨ä¸“ç”¨çš„å¢å¼ºæç¤ºè¯ ==========
            # è·å–ç”¨æˆ·æŸ¥è¯¢
            user_query = current_state.get('user_query', 'å¤„ç†ç”¨æˆ·æŸ¥è¯¢')
            
            # æ„å»ºå†·å¯åŠ¨ä¸Šä¸‹æ–‡ä¿¡æ¯
            cold_start_context = self._build_cold_start_context(mab_suggestion, cold_analysis)
            
            # å¤ç”¨æ¨¡å¼ä¸€ - æ„å»ºå·¥å…·å¢å¼ºæç¤ºè¯ï¼Œå¹¶åŠ å…¥å†·å¯åŠ¨ä¿¡æ¯
            base_enhanced_prompt = self._build_tool_enhanced_prompt(
                user_query, tool_descriptions, cold_start_context
            )
            
            # ä¸ºå†·å¯åŠ¨åœºæ™¯æ·»åŠ ç‰¹æ®Šè¯´æ˜
            cold_start_enhanced_prompt = self._add_cold_start_guidance(
                base_enhanced_prompt, mab_suggestion, cold_analysis
            )
            
            logger.debug("âœ… å†·å¯åŠ¨å¢å¼ºæç¤ºè¯æ„å»ºå®Œæˆ")
            
            # ========== æ­¥éª¤3: è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†æ ==========
            llm_response = ""
            
            if self.llm_manager:
                llm_result = self.llm_manager.chat_completion(cold_start_enhanced_prompt)
                if llm_result.success:
                    llm_response = llm_result.content
                    logger.info("âœ… LLMç®¡ç†å™¨è°ƒç”¨æˆåŠŸ")
                else:
                    logger.error(f"âŒ LLMç®¡ç†å™¨è°ƒç”¨å¤±è´¥: {llm_result.error_message}")
                    
            elif self.llm_client:
                # å›é€€åˆ°ç›´æ¥å®¢æˆ·ç«¯
                if hasattr(self.llm_client, 'chat_completion'):
                    llm_result = self.llm_client.chat_completion(cold_start_enhanced_prompt)
                    if llm_result.success:
                        llm_response = llm_result.content
                        logger.info("âœ… LLMå®¢æˆ·ç«¯è°ƒç”¨æˆåŠŸ")
                    else:
                        logger.error(f"âŒ LLMå®¢æˆ·ç«¯è°ƒç”¨å¤±è´¥: {llm_result.error_message}")
                else:
                    logger.error("âŒ LLMå®¢æˆ·ç«¯ä¸æ”¯æŒchat_completionæ–¹æ³•")
            else:
                logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯")
                raise Exception("LLMå®¢æˆ·ç«¯ä¸å¯ç”¨")
            
            if not llm_response:
                raise Exception("LLMè°ƒç”¨å¤±è´¥ï¼Œæ²¡æœ‰è·å¾—å“åº”")
            
            # ========== æ­¥éª¤4: å¤ç”¨æ¨¡å¼ä¸€ - æ£€æµ‹å·¥å…·è°ƒç”¨æ„å›¾ ==========
            tool_call_intent = self._detect_tool_call_intent(llm_response)
            
            if tool_call_intent:
                # LLMæ˜ç¡®æŒ‡å‡ºè¦ä½¿ç”¨æŸä¸ªå·¥å…·
                chosen_tool = tool_call_intent['tool_name']
                
                # éªŒè¯å·¥å…·æ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­
                if chosen_tool in available_tools:
                    logger.info(f"ğŸ§  LLMå†·å¯åŠ¨é€‰æ‹©: {chosen_tool}")
                    logger.info(f"   å·¥å…·å‚æ•°: {tool_call_intent['tool_params']}")
                    
                    # è®°å½•ä¸MABå»ºè®®çš„å¯¹æ¯”
                    if mab_suggestion and chosen_tool != mab_suggestion:
                        logger.info(f"ğŸ“Š LLMé€‰æ‹©ä¸MABå»ºè®®ä¸åŒ: LLM={chosen_tool} vs MAB={mab_suggestion}")
                    elif mab_suggestion and chosen_tool == mab_suggestion:
                        logger.info(f"ğŸ¯ LLMé€‰æ‹©ä¸MABå»ºè®®ä¸€è‡´: {chosen_tool}")
                    
                    return chosen_tool
                else:
                    logger.warning(f"âš ï¸ LLMé€‰æ‹©çš„å·¥å…· '{chosen_tool}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­")
                    # å›é€€åˆ°å¯ç”¨å·¥å…·ä¸­çš„ç›¸ä¼¼å·¥å…·
                    fallback_tool = self._find_similar_tool(chosen_tool, available_tools)
                    if fallback_tool:
                        logger.info(f"ğŸ”„ å›é€€åˆ°ç›¸ä¼¼å·¥å…·: {fallback_tool}")
                        return fallback_tool
            else:
                # LLMè®¤ä¸ºä¸éœ€è¦ä½¿ç”¨å·¥å…·
                logger.info("ğŸ§  LLMåˆ¤æ–­ä¸éœ€è¦ä½¿ç”¨å·¥å…·")
                return None
            
        except Exception as e:
            logger.error(f"âŒ LLMä¸»å¯¼çš„å†·å¯åŠ¨å·¥å…·é€‰æ‹©å¤±è´¥: {e}")
        
        # ========== æœ€ç»ˆå›é€€ç­–ç•¥ ==========
        # å¦‚æœLLMé€‰æ‹©å¤±è´¥ï¼Œæ ¹æ®æƒ…å†µé‡‡ç”¨ä¸åŒçš„å›é€€ç­–ç•¥
        if mab_suggestion and mab_suggestion in available_tools:
            logger.info(f"ğŸ”„ LLMå†·å¯åŠ¨å¤±è´¥ï¼Œå›é€€åˆ°MABå»ºè®®: {mab_suggestion}")
            return mab_suggestion
        elif available_tools:
            # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨å·¥å…·ä½œä¸ºä¿åº•
            fallback_tool = available_tools[0]
            logger.info(f"ğŸ”„ LLMå†·å¯åŠ¨å¤±è´¥ï¼Œä¿åº•é€‰æ‹©: {fallback_tool}")
            return fallback_tool
        else:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨å·¥å…·ï¼Œè¿”å›None")
            return None
    
    def _build_cold_start_context(self, mab_suggestion: str = None, 
                                 cold_analysis: Dict[str, any] = None) -> Dict[str, Any]:
        """
        æ„å»ºå†·å¯åŠ¨ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Args:
            mab_suggestion: MABå»ºè®®
            cold_analysis: å†·å¯åŠ¨åˆ†æç»“æœ
            
        Returns:
            å†·å¯åŠ¨ä¸Šä¸‹æ–‡å­—å…¸
        """
        context = {}
        
        if mab_suggestion and cold_analysis:
            context['mab_analysis'] = {
                'suggested_tool': mab_suggestion,
                'cold_start_status': cold_analysis['is_cold_start'],
                'confidence': cold_analysis['confidence'],
                'usage_count': cold_analysis['analysis']['usage_count'],
                'reason': cold_analysis['reason']
            }
        
        return context
    
    def _add_cold_start_guidance(self, base_prompt: str, 
                                mab_suggestion: str = None,
                                cold_analysis: Dict[str, any] = None) -> str:
        """
        ä¸ºåŸºç¡€æç¤ºè¯æ·»åŠ å†·å¯åŠ¨ä¸“ç”¨æŒ‡å¯¼
        
        Args:
            base_prompt: åŸºç¡€å¢å¼ºæç¤ºè¯
            mab_suggestion: MABå»ºè®®
            cold_analysis: å†·å¯åŠ¨åˆ†æ
            
        Returns:
            å¢å¼ºçš„å†·å¯åŠ¨æç¤ºè¯
        """
        if not mab_suggestion or not cold_analysis:
            return base_prompt
        
        cold_start_guidance = f"""
        
ğŸ§Š å†·å¯åŠ¨å†³ç­–è¾…åŠ©ä¿¡æ¯ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š MABç³»ç»Ÿåˆ†æï¼š
   â€¢ æ¨èå·¥å…·: {mab_suggestion}
   â€¢ ç»éªŒçŠ¶æ€: {'ğŸ§Š å†·å¯åŠ¨çŠ¶æ€' if cold_analysis['is_cold_start'] else 'ğŸ”¥ ç»éªŒä¸°å¯Œ'}
   â€¢ æ•°æ®ç½®ä¿¡åº¦: {cold_analysis['confidence']:.1%}
   â€¢ å†å²ä½¿ç”¨: {cold_analysis['analysis']['usage_count']}æ¬¡
   â€¢ åˆ†æåŸå› : {cold_analysis['reason']}

ğŸ§  LLMå†³ç­–æŒ‡å¯¼ï¼š
   â€¢ å½“å‰MABæ¨èçš„å·¥å…·ç»éªŒä¸è¶³ï¼Œéœ€è¦ä½ åŸºäºè¯­ä¹‰ç†è§£è¿›è¡Œç‹¬ç«‹åˆ¤æ–­
   â€¢ è¯·ä»”ç»†åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ ¸å¿ƒéœ€æ±‚ï¼Œé€‰æ‹©æœ€åˆé€‚çš„å·¥å…·
   â€¢ ä½ å¯ä»¥å‚è€ƒMABçš„å»ºè®®ï¼Œä½†è¯·ä¼˜å…ˆåŸºäºè¯­ä¹‰åŒ¹é…åšå‡ºé€‰æ‹©
   â€¢ å¦‚æœMABå»ºè®®çš„å·¥å…·ç¡®å®æœ€åˆé€‚ï¼Œé€‰æ‹©å®ƒä¹Ÿæ˜¯å®Œå…¨å¯ä»¥çš„
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
        
        return base_prompt + cold_start_guidance
    
    def _find_similar_tool(self, target_tool: str, available_tools: List[str]) -> Optional[str]:
        """
        åœ¨å¯ç”¨å·¥å…·ä¸­æŸ¥æ‰¾ä¸ç›®æ ‡å·¥å…·ç›¸ä¼¼çš„å·¥å…·
        
        Args:
            target_tool: ç›®æ ‡å·¥å…·åç§°
            available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            
        Returns:
            æœ€ç›¸ä¼¼çš„å·¥å…·åç§°ï¼Œæˆ–None
        """
        if not target_tool or not available_tools:
            return None
        
        target_lower = target_tool.lower()
        
        # ç²¾ç¡®åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
        for tool in available_tools:
            if tool.lower() == target_lower:
                return tool
        
        # åŒ…å«åŒ¹é…
        for tool in available_tools:
            if target_lower in tool.lower() or tool.lower() in target_lower:
                logger.debug(f"ğŸ” æ‰¾åˆ°ç›¸ä¼¼å·¥å…·: {target_tool} -> {tool}")
                return tool
        
        # å…³é”®è¯åŒ¹é…
        target_keywords = set(target_lower.split('_'))
        best_match = None
        best_score = 0
        
        for tool in available_tools:
            tool_keywords = set(tool.lower().split('_'))
            common_keywords = target_keywords.intersection(tool_keywords)
            score = len(common_keywords)
            
            if score > best_score:
                best_score = score
                best_match = tool
        
        if best_match and best_score > 0:
            logger.debug(f"ğŸ” å…³é”®è¯åŒ¹é…å·¥å…·: {target_tool} -> {best_match} (åˆ†æ•°: {best_score})")
            return best_match
        
        return None
    
    def _provide_mab_feedback(self, tool_name: str, success: bool, reward: float):
        """
        ğŸ† å‘MABæä¾›å·¥å…·æ‰§è¡Œåé¦ˆ - æ”¯æŒLLMè£åˆ¤å¥–åŠ±ä¿¡å·
        
        Args:
            tool_name: å·¥å…·åç§°
            success: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
            reward: å¥–åŠ±å€¼ (-1.0 åˆ° 1.0ï¼ŒLLMè£åˆ¤è¯„åˆ†)
        """
        if self.mab_converger:
            try:
                # éªŒè¯å¥–åŠ±å€¼èŒƒå›´
                validated_reward = max(-1.0, min(1.0, reward))
                if abs(validated_reward - reward) > 0.001:
                    logger.warning(f"âš ï¸ å¥–åŠ±å€¼è¶…å‡ºèŒƒå›´ï¼Œå·²è°ƒæ•´: {reward:.3f} -> {validated_reward:.3f}")
                
                # æ›´æ–°MABæ€§èƒ½
                self.mab_converger.update_path_performance(tool_name, success, validated_reward)
                
                # è®°å½•è¯¦ç»†çš„åé¦ˆä¿¡æ¯
                feedback_info = {
                    'tool_name': tool_name,
                    'success': success,
                    'original_reward': reward,
                    'validated_reward': validated_reward,
                    'timestamp': time.time(),
                    'feedback_source': 'llm_judge' if abs(reward) > 0.001 else 'default'
                }
                
                # å­˜å‚¨åé¦ˆå†å²ï¼ˆç”¨äºåˆ†æï¼‰
                if not hasattr(self, '_mab_feedback_history'):
                    self._mab_feedback_history = []
                self._mab_feedback_history.append(feedback_info)
                
                # é™åˆ¶å†å²è®°å½•é•¿åº¦
                if len(self._mab_feedback_history) > 1000:
                    self._mab_feedback_history = self._mab_feedback_history[-500:]
                
                logger.info(f"ğŸ† MABåé¦ˆå·²æä¾›: {tool_name}, æˆåŠŸ={success}, å¥–åŠ±={validated_reward:.3f}")
                
                # å¦‚æœæ˜¯LLMè£åˆ¤è¯„åˆ†ï¼Œé¢å¤–è®°å½•
                if abs(reward) > 0.001:  # éé›¶å¥–åŠ±ï¼Œè¯´æ˜æ˜¯LLMè£åˆ¤è¯„åˆ†
                    logger.info(f"ğŸ§  LLMè£åˆ¤è¯„åˆ†åé¦ˆ: {tool_name} -> {validated_reward:.3f}")
                
            except Exception as e:
                logger.error(f"âŒ MABåé¦ˆå¤±è´¥: {e}")
                import traceback
                logger.error(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        else:
            logger.warning("âš ï¸ MABConvergeræœªåˆå§‹åŒ–ï¼Œæ— æ³•æä¾›åé¦ˆ")
    
    def get_mab_feedback_statistics(self) -> Dict[str, Any]:
        """
        ğŸ“Š è·å–MABåé¦ˆç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«åé¦ˆç»Ÿè®¡çš„å­—å…¸
        """
        if not hasattr(self, '_mab_feedback_history') or not self._mab_feedback_history:
            return {
                'total_feedbacks': 0,
                'llm_judge_feedbacks': 0,
                'average_reward': 0.0,
                'success_rate': 0.0,
                'tool_statistics': {}
            }
        
        history = self._mab_feedback_history
        
        # åŸºæœ¬ç»Ÿè®¡
        total_feedbacks = len(history)
        llm_judge_feedbacks = sum(1 for f in history if f['feedback_source'] == 'llm_judge')
        successful_feedbacks = sum(1 for f in history if f['success'])
        
        # å¥–åŠ±ç»Ÿè®¡
        rewards = [f['validated_reward'] for f in history]
        average_reward = sum(rewards) / len(rewards) if rewards else 0.0
        
        # å·¥å…·ç»Ÿè®¡
        tool_stats = {}
        for feedback in history:
            tool_name = feedback['tool_name']
            if tool_name not in tool_stats:
                tool_stats[tool_name] = {
                    'total_uses': 0,
                    'successes': 0,
                    'total_reward': 0.0,
                    'llm_judge_uses': 0
                }
            
            tool_stats[tool_name]['total_uses'] += 1
            if feedback['success']:
                tool_stats[tool_name]['successes'] += 1
            tool_stats[tool_name]['total_reward'] += feedback['validated_reward']
            if feedback['feedback_source'] == 'llm_judge':
                tool_stats[tool_name]['llm_judge_uses'] += 1
        
        # è®¡ç®—æ¯ä¸ªå·¥å…·çš„å¹³å‡å¥–åŠ±å’ŒæˆåŠŸç‡
        for tool_name, stats in tool_stats.items():
            stats['average_reward'] = stats['total_reward'] / stats['total_uses']
            stats['success_rate'] = stats['successes'] / stats['total_uses']
            stats['llm_judge_ratio'] = stats['llm_judge_uses'] / stats['total_uses']
        
        return {
            'total_feedbacks': total_feedbacks,
            'llm_judge_feedbacks': llm_judge_feedbacks,
            'llm_judge_ratio': llm_judge_feedbacks / total_feedbacks,
            'average_reward': average_reward,
            'success_rate': successful_feedbacks / total_feedbacks,
            'tool_statistics': tool_stats,
            'recent_feedbacks': history[-10:] if len(history) >= 10 else history
        }
    
    def demonstrate_llm_judge_reward_system(self, user_query: str, 
                                          mock_tool_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ğŸ­ æ¼”ç¤ºLLMè£åˆ¤å¥–åŠ±ç³»ç»Ÿ - ç”¨äºæµ‹è¯•å’ŒéªŒè¯
        
        Args:
            user_query: ç”¨æˆ·é—®é¢˜
            mock_tool_results: æ¨¡æ‹Ÿå·¥å…·ç»“æœåˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{'tool_name': str, 'result': Any}, ...]
            
        Returns:
            åŒ…å«æ‰€æœ‰è¯„åˆ†ç»“æœçš„å­—å…¸
        """
        logger.info(f"ğŸ­ å¼€å§‹æ¼”ç¤ºLLMè£åˆ¤å¥–åŠ±ç³»ç»Ÿï¼Œç”¨æˆ·é—®é¢˜: {user_query}")
        
        demonstration_results = {
            'user_query': user_query,
            'tool_evaluations': [],
            'summary': {
                'total_tools': len(mock_tool_results),
                'successful_evaluations': 0,
                'failed_evaluations': 0,
                'average_score': 0.0,
                'score_distribution': {'positive': 0, 'neutral': 0, 'negative': 0}
            }
        }
        
        total_score = 0.0
        successful_evals = 0
        
        for tool_data in mock_tool_results:
            tool_name = tool_data.get('tool_name', 'unknown_tool')
            tool_result = tool_data.get('result', '')
            
            logger.info(f"ğŸ” è¯„ä¼°å·¥å…·: {tool_name}")
            
            # ä½¿ç”¨LLMè£åˆ¤ç³»ç»Ÿè¯„åˆ†
            score = self._llm_judge_tool_effectiveness(tool_result, user_query)
            
            evaluation_result = {
                'tool_name': tool_name,
                'tool_result': str(tool_result)[:200] + "..." if len(str(tool_result)) > 200 else str(tool_result),
                'llm_score': score,
                'evaluation_status': 'success' if score is not None else 'failed',
                'score_category': self._categorize_score(score) if score is not None else 'unknown'
            }
            
            demonstration_results['tool_evaluations'].append(evaluation_result)
            
            if score is not None:
                successful_evals += 1
                total_score += score
                
                # æ›´æ–°åˆ†å¸ƒç»Ÿè®¡
                category = self._categorize_score(score)
                demonstration_results['summary']['score_distribution'][category] += 1
            else:
                demonstration_results['summary']['failed_evaluations'] += 1
        
        # æ›´æ–°æ€»ç»“
        demonstration_results['summary']['successful_evaluations'] = successful_evals
        if successful_evals > 0:
            demonstration_results['summary']['average_score'] = total_score / successful_evals
        
        logger.info(f"ğŸ­ LLMè£åˆ¤æ¼”ç¤ºå®Œæˆ: {successful_evals}/{len(mock_tool_results)} æˆåŠŸè¯„ä¼°")
        return demonstration_results
    
    def _categorize_score(self, score: float) -> str:
        """å°†è¯„åˆ†åˆ†ç±»"""
        if score >= 0.1:
            return 'positive'
        elif score <= -0.1:
            return 'negative'
        else:
            return 'neutral'
    
    # ==================== ğŸ§  LLMå‚æ•°ç”Ÿæˆæ–¹æ³• ====================
    
    def _llm_generate_direct_answer(self, prompt: str, context: Optional[Dict], 
                                  current_response: str) -> str:
        """
        LLMç›´æ¥ç”Ÿæˆå›ç­”ï¼ˆä¸ä½¿ç”¨å·¥å…·ï¼‰
        
        Args:
            prompt: åŸå§‹æç¤ºè¯
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            current_response: å½“å‰å“åº”ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            
        Returns:
            LLMç”Ÿæˆçš„æœ€ç»ˆå›ç­”
        """
        try:
            # æ„å»ºç›´æ¥å›ç­”çš„æç¤ºè¯
            if current_response:
                # å¦‚æœå·²æœ‰éƒ¨åˆ†å“åº”ï¼Œè®©LLMåŸºäºæ­¤ç”Ÿæˆæœ€ç»ˆå›ç­”
                final_prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆå›ç­”ï¼š

åŸå§‹é—®é¢˜ï¼š{prompt}

å½“å‰å·²æœ‰çš„åˆ†æå’Œä¿¡æ¯ï¼š
{current_response}

è¯·ç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€å‡†ç¡®çš„æœ€ç»ˆå›ç­”ï¼Œä¸éœ€è¦ä½¿ç”¨ä»»ä½•å·¥å…·ã€‚
"""
            else:
                # ç›´æ¥å›ç­”åŸå§‹é—®é¢˜
                final_prompt = f"""
è¯·ç›´æ¥å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œä¸éœ€è¦ä½¿ç”¨ä»»ä½•å·¥å…·ï¼š

é—®é¢˜ï¼š{prompt}

å¦‚æœæœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context if context else 'æ— '}

è¯·ç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€å‡†ç¡®çš„å›ç­”ã€‚
"""
            
            # è°ƒç”¨LLMç”Ÿæˆå›ç­”
            if self.llm_manager:
                llm_result = self.llm_manager.chat_completion(final_prompt)
                if llm_result.success:
                    return llm_result.content
                else:
                    logger.error(f"LLMç”Ÿæˆæœ€ç»ˆå›ç­”å¤±è´¥: {llm_result.error_message}")
                    return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{llm_result.error_message}"
            elif self.llm_client:
                if hasattr(self.llm_client, 'call_api'):
                    return self.llm_client.call_api(final_prompt)
                elif hasattr(self.llm_client, 'chat_completion'):
                    llm_result = self.llm_client.chat_completion(final_prompt)
                    if llm_result.success:
                        return llm_result.content
                    else:
                        return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{llm_result.error_message}"
            
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯æ¥ç”Ÿæˆå›ç­”ã€‚"
            
        except Exception as e:
            logger.error(f"âŒ LLMç›´æ¥å›ç­”ç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°å¼‚å¸¸ï¼š{str(e)}"
    
    def _llm_generate_tool_parameters_and_execute(self, tool_name: str, prompt: str, 
                                                context: Optional[Dict], 
                                                current_response: str) -> Dict[str, Any]:
        """
        LLMä¸ºæŒ‡å®šå·¥å…·ç”Ÿæˆå‚æ•°å¹¶æ‰§è¡Œå·¥å…·
        
        Args:
            tool_name: é€‰æ‹©çš„å·¥å…·åç§°
            prompt: åŸå§‹æç¤ºè¯
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            current_response: å½“å‰å“åº”
            
        Returns:
            åŒ…å«å·¥å…·æ‰§è¡Œç»“æœçš„å­—å…¸
        """
        result = {
            'success': False,
            'tool_call': {},
            'tool_result': None,
            'llm_response': '',
            'reward': 0.0,
            'error': ''
        }
        
        try:
            # è·å–å·¥å…·ä¿¡æ¯
            tool = get_tool(tool_name)
            if not tool:
                result['error'] = f"å·¥å…· {tool_name} ä¸å­˜åœ¨"
                return result
            
            # æ„å»ºå·¥å…·å‚æ•°ç”Ÿæˆæç¤ºè¯
            tool_prompt = f"""
ä½ éœ€è¦ä¸ºå·¥å…· "{tool_name}" ç”Ÿæˆåˆé€‚çš„å‚æ•°ã€‚

å·¥å…·æè¿°ï¼š{tool.description if hasattr(tool, 'description') else 'æ— æè¿°'}

åŸå§‹é—®é¢˜ï¼š{prompt}

å½“å‰ä¸Šä¸‹æ–‡ï¼š{context if context else 'æ— '}

å½“å‰å·²æœ‰çš„åˆ†æï¼š{current_response if current_response else 'æ— '}

è¯·åˆ†æåŸå§‹é—®é¢˜ï¼Œç”Ÿæˆè°ƒç”¨æ­¤å·¥å…·æ‰€éœ€çš„åˆé€‚å‚æ•°ã€‚
å¦‚æœå·¥å…·éœ€è¦æŸ¥è¯¢å…³é”®è¯ï¼Œè¯·æå–ç›¸å…³çš„å…³é”®è¯ã€‚
å¦‚æœå·¥å…·éœ€è¦å…¶ä»–å‚æ•°ï¼Œè¯·æ ¹æ®é—®é¢˜å†…å®¹è¿›è¡Œæ¨ç†ã€‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
å‚æ•°ï¼š[å…·ä½“çš„å‚æ•°å€¼]
"""
            
            # è°ƒç”¨LLMç”Ÿæˆå·¥å…·å‚æ•°
            tool_parameters = self._call_llm_for_parameters(tool_prompt)
            if not tool_parameters:
                result['error'] = "LLMæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„å·¥å…·å‚æ•°"
                return result
            
            # æ‰§è¡Œå·¥å…·
            tool_result = self._execute_tool_with_parameters(tool_name, tool_parameters)
            if tool_result is None:
                result['error'] = f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥"
                return result
            
            # å°†å·¥å…·ç»“æœèå…¥LLMå“åº”
            integration_prompt = f"""
åŸå§‹é—®é¢˜ï¼š{prompt}

ä¹‹å‰çš„åˆ†æï¼š{current_response if current_response else 'æ— '}

å·¥å…· "{tool_name}" çš„æ‰§è¡Œç»“æœï¼š
{tool_result}

è¯·å°†å·¥å…·ç»“æœèå…¥åˆ°å¯¹åŸå§‹é—®é¢˜çš„å›ç­”ä¸­ï¼Œç”Ÿæˆæ›´å®Œæ•´çš„å›ç­”ã€‚
"""
            
            integrated_response = self._call_llm_for_parameters(integration_prompt)
            
            # è®¡ç®—å¥–åŠ±å€¼ï¼ˆåŸºäºå·¥å…·ç»“æœè´¨é‡ï¼‰
            reward = self._calculate_tool_reward(tool_result, prompt)
            
            result.update({
                'success': True,
                'tool_call': {
                    'tool_name': tool_name,
                    'parameters': tool_parameters,
                    'timestamp': time.time()
                },
                'tool_result': tool_result,
                'llm_response': integrated_response or str(tool_result),
                'reward': reward
            })
            
            logger.debug(f"âœ… å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸï¼Œå¥–åŠ±={reward:.3f}")
            
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"âŒ å·¥å…·å‚æ•°ç”Ÿæˆå’Œæ‰§è¡Œå¤±è´¥: {e}")
        
        return result
    
    # ==================== ğŸ› ï¸ è¾…åŠ©å·¥å…·æ–¹æ³• ====================
    
    def _call_llm_for_parameters(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨LLMç”Ÿæˆå‚æ•°æˆ–æ–‡æœ¬"""
        try:
            if self.llm_manager:
                llm_result = self.llm_manager.chat_completion(prompt)
                return llm_result.content if llm_result.success else None
            elif self.llm_client:
                if hasattr(self.llm_client, 'call_api'):
                    return self.llm_client.call_api(prompt)
                elif hasattr(self.llm_client, 'chat_completion'):
                    llm_result = self.llm_client.chat_completion(prompt)
                    return llm_result.content if llm_result.success else None
            return None
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _execute_tool_with_parameters(self, tool_name: str, parameters: str) -> Optional[Any]:
        """æ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœ"""
        try:
            # ç®€åŒ–çš„å‚æ•°è§£æï¼ˆä»"å‚æ•°ï¼šxxx"æ ¼å¼ä¸­æå–å‚æ•°ï¼‰
            if parameters.startswith("å‚æ•°ï¼š"):
                clean_params = parameters[3:].strip()
            else:
                clean_params = parameters.strip()
            
            # ä½¿ç”¨å·¥å…·æŠ½è±¡å±‚æ‰§è¡Œå·¥å…·
            tool_result = execute_tool(tool_name, query=clean_params)
            
            if isinstance(tool_result, ToolResult):
                return tool_result.content if tool_result.success else None
            else:
                return tool_result
                
        except Exception as e:
            logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
            return None
    
    def _calculate_tool_reward(self, tool_result: Any, original_prompt: str) -> float:
        """
        ğŸ† LLMè£åˆ¤å¥–åŠ±è®¡ç®—ç³»ç»Ÿ - ä½¿ç”¨LLMè¯„ä¼°å·¥å…·è°ƒç”¨æ•ˆæœ
        
        Args:
            tool_result: å·¥å…·æ‰§è¡Œç»“æœ
            original_prompt: åŸå§‹ç”¨æˆ·é—®é¢˜
            
        Returns:
            å¥–åŠ±å€¼ (-1.0 åˆ° 1.0)
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨LLMè£åˆ¤ç³»ç»Ÿ
            llm_reward = self._llm_judge_tool_effectiveness(tool_result, original_prompt)
            if llm_reward is not None:
                logger.info(f"ğŸ† LLMè£åˆ¤å¥–åŠ±: {llm_reward:.3f}")
                return llm_reward
            
            # å›é€€åˆ°ä¼ ç»Ÿè®¡ç®—æ–¹æ³•
            logger.warning("âš ï¸ LLMè£åˆ¤å¤±è´¥ï¼Œä½¿ç”¨å›é€€å¥–åŠ±è®¡ç®—")
            return self._fallback_reward_calculation(tool_result, original_prompt)
            
        except Exception as e:
            logger.error(f"âŒ å¥–åŠ±è®¡ç®—å¤±è´¥: {e}")
            return 0.0  # ä¸­æ€§å¥–åŠ±ï¼Œé¿å…è¯¯å¯¼MABå­¦ä¹ 
    
    def _llm_judge_tool_effectiveness(self, tool_result: Any, original_prompt: str) -> Optional[float]:
        """
        ğŸ§  LLMè£åˆ¤ï¼šè¯„ä¼°å·¥å…·ç»“æœå¯¹è§£å†³é—®é¢˜çš„å¸®åŠ©ç¨‹åº¦
        
        Args:
            tool_result: å·¥å…·æ‰§è¡Œç»“æœ
            original_prompt: åŸå§‹ç”¨æˆ·é—®é¢˜
            
        Returns:
            LLMè¯„åˆ† (-1.0 åˆ° 1.0)ï¼ŒNoneè¡¨ç¤ºè¯„åˆ†å¤±è´¥
        """
        try:
            # å‡†å¤‡å·¥å…·ç»“æœæ–‡æœ¬
            result_text = self._prepare_tool_result_for_judgment(tool_result)
            if not result_text or len(result_text.strip()) == 0:
                logger.warning("âš ï¸ å·¥å…·ç»“æœä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒLLMè¯„åˆ†")
                return -0.3  # ç©ºç»“æœç»™äºˆè½»å¾®è´Ÿåˆ†
            
            # æ„å»ºLLMè£åˆ¤æç¤ºè¯
            judge_prompt = self._build_llm_judge_prompt(result_text, original_prompt)
            
            # è°ƒç”¨LLMè¿›è¡Œè¯„åˆ†
            llm_response = self._call_llm_for_parameters(judge_prompt)
            if not llm_response:
                logger.warning("âš ï¸ LLMè£åˆ¤å“åº”ä¸ºç©º")
                return None
            
            # è§£æLLMè¯„åˆ†
            score = self._parse_llm_judgment_score(llm_response)
            if score is not None:
                # éªŒè¯è¯„åˆ†èŒƒå›´
                score = max(-1.0, min(1.0, score))
                logger.debug(f"ğŸ† LLMè£åˆ¤è¯„åˆ†: {score:.3f}, åŸå§‹å“åº”: {llm_response[:100]}...")
                return score
            else:
                logger.warning(f"âš ï¸ æ— æ³•è§£æLLMè¯„åˆ†: {llm_response[:100]}...")
                return None
                
        except Exception as e:
            logger.error(f"âŒ LLMè£åˆ¤è¯„åˆ†å¤±è´¥: {e}")
            return None
    
    def _prepare_tool_result_for_judgment(self, tool_result: Any) -> str:
        """å‡†å¤‡å·¥å…·ç»“æœç”¨äºLLMè¯„åˆ¤"""
        if tool_result is None:
            return ""
        
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„å·¥å…·ç»“æœ
            if isinstance(tool_result, ToolResult):
                if tool_result.success and tool_result.content:
                    result_text = str(tool_result.content)
                else:
                    result_text = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_result.error_message if hasattr(tool_result, 'error_message') else 'æœªçŸ¥é”™è¯¯'}"
            elif isinstance(tool_result, dict):
                if 'content' in tool_result:
                    result_text = str(tool_result['content'])
                elif 'data' in tool_result:
                    result_text = str(tool_result['data'])
                else:
                    result_text = str(tool_result)
            else:
                result_text = str(tool_result)
            
            # é™åˆ¶ç»“æœé•¿åº¦ï¼Œé¿å…è¿‡é•¿çš„å†…å®¹å½±å“LLMåˆ¤æ–­
            max_length = 2000
            if len(result_text) > max_length:
                result_text = result_text[:max_length] + "... [å†…å®¹å·²æˆªæ–­]"
            
            return result_text
            
        except Exception as e:
            logger.error(f"âŒ å·¥å…·ç»“æœå‡†å¤‡å¤±è´¥: {e}")
            return str(tool_result)[:500] if tool_result else ""
    
    def _build_llm_judge_prompt(self, tool_result_text: str, original_prompt: str) -> str:
        """æ„å»ºLLMè£åˆ¤æç¤ºè¯"""
        judge_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå·¥å…·æ•ˆæœè¯„ä¼°ä¸“å®¶ã€‚ä½ éœ€è¦è¯„ä¼°ä¸€ä¸ªå·¥å…·çš„æ‰§è¡Œç»“æœå¯¹äºè§£å†³ç”¨æˆ·é—®é¢˜çš„å¸®åŠ©ç¨‹åº¦ã€‚

**ç”¨æˆ·åŸå§‹é—®é¢˜ï¼š**
{original_prompt}

**å·¥å…·æ‰§è¡Œç»“æœï¼š**
{tool_result_text}

**è¯„ä¼°ä»»åŠ¡ï¼š**
è¯·è¯„ä¼°ä¸Šè¿°å·¥å…·ç»“æœå¯¹äºè§£å†³ç”¨æˆ·åŸå§‹é—®é¢˜çš„å¸®åŠ©ç¨‹åº¦ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªç²¾ç¡®çš„æ•°å€¼è¯„åˆ†ã€‚

**è¯„åˆ†æ ‡å‡†ï¼š**
â€¢ 1.0ï¼šå·¥å…·ç»“æœå®Œç¾è§£å†³äº†é—®é¢˜ï¼Œæä¾›äº†å‡†ç¡®ã€å®Œæ•´ã€ç›´æ¥ç›¸å…³çš„ç­”æ¡ˆ
â€¢ 0.8ï¼šå·¥å…·ç»“æœå¾ˆå¥½åœ°è§£å†³äº†é—®é¢˜ï¼Œæä¾›äº†å¤§éƒ¨åˆ†éœ€è¦çš„ä¿¡æ¯
â€¢ 0.6ï¼šå·¥å…·ç»“æœéƒ¨åˆ†è§£å†³äº†é—®é¢˜ï¼Œæä¾›äº†ä¸€äº›æœ‰ç”¨çš„ç›¸å…³ä¿¡æ¯
â€¢ 0.4ï¼šå·¥å…·ç»“æœæœ‰ä¸€å®šå¸®åŠ©ï¼Œä½†ä¿¡æ¯ä¸å¤Ÿå……åˆ†æˆ–ä¸å¤Ÿå‡†ç¡®
â€¢ 0.2ï¼šå·¥å…·ç»“æœå¸®åŠ©å¾ˆå°ï¼Œä¿¡æ¯è¿‡äºç²—ç•¥æˆ–ç›¸å…³æ€§è¾ƒä½
â€¢ 0.0ï¼šå·¥å…·ç»“æœæ²¡æœ‰å¸®åŠ©ï¼Œä¿¡æ¯ä¸ç›¸å…³æˆ–æ— æ³•ç†è§£
â€¢ -0.2ï¼šå·¥å…·ç»“æœæœ‰è½»å¾®è¯¯å¯¼æ€§ï¼ŒåŒ…å«é”™è¯¯ä½†ä¸ä¸¥é‡
â€¢ -0.5ï¼šå·¥å…·ç»“æœæœ‰æ˜æ˜¾è¯¯å¯¼æ€§ï¼ŒåŒ…å«é‡è¦é”™è¯¯ä¿¡æ¯
â€¢ -1.0ï¼šå·¥å…·ç»“æœå®Œå…¨æœ‰å®³ï¼Œä¸¥é‡è¯¯å¯¼æˆ–åŒ…å«å¤§é‡é”™è¯¯ä¿¡æ¯

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»…è¾“å‡ºä¸€ä¸ª-1.0åˆ°1.0ä¹‹é—´çš„æ•°å€¼ï¼Œä¿ç•™ä¸¤ä½å°æ•°ã€‚ä¸éœ€è¦ä»»ä½•è§£é‡Šæˆ–å…¶ä»–æ–‡å­—ã€‚

è¯„åˆ†ï¼š"""
        
        return judge_prompt
    
    def _parse_llm_judgment_score(self, llm_response: str) -> Optional[float]:
        """è§£æLLMè¯„åˆ†å“åº”"""
        try:
            if not llm_response:
                return None
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            cleaned_response = llm_response.strip()
            
            # å°è¯•å¤šç§è§£ææ–¹å¼
            import re
            
            # æ–¹å¼1: ç›´æ¥åŒ¹é…æ•°å­—
            number_pattern = r'-?\d+\.?\d*'
            matches = re.findall(number_pattern, cleaned_response)
            
            if matches:
                try:
                    score = float(matches[0])
                    if -1.0 <= score <= 1.0:
                        return score
                except ValueError:
                    pass
            
            # æ–¹å¼2: æŸ¥æ‰¾"è¯„åˆ†ï¼š"åçš„æ•°å­—
            score_pattern = r'è¯„åˆ†ï¼š?\s*(-?\d+\.?\d*)'
            match = re.search(score_pattern, cleaned_response)
            if match:
                try:
                    score = float(match.group(1))
                    if -1.0 <= score <= 1.0:
                        return score
                except ValueError:
                    pass
            
            # æ–¹å¼3: æŸ¥æ‰¾è¡Œæœ«çš„æ•°å­—
            lines = cleaned_response.split('\n')
            for line in reversed(lines):
                line = line.strip()
                if line:
                    try:
                        score = float(line)
                        if -1.0 <= score <= 1.0:
                            return score
                    except ValueError:
                        continue
            
            logger.warning(f"âš ï¸ æ— æ³•è§£æLLMè¯„åˆ†: {cleaned_response}")
            return None
            
        except Exception as e:
            logger.error(f"âŒ LLMè¯„åˆ†è§£æå¤±è´¥: {e}")
            return None
    
    def _fallback_reward_calculation(self, tool_result: Any, original_prompt: str) -> float:
        """å›é€€çš„å¥–åŠ±è®¡ç®—æ–¹æ³•ï¼ˆåŸæœ‰é€»è¾‘çš„æ”¹è¿›ç‰ˆæœ¬ï¼‰"""
        try:
            base_reward = 0.0  # ä»ä¸­æ€§å¼€å§‹
            
            # æ£€æŸ¥å·¥å…·ç»“æœæ˜¯å¦å­˜åœ¨
            if not tool_result:
                return -0.2  # æ— ç»“æœç»™äºˆè½»å¾®è´Ÿåˆ†
            
            result_text = self._prepare_tool_result_for_judgment(tool_result)
            
            # åŸºäºç»“æœè´¨é‡è¯„åˆ†
            if "é”™è¯¯" in result_text or "å¤±è´¥" in result_text or "æ— æ³•" in result_text:
                base_reward -= 0.3  # é”™è¯¯ä¿¡æ¯æ‰£åˆ†
            
            if len(result_text) > 50:  # æœ‰å®è´¨å†…å®¹
                base_reward += 0.3
                
                if len(result_text) > 200:  # å†…å®¹ä¸°å¯Œ
                    base_reward += 0.2
            
            # ç®€å•çš„ç›¸å…³æ€§æ£€æŸ¥
            prompt_keywords = [word for word in original_prompt.lower().split() if len(word) > 3]
            result_lower = result_text.lower()
            
            matching_keywords = sum(1 for keyword in prompt_keywords if keyword in result_lower)
            if prompt_keywords:
                relevance_score = matching_keywords / len(prompt_keywords)
                base_reward += relevance_score * 0.4
            
            return max(-1.0, min(1.0, base_reward))
            
        except Exception as e:
            logger.error(f"âŒ å›é€€å¥–åŠ±è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _estimate_prompt_complexity(self, prompt: str) -> float:
        """ä¼°ç®—æç¤ºè¯å¤æ‚åº¦ï¼ˆ0-1ä¹‹é—´ï¼‰"""
        if not prompt:
            return 0.0
        
        factors = {
            'length': min(1.0, len(prompt) / 500),  # é•¿åº¦å› å­
            'questions': min(1.0, prompt.count('?') * 0.2),  # é—®å·æ•°é‡
            'keywords': min(1.0, len([w for w in prompt.split() if len(w) > 5]) * 0.01),  # é•¿å•è¯
            'complexity_words': 0.1 * sum(1 for word in ['åˆ†æ', 'æ¯”è¾ƒ', 'è¯„ä¼°', 'è§£é‡Š', 'æ¨ç†'] 
                                         if word in prompt)
        }
        
        return min(1.0, sum(factors.values()) / 2)
    
    def _calculate_current_success_rate(self) -> float:
        """è®¡ç®—å½“å‰æˆåŠŸç‡"""
        try:
            # è¿™é‡Œå¯ä»¥åŸºäºå†å²æ•°æ®è®¡ç®—ï¼Œæš‚æ—¶è¿”å›é»˜è®¤å€¼
            return 0.8
        except:
            return 0.5
    
    def _infer_task_type(self, prompt: str) -> str:
        """æ¨æ–­ä»»åŠ¡ç±»å‹"""
        if not prompt:
            return "unknown"
        
        prompt_lower = prompt.lower()
        
        if any(word in prompt_lower for word in ['æœç´¢', 'æŸ¥æ‰¾', 'æœ€æ–°', 'æ–°é—»']):
            return "search"
        elif any(word in prompt_lower for word in ['åˆ†æ', 'è§£é‡Š', 'åŸç†', 'å¦‚ä½•']):
            return "analysis"
        elif any(word in prompt_lower for word in ['åˆ›ä½œ', 'å†™', 'ç”Ÿæˆ', 'è®¾è®¡']):
            return "generation"
        elif any(word in prompt_lower for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'ä¼˜ç¼ºç‚¹']):
            return "comparison"
        else:
            return "general"
    
    def _requires_search_analysis(self, prompt: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢"""
        if not prompt:
            return False
        
        search_indicators = ['æœ€æ–°', 'æ–°é—»', 'å®æ—¶', 'æœç´¢', 'æŸ¥æ‰¾', 'ä¿¡æ¯', 'èµ„æ–™']
        return any(indicator in prompt for indicator in search_indicators)
    
    def _requires_analysis_capability(self, prompt: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†æèƒ½åŠ›"""
        if not prompt:
            return False
        
        analysis_indicators = ['åˆ†æ', 'è§£é‡Š', 'åŸç†', 'æœºåˆ¶', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•å·¥ä½œ']
        return any(indicator in prompt for indicator in analysis_indicators)
    
    def _calculate_no_tool_probability(self, state: Dict[str, Any]) -> float:
        """è®¡ç®—ä¸ä½¿ç”¨å·¥å…·çš„æ¦‚ç‡"""
        base_prob = 0.2  # åŸºç¡€æ¦‚ç‡
        
        # å¦‚æœä»»åŠ¡ç±»å‹ä¸éœ€è¦å·¥å…·ï¼Œå¢åŠ ä¸ä½¿ç”¨å·¥å…·çš„æ¦‚ç‡
        if state.get('task_type') == 'general':
            base_prob += 0.3
        
        # å¦‚æœå·²ç»ä½¿ç”¨äº†å¾ˆå¤šå·¥å…·ï¼Œå¢åŠ åœæ­¢ä½¿ç”¨çš„æ¦‚ç‡
        if state.get('conversation_turns', 0) >= 2:
            base_prob += 0.4
        
        # å¦‚æœä¸éœ€è¦æœç´¢å’Œåˆ†æï¼Œå¢åŠ æ¦‚ç‡
        if not state.get('requires_search') and not state.get('requires_analysis'):
            base_prob += 0.2
        
        return min(0.8, base_prob)  # æœ€å¤§80%æ¦‚ç‡ä¸ä½¿ç”¨å·¥å…·
    
    def _fallback_tool_selection(self, state: Dict[str, Any], available_tools: List[str]) -> Optional[str]:
        """å›é€€çš„å·¥å…·é€‰æ‹©ç­–ç•¥"""
        real_tools = [tool for tool in available_tools if tool != "no_tool"]
        
        if not real_tools:
            return None
        
        # åŸºäºä»»åŠ¡ç±»å‹é€‰æ‹©å·¥å…·
        task_type = state.get('task_type', 'general')
        
        if task_type == 'search' and any('search' in tool.lower() for tool in real_tools):
            search_tools = [tool for tool in real_tools if 'search' in tool.lower()]
            return search_tools[0]
        
        # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªå·¥å…·
        return real_tools[0]
    
    def _prepare_tool_descriptions(self, available_tools: Optional[List[str]] = None) -> str:
        """å‡†å¤‡å·¥å…·æè¿°ä¿¡æ¯"""
        if not self.tool_registry:
            return "å½“å‰æ²¡æœ‰å¯ç”¨å·¥å…·ã€‚"
        
        if available_tools:
            # ä½¿ç”¨æŒ‡å®šçš„å·¥å…·åˆ—è¡¨
            tool_list = []
            for tool_name in available_tools:
                tool = get_tool(tool_name)
                if tool:
                    tool_list.append(f"- {tool.name}: {tool.description}")
        else:
            # ä½¿ç”¨æ‰€æœ‰å·²æ³¨å†Œå·¥å…·
            all_tools = [tool for tool in self.tool_registry]
            tool_list = [f"- {tool.name}: {tool.description}" for tool in all_tools]
        
        if not tool_list:
            return "å½“å‰æ²¡æœ‰å¯ç”¨å·¥å…·ã€‚"
        
        return "å¯ç”¨å·¥å…·:\n" + "\n".join(tool_list)
    
    def _build_tool_enhanced_prompt(self, original_prompt: str, tool_descriptions: str, 
                                   context: Optional[Dict] = None) -> str:
        """æ„å»ºå·¥å…·å¢å¼ºçš„æç¤ºè¯"""
        enhanced_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ï¼Œå…·æœ‰ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ã€‚åœ¨å›ç­”é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥è°ƒç”¨ä»¥ä¸‹å·¥å…·æ¥è·å–ä¿¡æ¯æˆ–æ‰§è¡Œæ“ä½œï¼š

{tool_descriptions}

è°ƒç”¨å·¥å…·çš„æ ¼å¼ï¼š
**TOOL_CALL**: [å·¥å…·åç§°] | [è°ƒç”¨å‚æ•°]

ä¾‹å¦‚ï¼š
**TOOL_CALL**: web_search | Pythonç¼–ç¨‹æœ€ä½³å®è·µ

è¯·æ ¹æ®ä»¥ä¸‹ä»»åŠ¡æ€è€ƒæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œå¦‚æœéœ€è¦ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°æ ¼å¼è°ƒç”¨å·¥å…·ï¼š

{original_prompt}

å¦‚æœä½ éœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¯·åœ¨å›ç­”ä¸­æ˜ç¡®è¡¨æ˜ï¼Œå¹¶ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼ã€‚å¦‚æœä¸éœ€è¦å·¥å…·ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚
"""
        
        if context:
            enhanced_prompt += f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}"
        
        return enhanced_prompt
    
    def _detect_tool_call_intent(self, response: str) -> Optional[Dict[str, Any]]:
        """æ£€æµ‹LLMå“åº”ä¸­çš„å·¥å…·è°ƒç”¨æ„å›¾"""
        import re
        
        # æŸ¥æ‰¾å·¥å…·è°ƒç”¨æ¨¡å¼
        pattern = r'\*\*TOOL_CALL\*\*:\s*([^\|]+)\|\s*(.+)'
        match = re.search(pattern, response)
        
        if match:
            tool_name = match.group(1).strip()
            tool_params = match.group(2).strip()
            
            return {
                'tool_name': tool_name,
                'tool_params': tool_params,
                'raw_call': match.group(0)
            }
        
        return None
    
    def _execute_detected_tool_call(self, tool_call_request: Dict[str, Any]) -> Optional[ToolResult]:
        """æ‰§è¡Œæ£€æµ‹åˆ°çš„å·¥å…·è°ƒç”¨"""
        try:
            tool_name = tool_call_request['tool_name']
            tool_params = tool_call_request['tool_params']
            
            # é€šè¿‡å·¥å…·æ³¨å†Œè¡¨æ‰§è¡Œå·¥å…·
            result = execute_tool(tool_name, tool_params)
            
            if result and result.success:
                logger.debug(f"âœ… å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸ")
                return result
            else:
                logger.warning(f"âš ï¸ å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {result.error_message if result else 'æœªçŸ¥é”™è¯¯'}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ å·¥å…·è°ƒç”¨å¼‚å¸¸: {e}")
            return None
    
    def _build_tool_followup_prompt(self, original_prompt: str, previous_response: str,
                                   tool_call: Dict[str, Any], tool_result: ToolResult) -> str:
        """æ„å»ºå·¥å…·è°ƒç”¨åçš„è·Ÿè¿›æç¤ºè¯"""
        
        tool_result_summary = ""
        if tool_result.success and tool_result.data:
            # æ ¹æ®å·¥å…·ç±»å‹æ ¼å¼åŒ–ç»“æœ
            if isinstance(tool_result.data, dict) and 'results' in tool_result.data:
                # æœç´¢ç»“æœæ ¼å¼åŒ–
                results = tool_result.data['results'][:3]  # åªå–å‰3ä¸ªç»“æœ
                tool_result_summary = f"æœç´¢åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœï¼š\n"
                for i, item in enumerate(results, 1):
                    tool_result_summary += f"{i}. {item.get('title', 'æ— æ ‡é¢˜')}\n   {item.get('snippet', 'æ— æ‘˜è¦')[:100]}...\n"
            else:
                tool_result_summary = str(tool_result.data)[:500] + "..."
        else:
            tool_result_summary = f"å·¥å…·è°ƒç”¨å¤±è´¥: {tool_result.error_message}"
        
        followup_prompt = f"""
åŸå§‹ä»»åŠ¡: {original_prompt}

ä½ åˆšæ‰è°ƒç”¨äº†å·¥å…·: {tool_call['tool_name']}
å·¥å…·è°ƒç”¨å‚æ•°: {tool_call['tool_params']}

å·¥å…·è¿”å›çš„ç»“æœ:
{tool_result_summary}

è¯·åŸºäºè¿™äº›å·¥å…·è·å–çš„ä¿¡æ¯ï¼Œç»§ç»­å®ŒæˆåŸå§‹ä»»åŠ¡ã€‚å¦‚æœè¿˜éœ€è¦è°ƒç”¨å…¶ä»–å·¥å…·ï¼Œè¯·ç»§ç»­ä½¿ç”¨ **TOOL_CALL** æ ¼å¼ã€‚å¦åˆ™ï¼Œè¯·æä¾›æœ€ç»ˆç­”æ¡ˆã€‚
"""
        
        return followup_prompt
    
    def _complete_initialization(self):
        """å®Œæˆå‰©ä½™çš„åˆå§‹åŒ–å·¥ä½œ"""
        # ğŸ”§ åˆå§‹åŒ–å„ä¸ªç»„ä»¶ - æ³¨å…¥å…±äº«ä¾èµ–
        self.prior_reasoner = PriorReasoner(self.api_key)  # è½»é‡çº§ï¼Œä¸éœ€è¦LLMå®¢æˆ·ç«¯
        self.path_generator = PathGenerator(self.api_key, llm_client=self.llm_client)  # æ³¨å…¥LLMå®¢æˆ·ç«¯
        self.mab_converger = MABConverger()
        
        # ğŸ—ï¸ æ–°å¢ï¼šåˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
        self.state_manager = StateManager()
        logger.info("ğŸ—ï¸ StateManager å·²åˆå§‹åŒ–ï¼Œæ”¯æŒç»Ÿä¸€çŠ¶æ€ç®¡ç†")
        
        # ğŸš€ æ–°å¢ï¼šæ€§èƒ½ä¼˜åŒ–å™¨
        if FEATURE_FLAGS.get("enable_performance_optimization", False):
            self.performance_optimizer = PerformanceOptimizer(PERFORMANCE_CONFIG)
            logger.info("ğŸš€ æ€§èƒ½ä¼˜åŒ–å™¨å·²å¯ç”¨")
        else:
            self.performance_optimizer = None
            logger.info("ğŸ“Š æ€§èƒ½ä¼˜åŒ–å™¨å·²ç¦ç”¨")
        
        # æ³¨å†Œç³»ç»Ÿå…³é—­å›è°ƒ - æš‚æ—¶ç¦ç”¨é¿å…é€’å½’é—®é¢˜
        # register_for_shutdown(lambda: shutdown_neogenesis_system(self), "MainController")
        logger.debug("âš ï¸ å…³é—­å›è°ƒå·²ç¦ç”¨ï¼Œé¿å…é€’å½’é—®é¢˜")
        
        # ç³»ç»ŸçŠ¶æ€
        self.total_rounds = 0
        self.decision_history = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_decisions': 0,
            'successful_decisions': 0,
            'avg_decision_time': 0.0,
            'component_performance': {
                'prior_reasoner': {'calls': 0, 'avg_time': 0.0},
                'path_generator': {'calls': 0, 'avg_time': 0.0},
                'mab_converger': {'calls': 0, 'avg_time': 0.0},
                'idea_verification': {'calls': 0, 'avg_time': 0.0, 'success_rate': 0.0}  # æ–°å¢éªŒè¯ç»Ÿè®¡
            }
        }
        
        # ğŸ’¡ Aha-Momentå†³ç­–ç³»ç»Ÿ
        self.aha_moment_stats = {
            'consecutive_failures': 0,         # è¿ç»­å¤±è´¥æ¬¡æ•°
            'last_failure_timestamp': None,    # æœ€åå¤±è´¥æ—¶é—´
            'total_aha_moments': 0,            # æ€»Aha-Momentæ¬¡æ•°
            'aha_success_rate': 0.0,           # Aha-MomentæˆåŠŸç‡
            'last_decision_success': True,     # ä¸Šæ¬¡å†³ç­–æ˜¯å¦æˆåŠŸ
            'failure_threshold': 3,            # è¿ç»­å¤±è´¥é˜ˆå€¼
            'confidence_threshold': 0.3,       # ç½®ä¿¡åº¦é˜ˆå€¼
            'aha_decision_history': []         # Aha-Momentå†³ç­–å†å²
        }
        
        logger.info("ğŸš€ MainControlleråˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨å·¥å…·å¢å¼ºçš„äº”é˜¶æ®µå†³ç­–ç³»ç»Ÿ")
        logger.info(f"ğŸ”§ å·¥å…·æ³¨å†Œè¡¨å·²è£…å¤‡: {'âœ…' if self.tool_registry else 'âŒ'} ç»Ÿä¸€å·¥å…·æ¥å£")
        
        # æ˜¾ç¤ºå·²æ³¨å†Œå·¥å…·
        if self.tool_registry:
            from .utils.tool_abstraction import list_available_tools
            tools = list_available_tools()
            logger.info(f"ğŸ” å·²æ³¨å†Œå·¥å…·: {len(tools)} ä¸ª ({', '.join(tools)})")
        
        # æ˜¾ç¤ºLLMç³»ç»ŸçŠ¶æ€
        if self.llm_manager:
            status = self.llm_manager.get_provider_status()
            logger.info(f"ğŸ§  LLMç³»ç»Ÿå·²è£…å¤‡: âœ… ç®¡ç†å™¨æ¨¡å¼")
            logger.info(f"   å¯ç”¨æä¾›å•†: {status['healthy_providers']}/{status['total_providers']}")
            
            # æ˜¾ç¤ºä¸»è¦æä¾›å•†
            if status['providers']:
                healthy_providers = [name for name, info in status['providers'].items() if info['healthy']]
                if healthy_providers:
                    logger.info(f"   æ´»è·ƒæä¾›å•†: {', '.join(healthy_providers)}")
        else:
            logger.info(f"ğŸ§  LLMç³»ç»Ÿå·²è£…å¤‡: {'âœ…' if self.llm_client else 'âŒ'} å›é€€æ¨¡å¼")
    
    def make_decision(self, user_query: str, deepseek_confidence: float = 0.5, 
                     execution_context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ğŸš€ äº”é˜¶æ®µæ™ºèƒ½éªŒè¯-å­¦ä¹ å†³ç­–æµç¨‹ - å†…ç½®æœç´¢éªŒè¯ç³»ç»Ÿ
        
        é˜¶æ®µä¸€ï¼šæ€ç»´ç§å­ç”Ÿæˆ (PriorReasoner) - ç†è§£é—®é¢˜ï¼Œç”Ÿæˆæ€ç»´ç§å­
        é˜¶æ®µäºŒï¼šç§å­éªŒè¯æ£€æŸ¥ (æ–°å¢) - éªŒè¯æ€ç»´ç§å­çš„å®è§‚æ–¹å‘
        é˜¶æ®µä¸‰ï¼šæ€ç»´è·¯å¾„ç”Ÿæˆ (PathGenerator) - åŸºäºç§å­ç”Ÿæˆå¤šæ¡è·¯å¾„
        é˜¶æ®µå››ï¼šè·¯å¾„éªŒè¯å­¦ä¹  (æ ¸å¿ƒåˆ›æ–°) - é€ä¸€éªŒè¯è·¯å¾„å¹¶å³æ—¶å­¦ä¹ 
        é˜¶æ®µäº”ï¼šæ™ºèƒ½æœ€ç»ˆå†³ç­– (å‡çº§) - åŸºäºéªŒè¯ç»“æœæ™ºèƒ½å†³ç­–
        
        æ ¸å¿ƒåˆ›æ–°ï¼šAIåœ¨æ€è€ƒé˜¶æ®µå°±è·å¾—å³æ—¶åé¦ˆï¼Œä¸å†ç­‰å¾…æœ€ç»ˆæ‰§è¡Œç»“æœ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            deepseek_confidence: DeepSeekå¯¹è¯¥ä»»åŠ¡çš„ç½®ä¿¡åº¦
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            å†³ç­–ç»“æœ
        """
        start_time = time.time()
        self.total_rounds += 1
        
        logger.info(f"ğŸš€ å¼€å§‹ç¬¬ {self.total_rounds} è½®äº”é˜¶æ®µæ™ºèƒ½éªŒè¯-å­¦ä¹ å†³ç­–")
        logger.info(f"   æŸ¥è¯¢: {user_query[:50]}...")
        logger.info(f"   DeepSeekç½®ä¿¡åº¦: {deepseek_confidence:.2f}")
        
        try:
            # ğŸ§  é˜¶æ®µä¸€ï¼šå…ˆéªŒæ¨ç† - ç”Ÿæˆæ€ç»´ç§å­ï¼ˆä¸å˜ï¼‰
            reasoner_start = time.time()
            thinking_seed = self.prior_reasoner.get_thinking_seed(user_query, execution_context)
            
            # å…¼å®¹æ€§ï¼šåŒæ—¶è·å–æ—§æ ¼å¼çš„æ•°æ®ç”¨äºå‘åå…¼å®¹
            task_confidence = self.prior_reasoner.assess_task_confidence(user_query, execution_context)
            complexity_info = self.prior_reasoner.analyze_task_complexity(user_query)
            
            reasoner_time = time.time() - reasoner_start
            self._update_component_performance('prior_reasoner', reasoner_time)
            
            logger.info(f"ğŸ§  é˜¶æ®µä¸€å®Œæˆ: æ€ç»´ç§å­ç”Ÿæˆ (é•¿åº¦: {len(thinking_seed)} å­—ç¬¦)")
            logger.debug(f"ğŸŒ± æ€ç»´ç§å­é¢„è§ˆ: {thinking_seed[:100]}...")
            
            # ğŸ” é˜¶æ®µäºŒï¼šéªŒè¯æ€ç»´ç§å­ï¼ˆæ–°å¢ï¼‰- å¯¹å®è§‚æ–¹å‘è¿›è¡Œå¿«é€ŸéªŒè¯
            seed_verification_start = time.time()
            seed_verification_result = self.verify_idea_feasibility(
                idea_text=thinking_seed,
                context={
                    'stage': 'thinking_seed',
                    'domain': 'strategic_planning',
                    'query': user_query,
                    **(execution_context if execution_context else {})
                }
            )
            seed_verification_time = time.time() - seed_verification_start
            
            # åˆ†æç§å­éªŒè¯ç»“æœ
            seed_feasibility = seed_verification_result.get('feasibility_analysis', {}).get('feasibility_score', 0.5)
            seed_reward = seed_verification_result.get('reward_score', 0.0)
            
            logger.info(f"ğŸ” é˜¶æ®µäºŒå®Œæˆ: æ€ç»´ç§å­éªŒè¯ (å¯è¡Œæ€§: {seed_feasibility:.2f}, å¥–åŠ±: {seed_reward:+.3f})")
            
            if seed_feasibility < 0.3:
                logger.warning(f"âš ï¸ æ€ç»´ç§å­æ–¹å‘å­˜åœ¨é—®é¢˜ (å¯è¡Œæ€§: {seed_feasibility:.2f})ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            
            # ğŸ›¤ï¸ é˜¶æ®µä¸‰ï¼šè·¯å¾„ç”Ÿæˆ - åŸºäºï¼ˆå·²éªŒè¯çš„ï¼‰æ€ç»´ç§å­ç”Ÿæˆæ€ç»´è·¯å¾„åˆ—è¡¨ï¼ˆä¸å˜ï¼‰
            generator_start = time.time()
            all_reasoning_paths = self.path_generator.generate_paths(
                thinking_seed=thinking_seed, 
                task=user_query,
                max_paths=6  # é™åˆ¶è·¯å¾„æ•°é‡ä»¥æé«˜æ€§èƒ½
            )
            generator_time = time.time() - generator_start
            self._update_component_performance('path_generator', generator_time)
            
            logger.info(f"ğŸ›¤ï¸ é˜¶æ®µä¸‰å®Œæˆ: ç”Ÿæˆäº† {len(all_reasoning_paths)} æ¡æ€ç»´è·¯å¾„")
            for i, path in enumerate(all_reasoning_paths[:3], 1):  # è®°å½•å‰3ä¸ªè·¯å¾„
                logger.debug(f"   è·¯å¾„{i}: {path.path_type} (ID: {path.path_id})")
            
            # ğŸš€ é˜¶æ®µå››ï¼šè·¯å¾„éªŒè¯å­¦ä¹ ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰- é€ä¸€éªŒè¯è·¯å¾„å¹¶å³æ—¶å­¦ä¹ 
            path_verification_start = time.time()
            # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæ™ºèƒ½è·¯å¾„éªŒè¯ï¼ˆæ”¯æŒå¹¶è¡ŒåŒ–å’Œè‡ªé€‚åº”ï¼‰
            verified_paths = []
            all_infeasible = True  # æ ‡è®°æ˜¯å¦æ‰€æœ‰è·¯å¾„éƒ½ä¸å¯è¡Œ
            
            # ç¡®å®šè¦éªŒè¯çš„è·¯å¾„æ•°é‡ï¼ˆè‡ªé€‚åº”ä¼˜åŒ–ï¼‰
            original_path_count = len(all_reasoning_paths)
            if self.performance_optimizer and PERFORMANCE_CONFIG.get("enable_adaptive_path_count", False):
                # æå–å¤æ‚åº¦åˆ†æ•°
                complexity_score = complexity_info.get('overall_score', 0.5) if isinstance(complexity_info, dict) else 0.5
                optimal_count = self.performance_optimizer.adaptive_selector.get_optimal_path_count(
                    confidence=deepseek_confidence,
                    complexity=complexity_score
                )
                # åªéªŒè¯å‰Næ¡æœ€æœ‰æ½œåŠ›çš„è·¯å¾„
                paths_to_verify = all_reasoning_paths[:optimal_count]
                logger.info(f"ğŸ¯ è‡ªé€‚åº”ä¼˜åŒ–: ä»{original_path_count}æ¡è·¯å¾„ä¸­é€‰æ‹©{optimal_count}æ¡è¿›è¡ŒéªŒè¯")
            else:
                paths_to_verify = all_reasoning_paths
            
            logger.info(f"ğŸ”¬ é˜¶æ®µå››å¼€å§‹: éªŒè¯ {len(paths_to_verify)} æ¡æ€ç»´è·¯å¾„")
            
            # ğŸš€ å¹¶è¡ŒéªŒè¯è·¯å¾„ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
            if (self.performance_optimizer and 
                PERFORMANCE_CONFIG.get("enable_parallel_path_verification", False) and 
                len(paths_to_verify) > 1):
                
                logger.info(f"âš¡ å¯ç”¨å¹¶è¡ŒéªŒè¯æ¨¡å¼ - æœ€å¤§å¹¶å‘æ•°: {PERFORMANCE_CONFIG.get('max_concurrent_verifications', 3)}")
                
                # åˆ›å»ºéªŒè¯ä»»åŠ¡
                def create_verification_task(path):
                    def verify_single_path(p):
                        return self.verify_idea_feasibility(
                            idea_text=f"{p.path_type}: {p.description}",
                            context={
                                'stage': 'reasoning_path',
                                'path_id': p.path_id,
                                'path_type': p.path_type,
                                'query': user_query,
                                **(execution_context if execution_context else {})
                            }
                        )
                    return (path, verify_single_path)
                
                verification_tasks = [create_verification_task(path) for path in paths_to_verify]
                
                # å¹¶è¡Œæ‰§è¡ŒéªŒè¯
                parallel_results = self.performance_optimizer.parallel_verifier.verify_paths_parallel(verification_tasks)
                
                # å¤„ç†å¹¶è¡ŒéªŒè¯ç»“æœ
                for i, (path, result) in enumerate(zip(paths_to_verify, parallel_results)):
                    if result is None:
                        logger.warning(f"âš ï¸ è·¯å¾„ {path.path_type} å¹¶è¡ŒéªŒè¯å¤±è´¥ï¼Œè·³è¿‡")
                        continue
                    
                    # æå–éªŒè¯ç»“æœ
                    path_feasibility = result.get('feasibility_analysis', {}).get('feasibility_score', 0.5)
                    path_reward = result.get('reward_score', 0.0)
                    verification_success = not result.get('fallback', False)
                    
                    # ğŸ’¡ å³æ—¶å­¦ä¹ ï¼šç«‹å³å°†éªŒè¯ç»“æœåé¦ˆç»™MABç³»ç»Ÿ
                    if verification_success and path_feasibility > 0.3:
                        # å¯è¡Œçš„è·¯å¾„ - æ­£é¢å­¦ä¹ ä¿¡å·
                        self.mab_converger.update_path_performance(
                            path_id=path.strategy_id,  # ğŸ¯ æ ¹æºä¿®å¤ï¼šä½¿ç”¨ç­–ç•¥IDè¿›è¡Œå­¦ä¹ 
                            success=True,
                            reward=path_reward
                        )
                        all_infeasible = False  # è‡³å°‘æœ‰ä¸€ä¸ªè·¯å¾„å¯è¡Œ
                        logger.debug(f"âœ… è·¯å¾„ {path.path_type} éªŒè¯é€šè¿‡: å¯è¡Œæ€§={path_feasibility:.2f}, å¥–åŠ±={path_reward:+.3f}")
                    else:
                        # ä¸å¯è¡Œçš„è·¯å¾„ - è´Ÿé¢å­¦ä¹ ä¿¡å·
                        self.mab_converger.update_path_performance(
                            path_id=path.strategy_id,  # ğŸ¯ æ ¹æºä¿®å¤ï¼šä½¿ç”¨ç­–ç•¥IDè¿›è¡Œå­¦ä¹ 
                            success=False,
                            reward=path_reward  # å¯èƒ½æ˜¯è´Ÿå€¼
                        )
                        logger.debug(f"âŒ è·¯å¾„ {path.path_type} éªŒè¯å¤±è´¥: å¯è¡Œæ€§={path_feasibility:.2f}, å¥–åŠ±={path_reward:+.3f}")
                    
                    # è®°å½•éªŒè¯ç»“æœ
                    verified_paths.append({
                        'path': path,
                        'verification_result': result,
                        'feasibility_score': path_feasibility,
                        'reward_score': path_reward,
                        'is_feasible': path_feasibility > 0.3
                    })
                    
                    # ğŸ”„ æ—©æœŸç»ˆæ­¢æ£€æŸ¥ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
                    if (PERFORMANCE_CONFIG.get("enable_early_termination", False) and 
                        len(verified_paths) >= 3 and
                        self.performance_optimizer.adaptive_selector.should_early_terminate(verified_paths)):
                        logger.info(f"ğŸ”„ æ—©æœŸç»ˆæ­¢: å·²éªŒè¯{len(verified_paths)}æ¡è·¯å¾„ï¼Œç»“æœä¸€è‡´æ€§è¶³å¤Ÿ")
                        break
                
            else:
                # ğŸ”„ ä¼ ç»Ÿä¸²è¡ŒéªŒè¯ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
                logger.info("ğŸ“Š ä½¿ç”¨ä¼ ç»Ÿä¸²è¡ŒéªŒè¯æ¨¡å¼")
                
                for i, path in enumerate(paths_to_verify, 1):
                    logger.debug(f"ğŸ”¬ éªŒè¯è·¯å¾„ {i}/{len(paths_to_verify)}: {path.path_type}")
                    
                    # ğŸ§  æ™ºèƒ½ç¼“å­˜æ£€æŸ¥
                    cache_key = f"{path.path_type}_{path.description[:50]}"
                    cached_result = None
                    if (self.performance_optimizer and 
                        PERFORMANCE_CONFIG.get("enable_intelligent_caching", False)):
                        cached_result = self.performance_optimizer.cache.get(cache_key, execution_context)
                    
                    if cached_result:
                        logger.debug(f"ğŸ’¾ ä½¿ç”¨ç¼“å­˜ç»“æœ: {path.path_type}")
                        path_verification_result = cached_result
                    else:
                        # éªŒè¯å•ä¸ªè·¯å¾„
                        path_verification_result = self.verify_idea_feasibility(
                            idea_text=f"{path.path_type}: {path.description}",
                            context={
                                'stage': 'reasoning_path',
                                'path_id': path.path_id,
                                'path_type': path.path_type,
                                'query': user_query,
                                **(execution_context if execution_context else {})
                            }
                        )
                        
                        # ğŸ’¾ ç¼“å­˜ç»“æœ
                        if (self.performance_optimizer and 
                            PERFORMANCE_CONFIG.get("enable_intelligent_caching", False)):
                            self.performance_optimizer.cache.set(cache_key, path_verification_result, execution_context)
                    
                    # æå–éªŒè¯ç»“æœ
                    path_feasibility = path_verification_result.get('feasibility_analysis', {}).get('feasibility_score', 0.5)
                    path_reward = path_verification_result.get('reward_score', 0.0)
                    verification_success = not path_verification_result.get('fallback', False)
                    
                    # ğŸ’¡ å³æ—¶å­¦ä¹ ï¼šç«‹å³å°†éªŒè¯ç»“æœåé¦ˆç»™MABç³»ç»Ÿ
                    if verification_success and path_feasibility > 0.3:
                        # å¯è¡Œçš„è·¯å¾„ - æ­£é¢å­¦ä¹ ä¿¡å·
                        self.mab_converger.update_path_performance(
                            path_id=path.strategy_id,  # ğŸ¯ æ ¹æºä¿®å¤ï¼šä½¿ç”¨ç­–ç•¥IDè¿›è¡Œå­¦ä¹ 
                            success=True,
                            reward=path_reward
                        )
                        all_infeasible = False  # è‡³å°‘æœ‰ä¸€ä¸ªè·¯å¾„å¯è¡Œ
                        logger.debug(f"âœ… è·¯å¾„ {path.path_type} éªŒè¯é€šè¿‡: å¯è¡Œæ€§={path_feasibility:.2f}, å¥–åŠ±={path_reward:+.3f}")
                    else:
                        # ä¸å¯è¡Œçš„è·¯å¾„ - è´Ÿé¢å­¦ä¹ ä¿¡å·
                        self.mab_converger.update_path_performance(
                            path_id=path.strategy_id,  # ğŸ¯ æ ¹æºä¿®å¤ï¼šä½¿ç”¨ç­–ç•¥IDè¿›è¡Œå­¦ä¹ 
                            success=False,
                            reward=path_reward  # å¯èƒ½æ˜¯è´Ÿå€¼
                        )
                        logger.debug(f"âŒ è·¯å¾„ {path.path_type} éªŒè¯å¤±è´¥: å¯è¡Œæ€§={path_feasibility:.2f}, å¥–åŠ±={path_reward:+.3f}")
                    
                    # è®°å½•éªŒè¯ç»“æœ
                    verified_paths.append({
                        'path': path,
                        'verification_result': path_verification_result,
                        'feasibility_score': path_feasibility,
                        'reward_score': path_reward,
                        'is_feasible': path_feasibility > 0.3
                    })
                    
                    # ğŸ”„ æ—©æœŸç»ˆæ­¢æ£€æŸ¥ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
                    if (self.performance_optimizer and 
                        PERFORMANCE_CONFIG.get("enable_early_termination", False) and 
                        len(verified_paths) >= 3 and
                        self.performance_optimizer.adaptive_selector.should_early_terminate(verified_paths)):
                        logger.info(f"ğŸ”„ æ—©æœŸç»ˆæ­¢: å·²éªŒè¯{len(verified_paths)}æ¡è·¯å¾„ï¼Œç»“æœä¸€è‡´æ€§è¶³å¤Ÿ")
                        break
            
            path_verification_time = time.time() - path_verification_start
            feasible_count = sum(1 for vp in verified_paths if vp['is_feasible'])
            
            logger.info(f"ğŸ”¬ é˜¶æ®µå››å®Œæˆ: {feasible_count}/{len(all_reasoning_paths)} æ¡è·¯å¾„å¯è¡Œ")
            logger.info(f"   ğŸ’¡ å³æ—¶å­¦ä¹ : MABç³»ç»Ÿå·²æ›´æ–°æ‰€æœ‰è·¯å¾„æƒé‡")
            
            # ğŸ¯ é˜¶æ®µäº”ï¼šæ™ºèƒ½æœ€ç»ˆå†³ç­–ï¼ˆå‡çº§ï¼‰- åŸºäºéªŒè¯ç»“æœæ™ºèƒ½å†³ç­–
            final_decision_start = time.time()
            
            if all_infeasible:
                # ğŸš¨ æ‰€æœ‰è·¯å¾„éƒ½ä¸å¯è¡Œ - è§¦å‘æ™ºèƒ½ç»•é“æ€è€ƒ
                logger.warning("ğŸš¨ æ‰€æœ‰æ€ç»´è·¯å¾„éƒ½è¢«éªŒè¯ä¸ºä¸å¯è¡Œï¼Œè§¦å‘æ™ºèƒ½ç»•é“æ€è€ƒ")
                
                # è¿™æ˜¯æ›´æ™ºèƒ½çš„Aha-Momentè§¦å‘å™¨
                chosen_path, mab_decision = self._execute_intelligent_detour_thinking(
                    user_query, thinking_seed, all_reasoning_paths, verified_paths
                )
                
                mab_decision.update({
                    'selection_algorithm': 'intelligent_detour',
                    'all_paths_infeasible': True,
                    'detour_triggered': True,
                    'verification_triggered_detour': True
                })
                
                logger.info(f"ğŸš€ æ™ºèƒ½ç»•é“å®Œæˆ: é€‰æ‹©åˆ›æ–°è·¯å¾„ '{chosen_path.path_type}'")
                
            else:
                # âœ… è‡³å°‘æœ‰å¯è¡Œè·¯å¾„ - ä½¿ç”¨å¢å¼ºçš„MABé€‰æ‹©
                logger.info("âœ… å‘ç°å¯è¡Œè·¯å¾„ï¼Œä½¿ç”¨éªŒè¯å¢å¼ºçš„MABå†³ç­–")
                
                # MABç°åœ¨å·²ç»æœ‰äº†å³æ—¶å­¦ä¹ çš„æƒé‡ï¼Œä¼šè‡ªç„¶å€¾å‘äºå¯è¡Œè·¯å¾„
                chosen_path = self.mab_converger.select_best_path(all_reasoning_paths)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¼ ç»Ÿçš„Aha-Momentï¼ˆä½œä¸ºé¢å¤–ä¿é™©ï¼‰
                aha_triggered, aha_reason = self._check_aha_moment_trigger(chosen_path)
                
                if aha_triggered:
                    logger.info(f"ğŸ’¡ é¢å¤–è§¦å‘ä¼ ç»ŸAha-Moment: {aha_reason}")
                    chosen_path, _ = self._execute_aha_moment_thinking(
                        user_query, thinking_seed, chosen_path, all_reasoning_paths
                    )
                
                mab_decision = {
                    'chosen_path': chosen_path,
                    'available_paths': all_reasoning_paths,
                    'verified_paths': verified_paths,
                    'selection_algorithm': 'verification_enhanced_mab',
                    'converged': self.mab_converger.check_path_convergence(),
                    'all_paths_infeasible': False,
                    'feasible_paths_count': feasible_count,
                    'total_paths_count': len(all_reasoning_paths),
                    'verification_triggered_detour': False,
                    'traditional_aha_triggered': aha_triggered
                }
                
                logger.info(f"ğŸ¯ æ™ºèƒ½å†³ç­–å®Œæˆ: é€‰æ‹©éªŒè¯ä¼˜åŒ–è·¯å¾„ '{chosen_path.path_type}'")
            
            final_decision_time = time.time() - final_decision_start
            total_mab_time = path_verification_time + final_decision_time
            self._update_component_performance('mab_converger', total_mab_time)
            
            # è®¡ç®—æ€»ä½“å†³ç­–æ—¶é—´
            total_decision_time = time.time() - start_time
            
            # æ„å»ºå‡çº§ç‰ˆäº”é˜¶æ®µå†³ç­–ç»“æœ
            decision_result = {
                # åŸºæœ¬ä¿¡æ¯
                'timestamp': time.time(),
                'round_number': self.total_rounds,
                'user_query': user_query,
                'deepseek_confidence': deepseek_confidence,
                'execution_context': execution_context,
                
                # ğŸš€ äº”é˜¶æ®µå†³ç­–ç»“æœ
                'thinking_seed': thinking_seed,  # é˜¶æ®µä¸€ï¼šæ€ç»´ç§å­
                'seed_verification': seed_verification_result,  # é˜¶æ®µäºŒï¼šç§å­éªŒè¯
                'chosen_path': chosen_path,  # æœ€ç»ˆé€‰ä¸­çš„æ€ç»´è·¯å¾„
                'available_paths': all_reasoning_paths,  # é˜¶æ®µä¸‰ï¼šæ‰€æœ‰å€™é€‰è·¯å¾„
                'verified_paths': verified_paths,  # é˜¶æ®µå››ï¼šéªŒè¯ç»“æœ
                'mab_decision': mab_decision,  # é˜¶æ®µäº”ï¼šæœ€ç»ˆå†³ç­–è¯¦æƒ…
                
                # å‘åå…¼å®¹å­—æ®µ
                'selected_path': chosen_path,  # å…¼å®¹æ—§æ¥å£
                'task_confidence': task_confidence,
                'complexity_analysis': complexity_info,
                
                # å†³ç­–å…ƒä¿¡æ¯
                'reasoning': f"äº”é˜¶æ®µæ™ºèƒ½éªŒè¯-å­¦ä¹ å†³ç­–: {chosen_path.path_type} - {chosen_path.description}",
                'path_count': len(all_reasoning_paths),
                'feasible_path_count': feasible_count,
                'architecture_version': '5-stage-verification',  # æ–°å¢ï¼šæ¶æ„ç‰ˆæœ¬æ ‡è¯†
                'verification_enabled': True,  # æ ‡è¯†å¯ç”¨éªŒè¯
                'instant_learning_enabled': True,  # æ ‡è¯†å¯ç”¨å³æ—¶å­¦ä¹ 
                
                # ğŸ”¬ éªŒè¯ç»Ÿè®¡
                'verification_stats': {
                    'seed_feasibility': seed_feasibility,
                    'seed_reward': seed_reward,
                    'paths_verified': len(verified_paths),
                    'feasible_paths': feasible_count,
                    'infeasible_paths': len(verified_paths) - feasible_count,
                    'all_paths_infeasible': all_infeasible,
                    'average_path_feasibility': sum(vp['feasibility_score'] for vp in verified_paths) / len(verified_paths) if verified_paths else 0.0,
                    'total_verification_time': seed_verification_time + path_verification_time
                },
                
                # æ€§èƒ½æŒ‡æ ‡
                'performance_metrics': {
                    'total_time': total_decision_time,
                    'stage1_reasoner_time': reasoner_time,
                    'stage2_seed_verification_time': seed_verification_time,
                    'stage3_generator_time': generator_time,
                    'stage4_path_verification_time': path_verification_time,
                    'stage5_final_decision_time': final_decision_time,
                    'stages_breakdown': {
                        'thinking_seed_generation': reasoner_time,
                        'seed_verification': seed_verification_time,
                        'path_generation': generator_time,
                        'path_verification_learning': path_verification_time,
                        'intelligent_final_decision': final_decision_time
                    }
                }
            }
            
            # è®°å½•å†³ç­–å†å²
            self.decision_history.append(decision_result)
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦
            max_history = SYSTEM_LIMITS["max_decision_history"]
            if len(self.decision_history) > max_history:
                self.decision_history = self.decision_history[-max_history//2:]
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self.performance_stats['total_decisions'] += 1
            self._update_avg_decision_time(total_decision_time)
            
            logger.info(f"ğŸ‰ äº”é˜¶æ®µæ™ºèƒ½éªŒè¯-å­¦ä¹ å†³ç­–å®Œæˆ:")
            logger.info(f"   ğŸŒ± æ€ç»´ç§å­: {len(thinking_seed)}å­—ç¬¦ (å¯è¡Œæ€§: {seed_feasibility:.2f})")
            logger.info(f"   ğŸ›¤ï¸ ç”Ÿæˆè·¯å¾„: {len(all_reasoning_paths)}æ¡")
            logger.info(f"   ğŸ”¬ éªŒè¯ç»“æœ: {feasible_count}æ¡å¯è¡Œ/{len(verified_paths)}æ¡æ€»æ•°")
            logger.info(f"   ğŸ¯ æœ€ç»ˆé€‰æ‹©: {chosen_path.path_type}")
            logger.info(f"   ğŸ’¡ å³æ—¶å­¦ä¹ : MABæƒé‡å·²æ›´æ–°")
            logger.info(f"   â±ï¸ æ€»è€—æ—¶: {total_decision_time:.3f}s")
            logger.info(f"      - é˜¶æ®µä¸€(ç§å­ç”Ÿæˆ): {reasoner_time:.3f}s")
            logger.info(f"      - é˜¶æ®µäºŒ(ç§å­éªŒè¯): {seed_verification_time:.3f}s")
            logger.info(f"      - é˜¶æ®µä¸‰(è·¯å¾„ç”Ÿæˆ): {generator_time:.3f}s") 
            logger.info(f"      - é˜¶æ®µå››(è·¯å¾„éªŒè¯): {path_verification_time:.3f}s")
            logger.info(f"      - é˜¶æ®µäº”(æ™ºèƒ½å†³ç­–): {final_decision_time:.3f}s")
            
            return decision_result
            
        except Exception as e:
            logger.error(f"âŒ å†³ç­–è¿‡ç¨‹å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯å†³ç­–ç»“æœ
            return self._create_error_decision_result(user_query, str(e), time.time() - start_time)
    
    def _update_component_performance(self, component_name: str, execution_time: float):
        """æ›´æ–°ç»„ä»¶æ€§èƒ½ç»Ÿè®¡"""
        component_stats = self.performance_stats['component_performance'][component_name]
        component_stats['calls'] += 1
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        current_avg = component_stats['avg_time']
        call_count = component_stats['calls']
        component_stats['avg_time'] = (current_avg * (call_count - 1) + execution_time) / call_count
    
    def _update_avg_decision_time(self, decision_time: float):
        """æ›´æ–°å¹³å‡å†³ç­–æ—¶é—´"""
        current_avg = self.performance_stats['avg_decision_time']
        total_decisions = self.performance_stats['total_decisions']
        
        if total_decisions == 1:
            self.performance_stats['avg_decision_time'] = decision_time
        else:
            self.performance_stats['avg_decision_time'] = (
                current_avg * (total_decisions - 1) + decision_time
            ) / total_decisions
    
    def _create_error_decision_result(self, user_query: str, error_msg: str, execution_time: float) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯å†³ç­–ç»“æœ"""
        return {
            'timestamp': time.time(),
            'round_number': self.total_rounds,
            'user_query': user_query,
            'selected_dimensions': {},
            'confidence_scores': {},
            'task_confidence': 0.0,
            'complexity_analysis': {'complexity_score': 0.5, 'estimated_domain': 'error'},
            'mab_decisions': {},
            'reasoning': f"å†³ç­–å¤±è´¥: {error_msg}",
            'fallback_used': True,
            'component_architecture': True,
            'error': error_msg,
            'performance_metrics': {
                'total_time': execution_time,
                'error_occurred': True
            }
        }
    
    def update_performance_feedback(self, decision_result: Dict[str, Any], 
                                  execution_success: bool, execution_time: float = 30.0,
                                  user_satisfaction: float = 0.5, rl_reward: float = 0.5):
        """
        æ›´æ–°ä¸‰é˜¶æ®µå†³ç­–æ€§èƒ½åé¦ˆ - å‡çº§ç‰ˆç»„ä»¶åŒ–æ¶æ„
        
        Args:
            decision_result: å†³ç­–ç»“æœ
            execution_success: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
            execution_time: æ‰§è¡Œæ—¶é—´
            user_satisfaction: ç”¨æˆ·æ»¡æ„åº¦
            rl_reward: å¼ºåŒ–å­¦ä¹ å¥–åŠ±
        """
        try:
            # ğŸ¯ æ ¸å¿ƒæ›´æ–°ï¼šåŸºäºé€‰ä¸­çš„æ€ç»´è·¯å¾„æ›´æ–°MABæ€§èƒ½
            chosen_path = decision_result.get('chosen_path') or decision_result.get('selected_path')
            if chosen_path:
                # ä½¿ç”¨å‡çº§åçš„è·¯å¾„æ€§èƒ½æ›´æ–°æ–¹æ³•
                self.mab_converger.update_path_performance(
                    path_id=chosen_path.strategy_id,  # ğŸ¯ æ ¹æºä¿®å¤ï¼šä½¿ç”¨ç­–ç•¥IDè¿›è¡Œå­¦ä¹ 
                    success=execution_success,
                    reward=rl_reward
                )
                logger.debug(f"ğŸ° å·²æ›´æ–°è·¯å¾„æ€§èƒ½: {chosen_path.strategy_id} -> æˆåŠŸ={execution_success}, å¥–åŠ±={rl_reward:.3f}")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°é€‰ä¸­çš„æ€ç»´è·¯å¾„ï¼Œæ— æ³•æ›´æ–°MABæ€§èƒ½")
            
            # ğŸ“Š æ›´æ–°å…ˆéªŒæ¨ç†å™¨çš„ç½®ä¿¡åº¦å†å²
            task_confidence = decision_result.get('task_confidence', 0.5)
            if hasattr(self.prior_reasoner, 'update_confidence_feedback'):
                self.prior_reasoner.update_confidence_feedback(
                    task_confidence, execution_success, execution_time
                )
            
            # ğŸ§  æ–°å¢ï¼šæ›´æ–°æ€ç»´ç§å­çš„æ•ˆæœè·Ÿè¸ª
            thinking_seed = decision_result.get('thinking_seed')
            if thinking_seed and hasattr(self.prior_reasoner, 'update_seed_feedback'):
                self.prior_reasoner.update_seed_feedback(
                    thinking_seed, execution_success, rl_reward
                )
            
            # ğŸ“ˆ æ›´æ–°ç³»ç»Ÿçº§æ€§èƒ½ç»Ÿè®¡
            if execution_success:
                self.performance_stats['successful_decisions'] += 1
            
            # ğŸ’¡ æ›´æ–°Aha-Momentå†³ç­–åé¦ˆ
            self.update_aha_moment_feedback(execution_success)
            
            # ğŸ“ æ ‡è®°å†³ç­–å†å²ä¸­çš„æ‰§è¡Œç»“æœï¼ˆç”¨äºåç»­å¤±è´¥æ£€æµ‹ï¼‰
            if self.decision_history:
                self.decision_history[-1]['execution_success'] = execution_success
            
            # ğŸ•’ è®°å½•è¯¦ç»†åé¦ˆåˆ°å†³ç­–å†å²ä¸­
            if self.decision_history and self.decision_history[-1]['round_number'] == decision_result.get('round_number'):
                feedback_data = {
                    'execution_success': execution_success,
                    'execution_time': execution_time,
                    'user_satisfaction': user_satisfaction,
                    'rl_reward': rl_reward,
                    'feedback_timestamp': time.time(),
                    'architecture_version': '3-stage',  # æ ‡è¯†ä¸ºä¸‰é˜¶æ®µæ¶æ„åé¦ˆ
                    'strategy_id': chosen_path.strategy_id if chosen_path else None,  # ğŸ¯ æ ¹æºä¿®å¤ï¼šè®°å½•ç­–ç•¥ID
                    'instance_id': chosen_path.instance_id if chosen_path else None,  # ä¿ç•™å®ä¾‹IDç”¨äºè¿½è¸ª
                    'path_type': chosen_path.path_type if chosen_path else None
                }
                self.decision_history[-1]['feedback'] = feedback_data
            
            # ğŸ“Š è®¡ç®—å¹¶è®°å½•æ€§èƒ½æŒ‡æ ‡
            success_rate = self.performance_stats['successful_decisions'] / max(self.performance_stats['total_decisions'], 1)
            
            logger.info(f"ğŸ“ˆ ä¸‰é˜¶æ®µæ€§èƒ½åé¦ˆå·²æ›´æ–°:")
            logger.info(f"   ğŸ¯ è·¯å¾„: {chosen_path.path_type if chosen_path else 'Unknown'}")
            logger.info(f"   âœ… æ‰§è¡Œ: æˆåŠŸ={execution_success}, è€—æ—¶={execution_time:.2f}s")
            logger.info(f"   ğŸ å¥–åŠ±: RL={rl_reward:.3f}, æ»¡æ„åº¦={user_satisfaction:.3f}")
            logger.info(f"   ğŸ“Š æ•´ä½“æˆåŠŸç‡: {success_rate:.1%}")
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä¸‰é˜¶æ®µæ€§èƒ½åé¦ˆå¤±è´¥: {e}")
            logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ä¸‰é˜¶æ®µæ™ºèƒ½å†³ç­–ç³»ç»ŸçŠ¶æ€"""
        try:
            # è®¡ç®—æˆåŠŸç‡
            success_rate = 0.0
            if self.performance_stats['total_decisions'] > 0:
                success_rate = self.performance_stats['successful_decisions'] / self.performance_stats['total_decisions']
            
            # è·å–å„ç»„ä»¶ç»Ÿè®¡ï¼ˆå®‰å…¨è°ƒç”¨ï¼Œé¿å…æ–¹æ³•ä¸å­˜åœ¨çš„é”™è¯¯ï¼‰
            prior_reasoner_stats = {}
            if hasattr(self.prior_reasoner, 'get_confidence_statistics'):
                prior_reasoner_stats = self.prior_reasoner.get_confidence_statistics()
            else:
                prior_reasoner_stats = {'total_assessments': 0, 'avg_confidence': 0.5, 'confidence_trend': 'stable'}
            
            path_generator_stats = {}
            if hasattr(self.path_generator, 'get_generation_statistics'):
                path_generator_stats = self.path_generator.get_generation_statistics()
            else:
                path_generator_stats = {'total_generations': 0, 'fallback_usage_rate': 0.0, 'avg_dimensions_per_generation': 0}
            
            # ä½¿ç”¨å‡çº§åçš„MABçŠ¶æ€è·å–æ–¹æ³•
            mab_stats = self.mab_converger.get_system_status()
            
            # è·å–è·¯å¾„çº§ç»Ÿè®¡
            path_summary = {}
            if hasattr(self.mab_converger, 'get_system_path_summary'):
                path_summary = self.mab_converger.get_system_path_summary()
            
            return {
                # åŸºæœ¬ä¿¡æ¯
                'total_rounds': self.total_rounds,
                'architecture_version': '3-stage',  # æ ‡è¯†æ¶æ„ç‰ˆæœ¬
                'component_architecture': True,  # å‘åå…¼å®¹
                
                # ç³»ç»Ÿæ€§èƒ½
                'system_performance': {
                    'success_rate': success_rate,
                    'avg_decision_time': self.performance_stats['avg_decision_time'],
                    'total_decisions': self.performance_stats['total_decisions'],
                    'successful_decisions': self.performance_stats['successful_decisions']
                },
                
                # ä¸‰é˜¶æ®µç»„ä»¶çŠ¶æ€
                'component_status': {
                    'stage1_prior_reasoner': {
                        'assessments_count': prior_reasoner_stats.get('total_assessments', 0),
                        'avg_confidence': prior_reasoner_stats.get('avg_confidence', 0.5),
                        'confidence_trend': prior_reasoner_stats.get('confidence_trend', 'stable'),
                        'thinking_seeds_generated': self.total_rounds,  # è¿‘ä¼¼å€¼
                        'performance': self.performance_stats['component_performance']['prior_reasoner']
                    },
                    'stage2_path_generator': {
                        'generations_count': path_generator_stats.get('total_generations', 0),
                        'fallback_rate': path_generator_stats.get('fallback_usage_rate', 0.0),
                        'avg_paths_per_generation': path_generator_stats.get('avg_dimensions_per_generation', 0),
                        'performance': self.performance_stats['component_performance']['path_generator']
                    },
                    'stage3_mab_converger': {
                        'mode': mab_stats.get('mode', 'path_selection'),
                        'total_paths': mab_stats.get('total_paths', 0),
                        'active_paths': mab_stats.get('active_paths', 0),
                        'total_selections': mab_stats.get('total_selections', 0),
                        'convergence_level': mab_stats.get('convergence_level', 0.0),
                        'most_popular_path_type': mab_stats.get('most_popular_path_type'),
                        'performance': self.performance_stats['component_performance']['mab_converger']
                    }
                },
                
                # è·¯å¾„é€‰æ‹©çŠ¶æ€
                'path_selection_status': {
                    'total_paths_tracked': path_summary.get('total_paths', 0),
                    'is_converged': path_summary.get('is_converged', False),
                    'convergence_level': path_summary.get('convergence_level', 0.0),
                    'most_used_path': path_summary.get('most_used_path'),
                    'best_performing_path': path_summary.get('best_performing_path'),
                    'algorithm_performance': path_summary.get('algorithm_performance', {})
                },
                
                # ç³»ç»Ÿèµ„æºä½¿ç”¨
                'memory_usage': {
                    'decision_history_size': len(self.decision_history),
                    'path_arms_count': len(getattr(self.mab_converger, 'path_arms', {})),
                    'path_generation_cache_size': len(getattr(self.path_generator, 'path_generation_cache', {})),
                    'confidence_cache_size': len(getattr(self.prior_reasoner, 'assessment_cache', {}))
                },
                
                # ğŸš€ æ€§èƒ½ä¼˜åŒ–å™¨çŠ¶æ€
                'performance_optimization': self._get_performance_optimization_status(),
                
                # æ‰©å±•åŠŸèƒ½çŠ¶æ€
                'extended_features': {
                    'thinking_seed_tracking': True,  # æ–°åŠŸèƒ½æ ‡è¯†
                    'path_performance_tracking': True,
                    'multi_algorithm_mab': True,
                    'aha_moment_decision_enabled': True  # ğŸ’¡ æ–°å¢ï¼šAha-Momentå†³ç­–åŠŸèƒ½
                },
                
                # ğŸ’¡ Aha-Momentå†³ç­–ç³»ç»ŸçŠ¶æ€
                'aha_moment_system': self.get_aha_moment_stats(),
                
                # ğŸ’¡ è·¯å¾„ç½®ä¿¡åº¦åˆ†æ
                'confidence_analysis': self.mab_converger.get_confidence_analysis(),
                
                # ğŸ’¡ åˆ›é€ æ€§ç»•é“ç»Ÿè®¡
                'creative_bypass_stats': self.path_generator.get_creative_bypass_stats(),
                
                # ğŸ† é»„é‡‘æ¨¡æ¿ç³»ç»ŸçŠ¶æ€ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                'golden_template_system': mab_stats.get('golden_template_system', {}),
                
                # å‘åå…¼å®¹å­—æ®µ
                'recent_decisions': len(self.decision_history)
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
            return {
                'error': str(e),
                'total_rounds': self.total_rounds,
                'component_architecture': True
            }
    
    def get_decision_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        è·å–å†³ç­–å†å²è®°å½•
        
        Args:
            limit: è¿”å›è®°å½•æ•°é‡é™åˆ¶
            
        Returns:
            å†³ç­–å†å²è®°å½•åˆ—è¡¨
        """
        return self.decision_history[-limit:] if self.decision_history else []
    
    def _get_performance_optimization_status(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ä¼˜åŒ–å™¨çŠ¶æ€"""
        if not self.performance_optimizer:
            return {
                'enabled': False,
                'status': 'disabled',
                'message': 'æ€§èƒ½ä¼˜åŒ–å™¨æœªå¯ç”¨'
            }
        
        try:
            optimization_report = self.performance_optimizer.get_performance_report()
            
            return {
                'enabled': True,
                'status': 'active',
                'features': {
                    'parallel_verification': PERFORMANCE_CONFIG.get("enable_parallel_path_verification", False),
                    'intelligent_caching': PERFORMANCE_CONFIG.get("enable_intelligent_caching", False),
                    'adaptive_path_count': PERFORMANCE_CONFIG.get("enable_adaptive_path_count", False),
                    'early_termination': PERFORMANCE_CONFIG.get("enable_early_termination", False)
                },
                'cache_stats': optimization_report.get('cache_stats', {}),
                'performance_improvements': optimization_report.get('optimization_stats', {}),
                'config': {
                    'max_concurrent_verifications': PERFORMANCE_CONFIG.get("max_concurrent_verifications", 3),
                    'cache_ttl_seconds': PERFORMANCE_CONFIG.get("cache_ttl_seconds", 3600),
                    'path_consistency_threshold': PERFORMANCE_CONFIG.get("path_consistency_threshold", 0.8)
                },
                'uptime_hours': optimization_report.get('uptime_hours', 0)
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ€§èƒ½ä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {e}")
            return {
                'enabled': True,
                'status': 'error',
                'error': str(e)
            }
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†æ€§èƒ½æŠ¥å‘Š"""
        try:
            report = {
                'overview': {
                    'total_decisions': self.performance_stats['total_decisions'],
                    'successful_decisions': self.performance_stats['successful_decisions'],
                    'success_rate': 0.0,
                    'avg_decision_time': self.performance_stats['avg_decision_time']
                },
                'component_performance': self.performance_stats['component_performance'].copy(),
                'recent_trends': self._analyze_recent_trends(),
                'bottlenecks': self._identify_performance_bottlenecks(),
                'recommendations': self._generate_performance_recommendations()
            }
            
            # è®¡ç®—æˆåŠŸç‡
            if report['overview']['total_decisions'] > 0:
                report['overview']['success_rate'] = (
                    report['overview']['successful_decisions'] / report['overview']['total_decisions']
                )
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_recent_trends(self) -> Dict[str, Any]:
        """åˆ†ææœ€è¿‘çš„æ€§èƒ½è¶‹åŠ¿"""
        if len(self.decision_history) < 5:
            return {'status': 'insufficient_data'}
        
        recent_decisions = self.decision_history[-10:]
        
        # åˆ†ææˆåŠŸç‡è¶‹åŠ¿
        recent_successes = [
            1 if d.get('feedback', {}).get('execution_success', False) else 0 
            for d in recent_decisions if 'feedback' in d
        ]
        
        # åˆ†æå†³ç­–æ—¶é—´è¶‹åŠ¿
        recent_times = [
            d.get('performance_metrics', {}).get('total_time', 0) 
            for d in recent_decisions
        ]
        
        trends = {
            'success_rate_trend': 'stable',
            'decision_time_trend': 'stable',
            'recent_success_rate': 0.0,
            'recent_avg_time': 0.0
        }
        
        if recent_successes:
            trends['recent_success_rate'] = sum(recent_successes) / len(recent_successes)
            
        if recent_times:
            trends['recent_avg_time'] = sum(recent_times) / len(recent_times)
            
            # ç®€å•è¶‹åŠ¿åˆ†æ
            if len(recent_times) >= 5:
                first_half_avg = sum(recent_times[:len(recent_times)//2]) / (len(recent_times)//2)
                second_half_avg = sum(recent_times[len(recent_times)//2:]) / (len(recent_times) - len(recent_times)//2)
                
                if second_half_avg > first_half_avg * 1.2:
                    trends['decision_time_trend'] = 'increasing'
                elif second_half_avg < first_half_avg * 0.8:
                    trends['decision_time_trend'] = 'decreasing'
        
        return trends
    
    def _identify_performance_bottlenecks(self) -> List[str]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # åˆ†æç»„ä»¶æ€§èƒ½
        component_perfs = self.performance_stats['component_performance']
        
        # æ‰¾å‡ºæœ€æ…¢çš„ç»„ä»¶
        slowest_component = max(component_perfs.items(), key=lambda x: x[1]['avg_time'])
        if slowest_component[1]['avg_time'] > 2.0:  # è¶…è¿‡2ç§’
            bottlenecks.append(f"{slowest_component[0]}å“åº”æ—¶é—´è¿‡é•¿ ({slowest_component[1]['avg_time']:.2f}s)")
        
        # æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
        if hasattr(self.path_generator, 'generation_cache') and len(self.path_generator.generation_cache) > 20:
            bottlenecks.append("è·¯å¾„ç”Ÿæˆç¼“å­˜å¯èƒ½è¿‡å¤§ï¼Œå½±å“å†…å­˜ä½¿ç”¨")
        
        # æ£€æŸ¥å†³ç­–å†å²å¤§å°
        if len(self.decision_history) > SYSTEM_LIMITS["max_decision_history"] * 0.8:
            bottlenecks.append("å†³ç­–å†å²æ¥è¿‘ä¸Šé™ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        
        return bottlenecks
    
    def _generate_performance_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºæˆåŠŸç‡çš„å»ºè®®
        success_rate = 0.0
        if self.performance_stats['total_decisions'] > 0:
            success_rate = self.performance_stats['successful_decisions'] / self.performance_stats['total_decisions']
        
        if success_rate < 0.7:
            recommendations.append("æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ç»´åº¦ç”Ÿæˆç­–ç•¥å’ŒMABç®—æ³•å‚æ•°")
        
        # åŸºäºå†³ç­–æ—¶é—´çš„å»ºè®®
        if self.performance_stats['avg_decision_time'] > 3.0:
            recommendations.append("å¹³å‡å†³ç­–æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–APIè°ƒç”¨æˆ–å¢åŠ ç¼“å­˜")
        
        # åŸºäºç»„ä»¶æ€§èƒ½çš„å»ºè®®
        component_perfs = self.performance_stats['component_performance']
        for component_name, perf in component_perfs.items():
            if perf['avg_time'] > 1.5:
                recommendations.append(f"ä¼˜åŒ–{component_name}ç»„ä»¶æ€§èƒ½ï¼Œå½“å‰å¹³å‡è€—æ—¶{perf['avg_time']:.2f}ç§’")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œæ— ç‰¹åˆ«ä¼˜åŒ–å»ºè®®")
        
        return recommendations
    
    # ==================== ğŸ’¡ Aha-Momentå†³ç­–ç³»ç»Ÿå®ç° ====================
    
    def _check_aha_moment_trigger(self, chosen_path) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘Aha-Momentå†³ç­–ï¼ˆç»•é“æ€è€ƒï¼‰
        
        Args:
            chosen_path: MABé€‰æ‹©çš„æ€ç»´è·¯å¾„
            
        Returns:
            (æ˜¯å¦è§¦å‘, è§¦å‘åŸå› )
        """
        # è§¦å‘æ¡ä»¶1ï¼šè·¯å¾„ç½®ä¿¡åº¦è¿‡ä½
        if hasattr(chosen_path, 'strategy_id'):
            path_confidence = self.mab_converger.get_path_confidence(chosen_path.strategy_id)
            
            if path_confidence < self.aha_moment_stats['confidence_threshold']:
                return True, f"é€‰ä¸­è·¯å¾„ç½®ä¿¡åº¦è¿‡ä½ ({path_confidence:.3f} < {self.aha_moment_stats['confidence_threshold']})"
        
        # è§¦å‘æ¡ä»¶2ï¼šæ‰€æœ‰è·¯å¾„éƒ½è¡¨ç°å¾ˆå·®
        is_low_confidence_scenario = self.mab_converger.check_low_confidence_scenario(
            threshold=self.aha_moment_stats['confidence_threshold']
        )
        
        if is_low_confidence_scenario:
            return True, "æ‰€æœ‰å¯ç”¨è·¯å¾„çš„ç½®ä¿¡åº¦éƒ½åä½ï¼Œéœ€è¦åˆ›é€ æ€§çªç ´"
        
        # è§¦å‘æ¡ä»¶3ï¼šè¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤š
        if self.aha_moment_stats['consecutive_failures'] >= self.aha_moment_stats['failure_threshold']:
            return True, f"è¿ç»­å¤±è´¥ {self.aha_moment_stats['consecutive_failures']} æ¬¡ï¼Œè¶…è¿‡é˜ˆå€¼ {self.aha_moment_stats['failure_threshold']}"
        
        # è§¦å‘æ¡ä»¶4ï¼šç‰¹å®šæ—¶é—´é—´éš”å†…çš„å¤±è´¥å¯†åº¦è¿‡é«˜
        recent_failures = self._count_recent_failures(time_window=300)  # 5åˆ†é’Ÿå†…
        if recent_failures >= 3:
            return True, f"æœ€è¿‘5åˆ†é’Ÿå†…å¤±è´¥ {recent_failures} æ¬¡ï¼Œé¢‘ç‡è¿‡é«˜"
        
        return False, "å¸¸è§„å†³ç­–è·¯å¾„è¡¨ç°æ­£å¸¸"
    
    def _execute_aha_moment_thinking(self, user_query: str, thinking_seed: str, 
                                   original_path, original_paths) -> Tuple[Any, List[Any]]:
        """
        æ‰§è¡ŒAha-Momentç»•é“æ€è€ƒ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            thinking_seed: åŸå§‹æ€ç»´ç§å­
            original_path: åŸå§‹é€‰æ‹©çš„è·¯å¾„
            original_paths: åŸå§‹è·¯å¾„åˆ—è¡¨
            
        Returns:
            (æ–°é€‰æ‹©çš„è·¯å¾„, æ–°çš„è·¯å¾„åˆ—è¡¨)
        """
        aha_start_time = time.time()
        self.aha_moment_stats['total_aha_moments'] += 1
        
        logger.info("ğŸ’¡ å¼€å§‹æ‰§è¡ŒAha-Momentç»•é“æ€è€ƒ...")
        logger.info(f"   åŸå§‹è·¯å¾„: {original_path.path_type}")
        logger.info(f"   åŸå§‹è·¯å¾„æ•°é‡: {len(original_paths)}")
        
        try:
            # Step 1: ç”Ÿæˆåˆ›é€ æ€§ç»•é“è·¯å¾„
            logger.info("ğŸŒŸ ç”Ÿæˆåˆ›é€ æ€§æ€ç»´è·¯å¾„...")
            creative_paths = self.path_generator.generate_paths(
                thinking_seed=thinking_seed,
                task=user_query,
                max_paths=4,  # å‡å°‘æ•°é‡ï¼Œä¸“æ³¨è´¨é‡
                mode='creative_bypass'  # åˆ›é€ æ€§ç»•é“æ¨¡å¼
            )
            
            logger.info(f"ğŸŒŸ ç”Ÿæˆäº† {len(creative_paths)} æ¡åˆ›é€ æ€§è·¯å¾„")
            for i, path in enumerate(creative_paths, 1):
                logger.info(f"   åˆ›é€ æ€§è·¯å¾„{i}: {path.path_type}")
            
            # Step 2: åˆå¹¶åŸå§‹è·¯å¾„å’Œåˆ›é€ æ€§è·¯å¾„
            combined_paths = original_paths + creative_paths
            
            # Step 3: ä½¿ç”¨MABé‡æ–°é€‰æ‹©ï¼ˆç°åœ¨æœ‰æ›´å¤šé€‰æ‹©ï¼‰
            logger.info("ğŸ¯ åœ¨æ‰©å±•è·¯å¾„é›†åˆä¸­é‡æ–°é€‰æ‹©æœ€ä¼˜è·¯å¾„...")
            final_chosen_path = self.mab_converger.select_best_path(combined_paths)
            
            # Step 4: è®°å½•Aha-Momentå†³ç­–å†å²
            aha_record = {
                'timestamp': time.time(),
                'trigger_reason': 'low_confidence_scenario',
                'original_path': original_path.path_type,
                'creative_paths_generated': len(creative_paths),
                'final_chosen_path': final_chosen_path.path_type,
                'was_creative_path_chosen': final_chosen_path in creative_paths,
                'aha_thinking_time': time.time() - aha_start_time
            }
            
            self.aha_moment_stats['aha_decision_history'].append(aha_record)
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦
            if len(self.aha_moment_stats['aha_decision_history']) > 100:
                self.aha_moment_stats['aha_decision_history'] = self.aha_moment_stats['aha_decision_history'][-50:]
            
            logger.info(f"ğŸ’¡ Aha-Momentæ€è€ƒå®Œæˆ:")
            logger.info(f"   æœ€ç»ˆè·¯å¾„: {final_chosen_path.path_type}")
            logger.info(f"   æ˜¯å¦é€‰æ‹©åˆ›é€ æ€§è·¯å¾„: {'æ˜¯' if final_chosen_path in creative_paths else 'å¦'}")
            logger.info(f"   ç»•é“æ€è€ƒè€—æ—¶: {time.time() - aha_start_time:.3f}s")
            
            return final_chosen_path, combined_paths
            
        except Exception as e:
            logger.error(f"âŒ Aha-Momentæ€è€ƒè¿‡ç¨‹å¤±è´¥: {e}")
            logger.warning("ğŸ”„ å›é€€åˆ°åŸå§‹è·¯å¾„é€‰æ‹©")
            return original_path, original_paths
    
    def _count_recent_failures(self, time_window: int = 300) -> int:
        """
        ç»Ÿè®¡æœ€è¿‘æ—¶é—´çª—å£å†…çš„å¤±è´¥æ¬¡æ•°
        
        Args:
            time_window: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
            
        Returns:
            å¤±è´¥æ¬¡æ•°
        """
        if not self.decision_history:
            return 0
        
        current_time = time.time()
        failure_count = 0
        
        for decision in reversed(self.decision_history):
            if current_time - decision.get('timestamp', 0) > time_window:
                break  # è¶…å‡ºæ—¶é—´çª—å£
            
            # æ£€æŸ¥è¿™ä¸ªå†³ç­–æ˜¯å¦è¢«æ ‡è®°ä¸ºå¤±è´¥
            # è¿™éœ€è¦åœ¨update_performance_feedbackä¸­è®¾ç½®
            if decision.get('execution_success', True) is False:
                failure_count += 1
        
        return failure_count
    
    def update_aha_moment_feedback(self, success: bool):
        """
        æ›´æ–°Aha-Momentå†³ç­–çš„åé¦ˆ
        
        Args:
            success: å†³ç­–æ˜¯å¦æˆåŠŸ
        """
        if success:
            self.aha_moment_stats['consecutive_failures'] = 0
            self.aha_moment_stats['last_decision_success'] = True
            logger.debug("âœ… å†³ç­–æˆåŠŸï¼Œé‡ç½®è¿ç»­å¤±è´¥è®¡æ•°")
        else:
            self.aha_moment_stats['consecutive_failures'] += 1
            self.aha_moment_stats['last_decision_success'] = False
            self.aha_moment_stats['last_failure_timestamp'] = time.time()
            logger.debug(f"âŒ å†³ç­–å¤±è´¥ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°: {self.aha_moment_stats['consecutive_failures']}")
        
        # æ›´æ–°Aha-MomentæˆåŠŸç‡ç»Ÿè®¡
        if self.aha_moment_stats['aha_decision_history']:
            aha_successes = sum(1 for record in self.aha_moment_stats['aha_decision_history'] 
                              if record.get('success', False))
            total_aha = len(self.aha_moment_stats['aha_decision_history'])
            self.aha_moment_stats['aha_success_rate'] = aha_successes / total_aha
    
    def get_aha_moment_stats(self) -> Dict[str, any]:
        """
        è·å–Aha-Momentå†³ç­–ç³»ç»Ÿçš„ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        recent_aha_count = sum(1 for record in self.aha_moment_stats['aha_decision_history']
                              if time.time() - record['timestamp'] < 3600)  # æœ€è¿‘1å°æ—¶
        
        return {
            'total_aha_moments': self.aha_moment_stats['total_aha_moments'],
            'consecutive_failures': self.aha_moment_stats['consecutive_failures'],
            'aha_success_rate': self.aha_moment_stats['aha_success_rate'],
            'recent_aha_count': recent_aha_count,
            'failure_threshold': self.aha_moment_stats['failure_threshold'],
            'confidence_threshold': self.aha_moment_stats['confidence_threshold'],
            'last_failure_timestamp': self.aha_moment_stats['last_failure_timestamp'],
            'last_decision_success': self.aha_moment_stats['last_decision_success'],
            'history_count': len(self.aha_moment_stats['aha_decision_history'])
        }
    
    def reset_system(self, preserve_learnings: bool = True):
        """
        é‡ç½®ç³»ç»ŸçŠ¶æ€
        
        Args:
            preserve_learnings: æ˜¯å¦ä¿ç•™å­¦ä¹ åˆ°çš„çŸ¥è¯†
        """
        logger.info("ğŸ”„ å¼€å§‹é‡ç½®ç³»ç»Ÿ...")
        
        # é‡ç½®è®¡æ•°å™¨
        self.total_rounds = 0
        self.decision_history.clear()
        
        # é‡ç½®æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_decisions': 0,
            'successful_decisions': 0,
            'avg_decision_time': 0.0,
            'component_performance': {
                'prior_reasoner': {'calls': 0, 'avg_time': 0.0},
                'path_generator': {'calls': 0, 'avg_time': 0.0},
                'mab_converger': {'calls': 0, 'avg_time': 0.0},
                'idea_verification': {'calls': 0, 'avg_time': 0.0, 'success_rate': 0.0}  # åŒ…å«æ–°çš„éªŒè¯ç»Ÿè®¡
            }
        }
        
        if not preserve_learnings:
            # æ¸…é™¤æ‰€æœ‰å­¦ä¹ æ•°æ®
            self.prior_reasoner.reset_cache()
            self.path_generator.clear_cache()
            # MABæ•°æ®ä¸æ¸…é™¤ï¼Œå› ä¸ºå®ƒæ˜¯æ ¸å¿ƒå­¦ä¹ æœºåˆ¶
            logger.info("ğŸ§¹ å·²æ¸…é™¤ç¼“å­˜å’Œä¸´æ—¶æ•°æ®")
        
        logger.info("âœ… ç³»ç»Ÿé‡ç½®å®Œæˆ")
        
        # ğŸ—‘ï¸ å·²ç§»é™¤ï¼šä¸å†éœ€è¦æ¸…ç†éªŒè¯å®¢æˆ·ç«¯ç¼“å­˜ï¼Œå·¥å…·ç¼“å­˜ç”±ToolRegistryç®¡ç†
    
    # ================================
    # ğŸ”¬ æ–°å¢ï¼šæƒ³æ³•éªŒè¯ç ”ç©¶å‘˜èƒ½åŠ›
    # ================================
    
    def verify_idea_feasibility(self, idea_text: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ğŸ”§ å·¥å…·å¢å¼ºçš„æƒ³æ³•éªŒè¯æµç¨‹ - ä»ç¡¬ç¼–ç æœç´¢å‡çº§ä¸ºçµæ´»å·¥å…·è°ƒç”¨
        
        åŸæœ‰æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªç¡¬ç¼–ç çš„"æœç´¢å·¥å…·"è°ƒç”¨ã€‚ç°åœ¨å‡çº§ä¸ºï¼š
        1. æ™ºèƒ½å·¥å…·é€‰æ‹©ï¼šæ ¹æ®éªŒè¯éœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·
        2. å·¥å…·å¢å¼ºæ¨ç†ï¼šLLMå¯ä»¥è°ƒç”¨å¤šä¸ªå·¥å…·è·å–ä¿¡æ¯
        3. åŠ¨æ€éªŒè¯ç­–ç•¥ï¼šæ ¹æ®æƒ³æ³•ç±»å‹é‡‡ç”¨ä¸åŒéªŒè¯æ–¹æ³•
        4. å­¦ä¹ åé¦ˆæœºåˆ¶ï¼šå·¥å…·ä½¿ç”¨ç»“æœå½±å“MABå­¦ä¹ 
        
        Args:
            idea_text: éœ€è¦éªŒè¯çš„æƒ³æ³•æ–‡æœ¬ï¼ˆæ€ç»´ç§å­æˆ–æ€ç»´è·¯å¾„æè¿°ï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            å¢å¼ºçš„éªŒè¯ç»“æœå­—å…¸
        """
        start_time = time.time()
        logger.info(f"ğŸ”¬ å¼€å§‹æƒ³æ³•éªŒè¯ç ”ç©¶: {idea_text[:50]}...")
        
        try:
            # ğŸ”§ ä½¿ç”¨å·¥å…·å¢å¼ºçš„éªŒè¯æµç¨‹ï¼ˆæ— å›é€€æœºåˆ¶ï¼‰
            verification_result = self._enhanced_verification_with_tools(idea_text, context)
            
            # å¦‚æœå·¥å…·å¢å¼ºéªŒè¯å¤±è´¥ï¼Œç›´æ¥è¿”å›å¤±è´¥ç»“æœ
            if not verification_result.get('success', False):
                logger.error("âŒ å·¥å…·å¢å¼ºéªŒè¯å¤±è´¥ï¼Œæ— å›é€€æœºåˆ¶")
                execution_time = time.time() - start_time
                return self._create_direct_failure_result(
                    idea_text, 
                    verification_result.get('error_message', 'å·¥å…·å¢å¼ºéªŒè¯å¤±è´¥'), 
                    execution_time
                )
            
            # ç»Ÿè®¡å’Œå­¦ä¹ åé¦ˆ
            execution_time = time.time() - start_time
            self._update_verification_stats(verification_result, execution_time)
            
            logger.info(f"âœ… æƒ³æ³•éªŒè¯å®Œæˆ: å¯è¡Œæ€§={verification_result.get('feasibility_analysis', {}).get('feasibility_score', 0):.2f}")
            return verification_result
            
        except Exception as e:
            logger.error(f"âŒ æƒ³æ³•éªŒè¯å¼‚å¸¸: {e}")
            execution_time = time.time() - start_time
            return self._create_direct_failure_result(
                idea_text, f"éªŒè¯å¼‚å¸¸: {e}", execution_time
            )
    
    def _enhanced_verification_with_tools(self, idea_text: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ğŸ”§ å·¥å…·å¢å¼ºçš„éªŒè¯æ–¹æ³• - æ ¸å¿ƒå‡çº§
        
        è¿™ä¸ªæ–¹æ³•å±•ç¤ºäº†å¦‚ä½•å°†åŸæœ‰çš„ç¡¬ç¼–ç æœç´¢æ”¹é€ ä¸ºçµæ´»çš„å·¥å…·è°ƒç”¨ï¼š
        1. æ™ºèƒ½å·¥å…·é€‰æ‹©ï¼šæ ¹æ®æƒ³æ³•ç±»å‹é€‰æ‹©æœ€åˆé€‚çš„éªŒè¯å·¥å…·
        2. å¤šæ­¥éª¤éªŒè¯ï¼šå¯ä»¥è¿ç»­ä½¿ç”¨å¤šä¸ªå·¥å…·
        3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼šæ ¹æ®éªŒè¯é˜¶æ®µè°ƒæ•´å·¥å…·ä½¿ç”¨ç­–ç•¥
        """
        try:
            # æ„å»ºéªŒè¯æç¤ºè¯
            verification_prompt = self._build_verification_prompt(idea_text, context)
            
            # é€‰æ‹©é€‚åˆéªŒè¯çš„å·¥å…·
            available_tools = self._select_verification_tools(idea_text, context)
            
            # ä½¿ç”¨å·¥å…·å¢å¼ºçš„LLMæ¨ç†
            llm_result = self._execute_llm_with_tools(
                prompt=verification_prompt,
                context=context,
                available_tools=available_tools,
                max_tool_calls=2  # éªŒè¯é˜¶æ®µé™åˆ¶å·¥å…·è°ƒç”¨æ¬¡æ•°
            )
            
            if not llm_result['success']:
                return {'success': False, 'error_message': llm_result['error_message']}
            
            # è§£æLLMçš„éªŒè¯ç»“æœ
            verification_analysis = self._parse_verification_response(llm_result['llm_response'])
            
            # è®¡ç®—å¥–åŠ±åˆ†æ•°ï¼ˆè€ƒè™‘å·¥å…·ä½¿ç”¨æ•ˆæœï¼‰
            reward_score = self._calculate_enhanced_reward(verification_analysis, llm_result['tool_calls'])
            
            return {
                'success': True,
                'idea_text': idea_text,
                'feasibility_analysis': verification_analysis,
                'reward_score': reward_score,
                'tool_calls_made': len(llm_result['tool_calls']),
                'tool_results': llm_result['tool_results'],
                'execution_time': llm_result['execution_time'],
                'verification_method': 'tool_enhanced'
            }
            
        except Exception as e:
            logger.error(f"âŒ å·¥å…·å¢å¼ºéªŒè¯å¤±è´¥: {e}")
            return {'success': False, 'error_message': str(e)}
    
    def _build_verification_prompt(self, idea_text: str, context: Optional[Dict] = None) -> str:
        """æ„å»ºéªŒè¯æç¤ºè¯"""
        stage = context.get('stage', 'unknown') if context else 'unknown'
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹æƒ³æ³•çš„å¯è¡Œæ€§ï¼Œå¹¶æä¾›è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Šã€‚ä½ å¯ä»¥ä½¿ç”¨æœç´¢å·¥å…·æ¥è·å–ç›¸å…³ä¿¡æ¯ï¼š

æƒ³æ³•å†…å®¹ï¼š{idea_text}

åˆ†æé˜¶æ®µï¼š{stage}
"""
        
        if context:
            if 'query' in context:
                prompt += f"\nåŸå§‹æŸ¥è¯¢ï¼š{context['query']}"
            if 'domain' in context:
                prompt += f"\nåº”ç”¨é¢†åŸŸï¼š{context['domain']}"
        
        prompt += """

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æï¼š
1. æŠ€æœ¯å¯è¡Œæ€§ - ä»æŠ€æœ¯è§’åº¦è¯„ä¼°å®ç°éš¾åº¦
2. å¸‚åœºéœ€æ±‚ - åˆ†ææ˜¯å¦æœ‰å®é™…éœ€æ±‚
3. èµ„æºè¦æ±‚ - è¯„ä¼°æ‰€éœ€èµ„æºå’Œæˆæœ¬
4. é£é™©è¯„ä¼° - è¯†åˆ«æ½œåœ¨é£é™©å’ŒæŒ‘æˆ˜
5. åˆ›æ–°ç¨‹åº¦ - è¯„ä¼°æƒ³æ³•çš„æ–°é¢–æ€§

å¦‚æœéœ€è¦è·å–æœ€æ–°ä¿¡æ¯ï¼Œè¯·ä½¿ç”¨æœç´¢å·¥å…·ã€‚

æœ€åï¼Œè¯·ç»™å‡ºä¸€ä¸ª0-1ä¹‹é—´çš„å¯è¡Œæ€§è¯„åˆ†ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚
"""
        
        return prompt
    
    def _select_verification_tools(self, idea_text: str, context: Optional[Dict] = None) -> List[str]:
        """æ ¹æ®éªŒè¯éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·"""
        available_tools = ["web_search"]  # åŸºç¡€æœç´¢å·¥å…·
        
        # æ ¹æ®æƒ³æ³•ç±»å‹å’Œä¸Šä¸‹æ–‡é€‰æ‹©é¢å¤–å·¥å…·
        if context:
            stage = context.get('stage', '')
            if stage == 'thinking_seed':
                # æ€ç»´ç§å­é˜¶æ®µï¼šéœ€è¦å¹¿æ³›çš„ä¿¡æ¯æ”¶é›†
                available_tools.extend(["idea_verification"])
            elif stage == 'reasoning_path':
                # æ¨ç†è·¯å¾„é˜¶æ®µï¼šéœ€è¦æ·±åº¦éªŒè¯
                available_tools.extend(["idea_verification"])
        
        # æ ¹æ®æƒ³æ³•å†…å®¹æ¨æ–­éœ€è¦çš„å·¥å…·
        idea_lower = idea_text.lower()
        if any(keyword in idea_lower for keyword in ['æŠ€æœ¯', 'technology', 'ç¼–ç¨‹', 'programming']):
            # æŠ€æœ¯ç±»æƒ³æ³•å¯èƒ½éœ€è¦æ›´å¤šæŠ€æœ¯èµ„æº
            pass  # æœªæ¥å¯ä»¥æ·»åŠ æŠ€æœ¯ç±»éªŒè¯å·¥å…·
        
        return available_tools
    
    def _parse_verification_response(self, llm_response: str) -> Dict[str, Any]:
        """è§£æLLMçš„éªŒè¯å“åº”"""
        # å°è¯•ä»å“åº”ä¸­æå–å¯è¡Œæ€§è¯„åˆ†
        import re
        
        # æŸ¥æ‰¾è¯„åˆ†
        score_patterns = [
            r'å¯è¡Œæ€§è¯„åˆ†[ï¼š:]\s*([0-9]*\.?[0-9]+)',
            r'è¯„åˆ†[ï¼š:]\s*([0-9]*\.?[0-9]+)',
            r'score[ï¼š:]?\s*([0-9]*\.?[0-9]+)',
            r'([0-9]*\.?[0-9]+)\s*/\s*1',
            r'([0-9]*\.?[0-9]+)\s*åˆ†'
        ]
        
        feasibility_score = 0.5  # é»˜è®¤å€¼
        for pattern in score_patterns:
            match = re.search(pattern, llm_response, re.IGNORECASE)
            if match:
                try:
                    score = float(match.group(1))
                    if score > 1:  # å¯èƒ½æ˜¯ç™¾åˆ†åˆ¶
                        score = score / 100
                    feasibility_score = max(0, min(1, score))  # é™åˆ¶åœ¨0-1èŒƒå›´
                    break
                except ValueError:
                    continue
        
        # æå–å…³é”®åˆ†æè¦ç‚¹
        analysis_summary = llm_response[:500] + "..." if len(llm_response) > 500 else llm_response
        
        return {
            'feasibility_score': feasibility_score,
            'analysis_summary': analysis_summary,
            'full_response': llm_response
        }
    
    def _calculate_enhanced_reward(self, verification_analysis: Dict[str, Any], tool_calls: List[Dict]) -> float:
        """è®¡ç®—å¢å¼ºçš„å¥–åŠ±åˆ†æ•°ï¼ˆè€ƒè™‘å·¥å…·ä½¿ç”¨æ•ˆæœï¼‰"""
        base_score = verification_analysis.get('feasibility_score', 0.5)
        
        # å·¥å…·ä½¿ç”¨å¥–åŠ±
        tool_bonus = 0.0
        if tool_calls:
            # æˆåŠŸä½¿ç”¨å·¥å…·è·å¾—å°å¹…å¥–åŠ±
            tool_bonus = min(0.1, len(tool_calls) * 0.05)
        
        # æœ€ç»ˆå¥–åŠ±åˆ†æ•°
        reward = base_score + tool_bonus - 0.5  # è½¬æ¢ä¸º[-0.5, 0.6]èŒƒå›´
        
        return reward
    

    def _update_verification_stats(self, verification_result: Dict[str, Any], execution_time: float):
        """æ›´æ–°éªŒè¯ç»Ÿè®¡ä¿¡æ¯"""
        success = verification_result.get('success', False)
        
        # æ›´æ–°ç»„ä»¶æ€§èƒ½ç»Ÿè®¡
        current_stats = self.performance_stats['component_performance']['idea_verification']
        current_stats['calls'] += 1
        
        # æ›´æ–°å¹³å‡æ—¶é—´
        if current_stats['calls'] == 1:
            current_stats['avg_time'] = execution_time
        else:
            current_stats['avg_time'] = (current_stats['avg_time'] * (current_stats['calls'] - 1) + execution_time) / current_stats['calls']
        
        # æ›´æ–°æˆåŠŸç‡
        if 'total_success' not in current_stats:
            current_stats['total_success'] = 0
        
        if success:
            current_stats['total_success'] += 1
        
        current_stats['success_rate'] = current_stats['total_success'] / current_stats['calls']
        
        # è®°å½•å·¥å…·ä½¿ç”¨ç»Ÿè®¡
        verification_method = verification_result.get('verification_method', 'unknown')
        tool_calls_made = verification_result.get('tool_calls_made', 0)
        
        if 'verification_methods' not in current_stats:
            current_stats['verification_methods'] = {}
        
        if verification_method not in current_stats['verification_methods']:
            current_stats['verification_methods'][verification_method] = 0
        current_stats['verification_methods'][verification_method] += 1
        
        if 'tool_usage' not in current_stats:
            current_stats['tool_usage'] = {'total_calls': 0, 'calls_per_verification': 0}
        
        current_stats['tool_usage']['total_calls'] += tool_calls_made
        current_stats['tool_usage']['calls_per_verification'] = current_stats['tool_usage']['total_calls'] / current_stats['calls']
        
        logger.debug(f"ğŸ“Š éªŒè¯ç»Ÿè®¡æ›´æ–°: æˆåŠŸç‡={current_stats['success_rate']:.1%}, æ–¹æ³•={verification_method}, å·¥å…·è°ƒç”¨={tool_calls_made}")
    
    def _create_direct_failure_result(self, idea_text: str, error_message: str, execution_time: float = 0.0) -> Dict[str, Any]:
        """åˆ›å»ºç›´æ¥å¤±è´¥ç»“æœï¼ˆæ— å›é€€æœºåˆ¶ï¼‰"""
        return {
            'success': False,
            'idea_text': idea_text,
            'feasibility_analysis': {
                'feasibility_score': 0.0,  # å¤±è´¥æ—¶ç»™äºˆæœ€ä½è¯„åˆ†
                'analysis_summary': f"å·¥å…·å¢å¼ºéªŒè¯å¤±è´¥: {error_message}ã€‚ç³»ç»Ÿå°†ä¾èµ–MABå­¦ä¹ é¿å…æ­¤ç±»è·¯å¾„ã€‚"
            },
            'reward_score': -0.5,  # å¼ºè´Ÿå¥–åŠ±ï¼Œè®©MABç³»ç»Ÿå­¦ä¼šé¿å…å¯¼è‡´å¤±è´¥çš„è·¯å¾„
            'error_message': error_message,
            'execution_time': execution_time,
            'verification_method': 'tool_enhanced_failed',
            'tool_calls_made': 0,
            'tool_results': {}
        }
    
    def _preprocess_idea_text(self, idea_text: str) -> str:
        """é¢„å¤„ç†æƒ³æ³•æ–‡æœ¬"""
        # æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬
        cleaned = idea_text.strip()
        
        # é™åˆ¶é•¿åº¦ï¼ˆé¿å…è¿‡é•¿çš„æœç´¢æŸ¥è¯¢ï¼‰
        if len(cleaned) > 200:
            cleaned = cleaned[:200] + "..."
            
        return cleaned
    
    def _generate_verification_query(self, idea_text: str, context: Optional[Dict] = None) -> str:
        """
        ç”ŸæˆéªŒè¯æŸ¥è¯¢ - å°†æƒ³æ³•è½¬æ¢æˆé€‚åˆæœç´¢çš„é—®é¢˜
        
        Args:
            idea_text: æƒ³æ³•æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡
            
        Returns:
            æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # æå–æ ¸å¿ƒæŠ€æœ¯æ¦‚å¿µ
        tech_concepts = self._extract_technical_concepts(idea_text)
        
        # æ„å»ºæŸ¥è¯¢æ¨¡æ¿
        if tech_concepts:
            query_template = f"'{' '.join(tech_concepts[:2])}' æŠ€æœ¯å¯è¡Œæ€§ å®ç°æ–¹æ³• æ½œåœ¨é£é™© æœ€ä½³å®è·µ"
        else:
            query_template = f"'{idea_text[:50]}' è§£å†³æ–¹æ¡ˆ æŠ€æœ¯å®ç° æŒ‘æˆ˜åˆ†æ"
        
        # æ·»åŠ é¢†åŸŸä¸Šä¸‹æ–‡
        if context and 'domain' in context:
            query_template += f" {context['domain']}"
        
        return query_template
    
    def _extract_technical_concepts(self, text: str) -> List[str]:
        """æå–æŠ€æœ¯æ¦‚å¿µ"""
        tech_terms = [
            'API', 'api', 'ç®—æ³•', 'æ•°æ®åº“', 'ç³»ç»Ÿ', 'æ¶æ„', 'ä¼˜åŒ–',
            'æœºå™¨å­¦ä¹ ', 'ML', 'AI', 'äººå·¥æ™ºèƒ½', 'æ·±åº¦å­¦ä¹ ',
            'ç½‘ç»œ', 'çˆ¬è™«', 'æ•°æ®åˆ†æ', 'å®æ—¶', 'æ€§èƒ½', 'å®‰å…¨',
            'å¹¶å‘', 'åˆ†å¸ƒå¼', 'å¾®æœåŠ¡', 'å®¹å™¨', 'äº‘è®¡ç®—', 'åŒºå—é“¾',
            'Python', 'Java', 'JavaScript', 'React', 'Node.js'
        ]
        
        found_concepts = []
        text_lower = text.lower()
        
        for term in tech_terms:
            if term.lower() in text_lower:
                found_concepts.append(term)
        
        return found_concepts[:3]  # è¿”å›å‰3ä¸ªæ¦‚å¿µ
    
    # ğŸ—‘ï¸ å·²ç§»é™¤ _analyze_idea_feasibility - ä¼ ç»ŸéªŒè¯ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†
    
    # ğŸ—‘ï¸ å·²ç§»é™¤ _build_feasibility_analysis_prompt - ä¼ ç»ŸéªŒè¯ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†
    
    # ğŸ—‘ï¸ å·²ç§»é™¤ _parse_feasibility_analysis - ä¼ ç»ŸéªŒè¯ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†
    
    # ğŸ—‘ï¸ å·²ç§»é™¤ _create_default_analysis_result - ä¼ ç»ŸéªŒè¯ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†
    
    # ğŸ—‘ï¸ å·²ç§»é™¤ _heuristic_feasibility_analysis - ä¼ ç»ŸéªŒè¯ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†
    
    # ğŸ—‘ï¸ å·²ç§»é™¤ _calculate_verification_reward - ä¼ ç»ŸéªŒè¯ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†
    

    def _update_verification_performance(self, verification_result: Dict[str, Any]):
        """æ›´æ–°éªŒè¯æ€§èƒ½ç»Ÿè®¡"""
        perf_stats = self.performance_stats['component_performance']['idea_verification']
        
        # æ›´æ–°è°ƒç”¨æ¬¡æ•°
        perf_stats['calls'] += 1
        
        # æ›´æ–°å¹³å‡æ—¶é—´
        if perf_stats['calls'] == 1:
            perf_stats['avg_time'] = verification_result['verification_time']
        else:
            perf_stats['avg_time'] = (
                (perf_stats['avg_time'] * (perf_stats['calls'] - 1) + verification_result['verification_time'])
                / perf_stats['calls']
            )
        
        # æ›´æ–°æˆåŠŸç‡
        success = not verification_result.get('fallback', False)
        if perf_stats['calls'] == 1:
            perf_stats['success_rate'] = 1.0 if success else 0.0
        else:
            current_success_count = perf_stats['success_rate'] * (perf_stats['calls'] - 1)
            if success:
                current_success_count += 1
            perf_stats['success_rate'] = current_success_count / perf_stats['calls']
    
    # ================================
    # ğŸš€ æ™ºèƒ½ç»•é“æ€è€ƒç³»ç»Ÿï¼ˆå…¨æ–°Aha-Momentï¼‰
    # ================================
    
    def _execute_intelligent_detour_thinking(self, user_query: str, thinking_seed: str, 
                                           original_paths: List, verified_paths: List) -> Tuple[Any, Dict]:
        """
        æ™ºèƒ½ç»•é“æ€è€ƒ - å½“æ‰€æœ‰å¸¸è§„è·¯å¾„éƒ½è¢«éªŒè¯ä¸ºä¸å¯è¡Œæ—¶çš„åˆ›æ–°å†³ç­–
        
        è¿™æ˜¯åŸºäºéªŒè¯ç»“æœçš„æ–°å‹Aha-Momentè§¦å‘å™¨ï¼Œæ¯”ä¼ ç»Ÿæ–¹æ³•æ›´æ™ºèƒ½
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            thinking_seed: æ€ç»´ç§å­
            original_paths: åŸå§‹è·¯å¾„åˆ—è¡¨
            verified_paths: éªŒè¯ç»“æœåˆ—è¡¨
            
        Returns:
            Tuple[chosen_path, mab_decision]
        """
        logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½ç»•é“æ€è€ƒ - å¯»æ‰¾åˆ›æ–°è§£å†³æ–¹æ¡ˆ")
        
        try:
            # åˆ†æå¤±è´¥æ¨¡å¼ - ä»éªŒè¯ç»“æœä¸­å­¦ä¹ 
            failure_patterns = self._analyze_verification_failures(verified_paths)
            logger.debug(f"ğŸ“Š å¤±è´¥æ¨¡å¼åˆ†æ: {failure_patterns}")
            
            # åŸºäºå¤±è´¥åˆ†æç”Ÿæˆåˆ›æ–°ç§å­
            innovative_seed = self._generate_innovative_thinking_seed(
                user_query, thinking_seed, failure_patterns
            )
            
            # ç”Ÿæˆåˆ›æ–°è·¯å¾„
            if innovative_seed:
                logger.info("ğŸ’¡ åŸºäºå¤±è´¥åˆ†æç”Ÿæˆåˆ›æ–°æ€ç»´ç§å­")
                innovative_paths = self.path_generator.generate_paths(
                    thinking_seed=innovative_seed,
                    task=user_query,
                    max_paths=3  # ä¸“æ³¨äºå°‘æ•°åˆ›æ–°è·¯å¾„
                )
            else:
                innovative_paths = []
            
            # å¦‚æœåˆ›æ–°è·¯å¾„ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åº”æ€¥åˆ›æ–°è·¯å¾„
            if not innovative_paths:
                logger.warning("âš ï¸ åˆ›æ–°è·¯å¾„ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åº”æ€¥åˆ›æ–°è·¯å¾„")
                innovative_paths = self._create_emergency_innovative_paths(user_query, failure_patterns)
            
            # éªŒè¯åˆ›æ–°è·¯å¾„
            best_innovative_path = None
            best_feasibility = 0.0
            
            for path in innovative_paths:
                path_verification = self.verify_idea_feasibility(
                    idea_text=f"åˆ›æ–°æ–¹æ¡ˆ: {path.path_type}: {path.description}",
                    context={
                        'stage': 'innovative_detour',
                        'original_failure': True,
                        'query': user_query
                    }
                )
                
                feasibility = path_verification.get('feasibility_analysis', {}).get('feasibility_score', 0.0)
                
                if feasibility > best_feasibility:
                    best_feasibility = feasibility
                    best_innovative_path = path
                
                # æ›´æ–°MABå­¦ä¹ ï¼ˆå³ä½¿æ˜¯åˆ›æ–°è·¯å¾„ä¹Ÿè¦å­¦ä¹ ï¼‰
                self.mab_converger.update_path_performance(
                    path_id=path.path_id,
                    success=feasibility > 0.4,  # åˆ›æ–°è·¯å¾„çš„æˆåŠŸé˜ˆå€¼ç¨ä½
                    reward=path_verification.get('reward_score', 0.0)
                )
            
            # é€‰æ‹©æœ€ä½³åˆ›æ–°è·¯å¾„
            if best_innovative_path and best_feasibility > 0.2:
                chosen_path = best_innovative_path
                logger.info(f"âœ… é€‰æ‹©åˆ›æ–°è·¯å¾„: {chosen_path.path_type} (å¯è¡Œæ€§: {best_feasibility:.2f})")
            else:
                # æœ€åçš„åº”æ€¥æ–¹æ¡ˆ
                logger.warning("ğŸ†˜ æ‰€æœ‰åˆ›æ–°å°è¯•å¤±è´¥ï¼Œä½¿ç”¨ä¿å®ˆåº”æ€¥æ–¹æ¡ˆ")
                chosen_path = self._create_conservative_emergency_path(user_query)
            
            # æ„å»ºå†³ç­–ç»“æœ
            mab_decision = {
                'chosen_path': chosen_path,
                'available_paths': original_paths + innovative_paths,
                'innovative_paths': innovative_paths,
                'failure_patterns': failure_patterns,
                'innovative_seed': innovative_seed if 'innovative_seed' in locals() else None,
                'best_innovative_feasibility': best_feasibility,
                'detour_success': best_feasibility > 0.2
            }
            
            # è®°å½•æ™ºèƒ½ç»•é“ç»Ÿè®¡
            self.aha_moment_stats['total_aha_moments'] += 1
            self.aha_moment_stats['aha_decision_history'].append({
                'timestamp': time.time(),
                'type': 'intelligent_detour',
                'trigger_reason': 'all_paths_verification_failed',
                'innovative_paths_count': len(innovative_paths),
                'best_feasibility': best_feasibility,
                'success': best_feasibility > 0.2
            })
            
            return chosen_path, mab_decision
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½ç»•é“æ€è€ƒå¤±è´¥: {e}")
            # è¿”å›æœ€ä¿å®ˆçš„åº”æ€¥æ–¹æ¡ˆ
            emergency_path = self._create_conservative_emergency_path(user_query)
            emergency_decision = {
                'chosen_path': emergency_path,
                'available_paths': original_paths,
                'detour_error': str(e),
                'emergency_fallback': True
            }
            return emergency_path, emergency_decision
    
    def _analyze_verification_failures(self, verified_paths: List[Dict]) -> Dict[str, Any]:
        """åˆ†æéªŒè¯å¤±è´¥çš„æ¨¡å¼"""
        if not verified_paths:
            return {'error': 'no_paths_to_analyze'}
        
        # æ”¶é›†å¤±è´¥åŸå› 
        low_feasibility_paths = [vp for vp in verified_paths if vp['feasibility_score'] < 0.3]
        common_issues = []
        
        # åˆ†æå…±åŒçš„å¤±è´¥æ¨¡å¼
        for vp in low_feasibility_paths:
            verification_result = vp.get('verification_result', {})
            risk_analysis = verification_result.get('feasibility_analysis', {}).get('risk_analysis', {})
            key_risks = risk_analysis.get('key_risks', [])
            common_issues.extend(key_risks)
        
        # ç»Ÿè®¡æœ€å¸¸è§çš„é—®é¢˜
        from collections import Counter
        issue_counts = Counter(common_issues)
        
        return {
            'total_failed_paths': len(low_feasibility_paths),
            'average_feasibility': sum(vp['feasibility_score'] for vp in verified_paths) / len(verified_paths),
            'common_failure_reasons': dict(issue_counts.most_common(3)),
            'risk_patterns': [issue for issue, count in issue_counts.most_common(3)]
        }
    
    def _generate_innovative_thinking_seed(self, user_query: str, original_seed: str, 
                                         failure_patterns: Dict) -> str:
        """åŸºäºå¤±è´¥åˆ†æç”Ÿæˆåˆ›æ–°æ€ç»´ç§å­"""
        if not self.llm_client:
            return self._heuristic_innovative_seed(user_query, failure_patterns)
        
        try:
            innovation_prompt = f"""
åŸºäºå¤±è´¥åˆ†æï¼Œé‡æ–°æ€è€ƒé—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼š

ğŸ¯ **åŸå§‹é—®é¢˜**: {user_query}

ğŸŒ± **åŸå§‹æ€ç»´ç§å­**: {original_seed[:200]}...

âŒ **éªŒè¯å¤±è´¥æ¨¡å¼**:
- å¤±è´¥è·¯å¾„æ•°: {failure_patterns.get('total_failed_paths', 0)}
- å¹³å‡å¯è¡Œæ€§: {failure_patterns.get('average_feasibility', 0):.2f}
- ä¸»è¦é£é™©: {failure_patterns.get('risk_patterns', [])}

ğŸ’¡ **åˆ›æ–°æ€è€ƒè¦æ±‚**:
1. é¿å¼€å·²éªŒè¯çš„å¤±è´¥æ¨¡å¼
2. ä»ä¸åŒè§’åº¦é‡æ–°å®šä¹‰é—®é¢˜
3. è€ƒè™‘éå¸¸è§„ã€åˆ›æ–°æ€§çš„è§£å†³è·¯å¾„
4. é™ä½å·²è¯†åˆ«çš„é£é™©å› ç´ 

è¯·ç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„åˆ›æ–°æ€ç»´ç§å­ï¼Œé¿å¼€å¤±è´¥æ¨¡å¼ï¼Œæä¾›åˆ›æ–°è§†è§’ï¼š
"""
            
            # ä½¿ç”¨æ­£ç¡®çš„LLMè°ƒç”¨æ¥å£
            if self.llm_manager:
                llm_result = self.llm_manager.chat_completion(innovation_prompt, temperature=0.8)
                if llm_result.success:
                    innovative_response = llm_result.content
                else:
                    raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {llm_result.error_message}")
            elif self.llm_client:
                # æ£€æŸ¥å®¢æˆ·ç«¯ç±»å‹å¹¶è°ƒç”¨æ­£ç¡®çš„æ–¹æ³•
                if hasattr(self.llm_client, 'call_api'):
                    innovative_response = self.llm_client.call_api(innovation_prompt, temperature=0.8)
                elif hasattr(self.llm_client, 'chat_completion'):
                    llm_result = self.llm_client.chat_completion(innovation_prompt, temperature=0.8)
                    if llm_result.success:
                        innovative_response = llm_result.content
                    else:
                        raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {llm_result.error_message}")
                else:
                    raise Exception("LLMå®¢æˆ·ç«¯ä¸æ”¯æŒæ‰€éœ€çš„æ–¹æ³•")
            else:
                raise Exception("æ²¡æœ‰å¯ç”¨çš„LLMå®¢æˆ·ç«¯")
            
            # ç®€å•æå–å“åº”å†…å®¹
            if len(innovative_response) > 50:
                return innovative_response
            else:
                return self._heuristic_innovative_seed(user_query, failure_patterns)
                
        except Exception as e:
            logger.warning(f"âš ï¸ LLMåˆ›æ–°ç§å­ç”Ÿæˆå¤±è´¥: {e}")
            return self._heuristic_innovative_seed(user_query, failure_patterns)
    
    def _heuristic_innovative_seed(self, user_query: str, failure_patterns: Dict) -> str:
        """å¯å‘å¼åˆ›æ–°ç§å­ç”Ÿæˆ"""
        innovation_approaches = [
            f"é‡æ–°å®šä¹‰é—®é¢˜è§’åº¦: {user_query}",
            f"é€†å‘æ€ç»´è§£å†³: {user_query}",
            f"è·¨é¢†åŸŸå€Ÿé‰´æ–¹æ³•: {user_query}",
            f"ç®€åŒ–åˆ°æ ¸å¿ƒéœ€æ±‚: {user_query}",
            f"åˆ†é˜¶æ®µæ¸è¿›è§£å†³: {user_query}"
        ]
        
        import random
        return random.choice(innovation_approaches)
    
    def _create_emergency_innovative_paths(self, user_query: str, failure_patterns: Dict) -> List:
        """åˆ›å»ºåº”æ€¥åˆ›æ–°è·¯å¾„"""
        from .data_structures import ReasoningPath
        
        emergency_paths = [
            ReasoningPath(
                path_id="emergency_innovative_1",
                path_type="é—®é¢˜é‡å®šä¹‰å‹",
                description="é‡æ–°å®šä¹‰é—®é¢˜çš„æ ¸å¿ƒéœ€æ±‚ï¼Œå¯»æ‰¾æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆ",
                prompt_template="è®©æˆ‘ä»¬é‡æ–°å®¡è§†è¿™ä¸ªé—®é¢˜çš„æœ¬è´¨éœ€æ±‚ï¼š{task}"
            ),
            ReasoningPath(
                path_id="emergency_innovative_2", 
                path_type="åˆ†æ­¥ç®€åŒ–å‹",
                description="å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªç®€å•æ­¥éª¤",
                prompt_template="å°†å¤æ‚ä»»åŠ¡æ‹†åˆ†ä¸ºå¯ç®¡ç†çš„å­ä»»åŠ¡ï¼š{task}"
            ),
            ReasoningPath(
                path_id="emergency_innovative_3",
                path_type="æ›¿ä»£æ–¹æ¡ˆå‹", 
                description="å¯»æ‰¾å®ç°ç›¸åŒç›®æ ‡çš„æ›¿ä»£æ–¹æ³•",
                prompt_template="æ¢ç´¢è¾¾æˆç›®æ ‡çš„ä¸åŒè·¯å¾„ï¼š{task}"
            )
        ]
        
        return emergency_paths
    
    # ==================== å¤šLLMç®¡ç†æ–¹æ³• ====================
    
    def switch_llm_provider(self, provider_name: str) -> bool:
        """
        åˆ‡æ¢LLMæä¾›å•†
        
        Args:
            provider_name: æä¾›å•†åç§°
            
        Returns:
            bool: æ˜¯å¦åˆ‡æ¢æˆåŠŸ
        """
        if not self.llm_manager:
            logger.warning("âš ï¸ LLMç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ‡æ¢æä¾›å•†")
            return False
        
        success = self.llm_manager.switch_primary_provider(provider_name)
        if success:
            logger.info(f"ğŸ”„ å·²åˆ‡æ¢åˆ°LLMæä¾›å•†: {provider_name}")
        else:
            logger.error(f"âŒ åˆ‡æ¢LLMæä¾›å•†å¤±è´¥: {provider_name}")
        
        return success
    
    def get_llm_provider_status(self) -> Dict[str, Any]:
        """
        è·å–LLMæä¾›å•†çŠ¶æ€
        
        Returns:
            Dict[str, Any]: æä¾›å•†çŠ¶æ€ä¿¡æ¯
        """
        if not self.llm_manager:
            return {
                'initialized': False,
                'error': 'LLMç®¡ç†å™¨æœªåˆå§‹åŒ–',
                'fallback_mode': True,
                'available_providers': []
            }
        
        return self.llm_manager.get_provider_status()
    
    def get_available_llm_models(self, provider_name: Optional[str] = None) -> Dict[str, List[str]]:
        """
        è·å–å¯ç”¨çš„LLMæ¨¡å‹
        
        Args:
            provider_name: æä¾›å•†åç§°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict[str, List[str]]: æä¾›å•†å’Œå¯¹åº”çš„æ¨¡å‹åˆ—è¡¨
        """
        if not self.llm_manager:
            return {"error": "LLMç®¡ç†å™¨æœªåˆå§‹åŒ–"}
        
        return self.llm_manager.get_available_models(provider_name)
    
    def run_llm_health_check(self, force: bool = False) -> Dict[str, bool]:
        """
        è¿è¡ŒLLMæä¾›å•†å¥åº·æ£€æŸ¥
        
        Args:
            force: æ˜¯å¦å¼ºåˆ¶æ£€æŸ¥
            
        Returns:
            Dict[str, bool]: å„æä¾›å•†çš„å¥åº·çŠ¶æ€
        """
        if not self.llm_manager:
            return {"error": "LLMç®¡ç†å™¨æœªåˆå§‹åŒ–"}
        
        return self.llm_manager.health_check(force)
    
    def get_llm_cost_summary(self) -> Dict[str, Any]:
        """
        è·å–LLMä½¿ç”¨æˆæœ¬æ€»ç»“
        
        Returns:
            Dict[str, Any]: æˆæœ¬æ€»ç»“ä¿¡æ¯
        """
        if not self.llm_manager:
            return {"error": "LLMç®¡ç†å™¨æœªåˆå§‹åŒ–"}
        
        status = self.llm_manager.get_provider_status()
        cost_data = status.get('stats', {}).get('cost_tracking', {})
        
        total_cost = sum(cost_data.values())
        
        return {
            'total_cost_usd': total_cost,
            'cost_by_provider': dict(cost_data),
            'total_requests': status.get('stats', {}).get('total_requests', 0),
            'successful_requests': status.get('stats', {}).get('successful_requests', 0),
            'fallback_count': status.get('stats', {}).get('fallback_count', 0)
        }
    
    def _create_conservative_emergency_path(self, user_query: str):
        """åˆ›å»ºä¿å®ˆçš„åº”æ€¥è·¯å¾„"""
        from .data_structures import ReasoningPath
        
        return ReasoningPath(
            path_id="conservative_emergency",
            path_type="ä¿å®ˆåº”æ€¥å‹",
            description="ä½¿ç”¨æœ€åŸºç¡€ã€æœ€å¯é çš„æ–¹æ³•å¤„ç†é—®é¢˜",
            prompt_template="ä½¿ç”¨æœ€ç›´æ¥ã€æœ€åŸºç¡€çš„æ–¹æ³•å¤„ç†ï¼š{task}"
        )
