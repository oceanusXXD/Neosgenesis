#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è·¯å¾„ç”Ÿæˆå™¨ - è´Ÿè´£åŸºäºæ€ç»´ç§å­ç”Ÿæˆå¤šæ ·åŒ–æ€ç»´è·¯å¾„
Path Generator - responsible for generating diverse reasoning paths from thinking seeds

æ”¹é€ åæ”¯æŒï¼š
1. é˜¶æ®µäºŒï¼šæ¥æ”¶æ€ç»´ç§å­ï¼Œç”Ÿæˆæ€ç»´è·¯å¾„åˆ—è¡¨
2. é¢„å®šä¹‰è·¯å¾„æ¨¡æ¿åº“ï¼Œè¦†ç›–å¤šç§æ€è€ƒèŒƒå¼  
3. åŸºäºå…³é”®è¯çš„æ™ºèƒ½è·¯å¾„é€‰æ‹©ç®—æ³•
4. å‘åå…¼å®¹æ€§ä¿æŒ
"""

import json
import time
import random
import logging
import re
from typing import Dict, List, Optional, Any, Tuple, Set
from collections import defaultdict
from dataclasses import dataclass

from .data_structures import ReasoningPath, TaskComplexity
# from .utils.client_adapter import DeepSeekClientAdapter  # ä¸å†éœ€è¦ï¼Œä½¿ç”¨ä¾èµ–æ³¨å…¥
from .utils.common_utils import parse_json_response, extract_context_factors
from config import PROMPT_TEMPLATES

logger = logging.getLogger(__name__)


class LLMDrivenDimensionCreator:
    """LLMé©±åŠ¨çš„åŠ¨æ€ç»´åº¦åˆ›å»ºå™¨"""
    
    def __init__(self, api_key: str = "", llm_client=None):
        """
        åˆå§‹åŒ–LLMé©±åŠ¨çš„ç»´åº¦åˆ›å»ºå™¨
        
        Args:
            api_key: APIå¯†é’¥ï¼ˆå‘åå…¼å®¹ï¼‰
            llm_client: å…±äº«çš„LLMå®¢æˆ·ç«¯ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.api_key = api_key
        
        # ğŸ”§ ä¾èµ–æ³¨å…¥ï¼šä½¿ç”¨ä¼ å…¥çš„å®¢æˆ·ç«¯ï¼ˆçº¯ä¾èµ–æ³¨å…¥æ¨¡å¼ï¼‰
        self.api_caller = llm_client
        if self.api_caller:
            logger.debug("ğŸ”§ ç»´åº¦åˆ›å»ºå™¨ä½¿ç”¨å…±äº«LLMå®¢æˆ·ç«¯")
        else:
            logger.warning("âš ï¸ æœªæä¾›LLMå®¢æˆ·ç«¯ï¼Œç»´åº¦åˆ›å»ºå™¨å°†æ— æ³•ä½¿ç”¨AIåŠŸèƒ½")
            logger.info("ğŸ’¡ è¯·ç¡®ä¿ä»ä¸Šå±‚ï¼ˆMainControllerï¼‰ä¼ å…¥æœ‰æ•ˆçš„llm_client")
        
        # æ€§èƒ½å’Œå†å²è®°å½•
        self.performance_history = defaultdict(list)
        self.discovered_dimensions = defaultdict(dict)  # å­˜å‚¨LLMå‘ç°çš„ç»´åº¦
        self.dimension_usage_frequency = defaultdict(int)  # ç»´åº¦ä½¿ç”¨é¢‘ç‡
        self.dimension_creation_patterns = defaultdict(list)  # ç»´åº¦åˆ›å»ºæ¨¡å¼
        self.task_dimension_mapping = defaultdict(list)  # ä»»åŠ¡-ç»´åº¦æ˜ å°„å…³ç³»
        
        # LLMä¸“ç”¨å±æ€§
        self.llm_session_history = []  # LLMä¼šè¯å†å²
        self.dimension_quality_scores = defaultdict(float)  # ç»´åº¦è´¨é‡è¯„åˆ†
        self.creative_dimension_cache = {}  # åˆ›æ–°ç»´åº¦ç¼“å­˜
        
        # ğŸš€ LLMå…ƒä¼˜åŠ¿å‡½æ•°è¯„ä¼°ç³»ç»Ÿ
        self.llm_capability_tracking = {
            'task_type_advantages': defaultdict(lambda: {'success_count': 0, 'total_count': 0, 'avg_quality': 0.5}),
            'domain_expertise_scores': defaultdict(float),
            'complexity_handling_ability': defaultdict(float),
            'creative_dimension_success_rate': 0.5,
            'historical_limitation_patterns': [],
            'advantage_compensation_strategies': {}
        }
        
        logger.info("ğŸ¤– LLMé©±åŠ¨çš„ç»´åº¦åˆ›å»ºå™¨å·²åˆå§‹åŒ– (ä½¿ç”¨ç»Ÿä¸€å®¢æˆ·ç«¯æ¥å£)")

    def create_dynamic_dimensions(self, user_query: str, execution_context: Optional[Dict] = None) -> List[ReasoningPath]:
        """ä½¿ç”¨LLMåˆ›å»ºåŠ¨æ€ç»´åº¦"""
        
        logger.info(f"ğŸ¤– å¼€å§‹LLMç»´åº¦åˆ›å»º: {user_query[:50]}...")
        
        try:
            # æ„å»ºç»´åº¦åˆ›å»ºæç¤º
            llm_prompt = self._build_dimension_creation_prompt(user_query, execution_context)
            
            # è°ƒç”¨LLMè¿›è¡Œæ¨ç†
            llm_response = self.api_caller.call_api(llm_prompt, temperature=0.8)
            
            # è§£æLLMå“åº”
            dimension_result = self._parse_llm_dimension_response(llm_response)
            
            # åŸºäºLLMåˆ†æç”Ÿæˆæ€ç»´è·¯å¾„
            reasoning_paths = self._create_reasoning_paths_from_analysis(
                dimension_result, user_query, execution_context
            )
            
            logger.info(f"ğŸ§  ç”Ÿæˆ {len(reasoning_paths)} æ¡æ€ç»´è·¯å¾„")
            return reasoning_paths
            
        except Exception as e:
            logger.error(f"âŒ LLMç»´åº¦åˆ›å»ºå¤±è´¥: {e}")
            # å›é€€åˆ°æ™ºèƒ½ç»´åº¦ç”Ÿæˆ
            return self._create_fallback_reasoning_paths(user_query, execution_context, str(e))
        
    def _build_dimension_creation_prompt(self, user_query: str, execution_context: Optional[Dict] = None) -> str:
        """æ„å»ºLLMç»´åº¦åˆ›å»ºæç¤º"""
        
        context_info = ""
        if execution_context:
            context_info = f"\nğŸ”§ æ‰§è¡Œç¯å¢ƒä¿¡æ¯: {json.dumps(execution_context, ensure_ascii=False, indent=2)}"
        
        # æ·»åŠ å†å²å­¦ä¹ ä¿¡æ¯
        historical_insights = self._get_historical_insights(user_query)
        
        # ä½¿ç”¨é…ç½®ä¸­çš„æç¤ºæ¨¡æ¿
        prompt = PROMPT_TEMPLATES["dimension_creation"].format(
            user_query=user_query,
            context_info=context_info,
            historical_insights=historical_insights
        )
        
        return prompt
    
    def _get_historical_insights(self, user_query: str) -> str:
        """è·å–å†å²å­¦ä¹ æ´å¯Ÿ"""
        
        insights = []
        
        # åˆ†æç›¸ä¼¼ä»»åŠ¡çš„å†å²ç»´åº¦åˆ›å»º
        similar_tasks = self._find_similar_tasks(user_query)
        if similar_tasks:
            insights.append(f"ğŸ“ˆ å‘ç°{len(similar_tasks)}ä¸ªç›¸ä¼¼ä»»åŠ¡çš„å†å²è®°å½•")
            
            # åˆ†æé«˜è´¨é‡ç»´åº¦
            high_quality_dimensions = []
            for task_record in similar_tasks:
                for dim_name, score in task_record.get('dimension_scores', {}).items():
                    if score > 0.7:
                        high_quality_dimensions.append(dim_name)
            
            if high_quality_dimensions:
                unique_dims = list(set(high_quality_dimensions))
                insights.append(f"âœ… å†å²é«˜è´¨é‡ç»´åº¦: {', '.join(unique_dims[:5])}")
        
        # åˆ†æç»´åº¦åˆ›å»ºæ¨¡å¼
        if self.dimension_creation_patterns:
            common_patterns = []
            for pattern_type, patterns in self.dimension_creation_patterns.items():
                if len(patterns) >= 3:  # è‡³å°‘å‡ºç°3æ¬¡çš„æ¨¡å¼
                    common_patterns.append(pattern_type)
            
            if common_patterns:
                insights.append(f"ğŸ”„ å¸¸è§åˆ›å»ºæ¨¡å¼: {', '.join(common_patterns[:3])}")
        
        return '\n'.join(insights) if insights else "ğŸ†• é¦–æ¬¡å¤„ç†æ­¤ç±»ä»»åŠ¡ï¼ŒåŸºäºä¸“ä¸šçŸ¥è¯†åˆ›å»ºç»´åº¦"
    
    def _find_similar_tasks(self, user_query: str) -> List[Dict]:
        """æŸ¥æ‰¾ç›¸ä¼¼ä»»åŠ¡çš„å†å²è®°å½•"""
        
        similar_tasks = []
        query_keywords = set(user_query.lower().split())
        
        for task_key, task_records in self.task_dimension_mapping.items():
            task_keywords = set(task_key.lower().split())
            
            # è®¡ç®—å…³é”®è¯é‡å åº¦
            if query_keywords and task_keywords:
                overlap = len(query_keywords.intersection(task_keywords))
                total = len(query_keywords.union(task_keywords))
                similarity = overlap / total if total > 0 else 0.0
                
                if similarity > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    for record in task_records[-3:]:  # å–æœ€è¿‘3æ¡è®°å½•
                        record['similarity'] = similarity
                        similar_tasks.append(record)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similar_tasks.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        return similar_tasks[:5]  # è¿”å›æœ€ç›¸ä¼¼çš„5ä¸ªä»»åŠ¡
    
    def _parse_llm_dimension_response(self, response: str) -> Dict[str, Any]:
        """è§£æLLMçš„ç»´åº¦åˆ›å»ºå“åº”"""
        
        try:
            result = parse_json_response(response)
            
            # æ£€æŸ¥è§£æç»“æœæ˜¯å¦æœ‰æ•ˆ
            if result is None:
                logger.warning("âš ï¸ DeepSeekå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„")
                result = {
                    "task_analysis": {
                        "complexity": 0.5,
                        "domain": "unknown",
                        "key_challenges": ["ä»£ç å®ç°", "é”™è¯¯å¤„ç†"]
                    },
                    "suggested_dimensions": {},
                    "reasoning": "DeepSeekå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„"
                }
            
            # éªŒè¯å“åº”ç»“æ„
            if 'suggested_dimensions' not in result:
                logger.warning("âš ï¸ DeepSeekå“åº”ç¼ºå°‘suggested_dimensionså­—æ®µ")
                result['suggested_dimensions'] = self._create_fallback_dimensions(response)
            
            if 'task_analysis' not in result:
                result['task_analysis'] = {
                    "complexity": 0.5,
                    "domain": "unknown",
                    "key_challenges": ["ä»£ç å®ç°", "é”™è¯¯å¤„ç†"]
                }
            
            if 'reasoning' not in result:
                result['reasoning'] = "DeepSeekæ™ºèƒ½æ¨ç†"
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è§£æDeepSeekå“åº”å¤±è´¥: {e}")
            # è¿”å›å›é€€ç»“æ„
            return {
                "task_analysis": {
                    "complexity": 0.5,
                    "domain": "unknown",
                    "key_challenges": ["ä»£ç å®ç°", "é”™è¯¯å¤„ç†"]
                },
                "suggested_dimensions": self._create_fallback_dimensions(response),
                "reasoning": "DeepSeekå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½å›é€€ç»´åº¦"
            }
    
    def _create_fallback_dimensions(self, response: str) -> Dict[str, Dict[str, str]]:
        """ğŸš€ åˆ›å»ºå®Œå…¨åŠ¨æ€çš„å›é€€ç»´åº¦ - åŸºäºä»»åŠ¡è¯­ä¹‰é›¶é¢„è®¾"""
        
        # åŸºäºå“åº”å†…å®¹çš„å…³é”®è¯åˆ†æåˆ›å»ºç»´åº¦
        response_lower = response.lower()
        dimensions = {}
        
        # ğŸ§  åŠ¨æ€æå–æ ¸å¿ƒæ¦‚å¿µç”Ÿæˆç»´åº¦åç§°
        core_concepts = []
        
        # ä»å“åº”ä¸­æå–æŠ€æœ¯æ¦‚å¿µ
        tech_concepts = {
            'ç½‘ç»œ': ['è¿æ¥æ–¹å¼', 'é€šä¿¡åè®®', 'è¯·æ±‚æ¨¡å¼'],
            'api': ['æ¥å£è®¾è®¡', 'è°ƒç”¨ç­–ç•¥', 'å“åº”å¤„ç†'],
            'æ•°æ®': ['å­˜å‚¨æ–¹æ¡ˆ', 'å¤„ç†æµç¨‹', 'è®¿é—®æ¨¡å¼'],
            'ç®—æ³•': ['è®¡ç®—æ–¹æ³•', 'ä¼˜åŒ–ç­–ç•¥', 'æ‰§è¡Œè·¯å¾„'],
            'ç³»ç»Ÿ': ['æ¶æ„è®¾è®¡', 'è¿è¡Œæ¨¡å¼', 'æ‰©å±•æ–¹æ¡ˆ'],
            'ç”¨æˆ·': ['äº¤äº’æ¨¡å¼', 'ä½“éªŒè®¾è®¡', 'å“åº”ç­–ç•¥'],
            'å®‰å…¨': ['é˜²æŠ¤æœºåˆ¶', 'éªŒè¯æ–¹å¼', 'æƒé™æ§åˆ¶'],
            'æ€§èƒ½': ['ä¼˜åŒ–æ–¹å‘', 'èµ„æºç­–ç•¥', 'å“åº”é€Ÿåº¦'],
            'å­˜å‚¨': ['æ•°æ®ç®¡ç†', 'æŒä¹…åŒ–æ–¹æ¡ˆ', 'è®¿é—®ä¼˜åŒ–'],
            'å¹¶å‘': ['å¤„ç†æ¨¡å¼', 'åŒæ­¥ç­–ç•¥', 'èµ„æºåˆ†é…']
        }
        
        # åŠ¨æ€è¯†åˆ«ç›¸å…³æ¦‚å¿µ
        for keyword, possible_dims in tech_concepts.items():
            if keyword in response_lower:
                selected_dim = random.choice(possible_dims)
                core_concepts.append((keyword, selected_dim))
        
        # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç‰¹å®šæ¦‚å¿µï¼Œä½¿ç”¨é€šç”¨æ¦‚å¿µ
        if not core_concepts:
            generic_concepts = [
                ('å®ç°', 'æ‰§è¡Œæ–¹å¼'),
                ('å¤„ç†', 'è¿è¡Œæ¨¡å¼'),
                ('ç®¡ç†', 'æ§åˆ¶ç­–ç•¥'),
                ('è®¾è®¡', 'æ„å»ºæ–¹æ¡ˆ'),
                ('ä¼˜åŒ–', 'æ”¹è¿›è·¯å¾„')
            ]
            core_concepts = random.sample(generic_concepts, random.randint(2, 3))
        
        # ğŸ¯ åŸºäºæ ¸å¿ƒæ¦‚å¿µç”Ÿæˆç»´åº¦å’Œé€‰é¡¹
        for concept_key, dimension_name in core_concepts[:4]:  # æœ€å¤š4ä¸ªç»´åº¦
            # åŠ¨æ€ç”Ÿæˆé€‰é¡¹
            if concept_key in ['ç½‘ç»œ', 'api', 'è¯·æ±‚']:
                options = {
                    "é«˜æ•ˆè¿æ¥": f"é‡‡ç”¨é«˜æ•ˆçš„{dimension_name}æ–¹æ¡ˆ",
                    "ç¨³å®šè¿æ¥": f"ç¡®ä¿{dimension_name}çš„ç¨³å®šæ€§",
                    "æ™ºèƒ½è¿æ¥": f"å®ç°æ™ºèƒ½åŒ–çš„{dimension_name}"
                }
            elif concept_key in ['æ•°æ®', 'ä¿¡æ¯', 'å†…å®¹']:
                options = {
                    "æµå¼æ–¹æ¡ˆ": f"é‡‡ç”¨æµå¼çš„{dimension_name}",
                    "æ‰¹é‡æ–¹æ¡ˆ": f"é‡‡ç”¨æ‰¹é‡çš„{dimension_name}",
                    "å®æ—¶æ–¹æ¡ˆ": f"é‡‡ç”¨å®æ—¶çš„{dimension_name}"
                }
            elif concept_key in ['ç®—æ³•', 'è®¡ç®—', 'å¤„ç†']:
                options = {
                    "ä¼˜åŒ–ç®—æ³•": f"ä½¿ç”¨ä¼˜åŒ–çš„{dimension_name}",
                    "æ ‡å‡†ç®—æ³•": f"ä½¿ç”¨æ ‡å‡†çš„{dimension_name}",
                    "åˆ›æ–°ç®—æ³•": f"é‡‡ç”¨åˆ›æ–°çš„{dimension_name}"
                }
            elif concept_key in ['ç³»ç»Ÿ', 'æ¶æ„', 'è®¾è®¡']:
                options = {
                    "æ¨¡å—åŒ–è®¾è®¡": f"é‡‡ç”¨æ¨¡å—åŒ–çš„{dimension_name}",
                    "é›†æˆåŒ–è®¾è®¡": f"é‡‡ç”¨é›†æˆåŒ–çš„{dimension_name}",
                    "åˆ†å¸ƒå¼è®¾è®¡": f"é‡‡ç”¨åˆ†å¸ƒå¼çš„{dimension_name}"
                }
            else:
                # é€šç”¨é€‰é¡¹ç”Ÿæˆæ¨¡å¼
                approaches = ["é«˜æ•ˆ", "ç¨³å®š", "æ™ºèƒ½", "ä¼˜åŒ–", "æ ‡å‡†", "åˆ›æ–°"]
                selected_approaches = random.sample(approaches, 3)
                options = {
                    f"{approach}æ–¹æ¡ˆ": f"é‡‡ç”¨{approach}çš„{dimension_name}æ–¹æ¡ˆ"
                    for approach in selected_approaches
                }
            
            dimensions[dimension_name] = options
        
        # ç¡®ä¿è‡³å°‘æœ‰2ä¸ªç»´åº¦
        if len(dimensions) < 2:
            fallback_dims = {
                "æ‰§è¡Œæ–¹å¼": {
                    "ç›´æ¥æ‰§è¡Œ": "é‡‡ç”¨ç›´æ¥çš„æ‰§è¡Œæ–¹å¼",
                    "åˆ†æ­¥æ‰§è¡Œ": "é‡‡ç”¨åˆ†æ­¥çš„æ‰§è¡Œæ–¹å¼",
                    "æ™ºèƒ½æ‰§è¡Œ": "é‡‡ç”¨æ™ºèƒ½çš„æ‰§è¡Œæ–¹å¼"
                },
                "èµ„æºç®¡ç†": {
                    "é«˜æ•ˆç®¡ç†": "å®ç°é«˜æ•ˆçš„èµ„æºç®¡ç†",
                    "å‡è¡¡ç®¡ç†": "å®ç°å‡è¡¡çš„èµ„æºç®¡ç†",
                    "æ™ºèƒ½ç®¡ç†": "å®ç°æ™ºèƒ½çš„èµ„æºç®¡ç†"
                }
            }
            for dim_name, options in fallback_dims.items():
                if dim_name not in dimensions:
                    dimensions[dim_name] = options
                    if len(dimensions) >= 2:
                        break
        
        return dimensions
    
    def _optimize_dimension_selection_traditional(self, suggested_dimensions: Dict[str, Dict[str, str]], 
                                                user_query: str, execution_context: Optional[Dict]) -> Tuple[Dict[str, str], Dict[str, Any]]:
        """åŸºäºä¼ ç»Ÿæ–¹æ³•çš„ç»´åº¦é€‰æ‹©ä¼˜åŒ–"""
        
        logger.info("ğŸ¯ å¼€å§‹ä¼ ç»Ÿç»´åº¦é€‰æ‹©ä¼˜åŒ–")
        
        optimized_selection = {}
        semantic_tracking = {
            'semantic_matching_results': {},
            'semantic_raw_responses': {},
            'fallback_dimensions': [],
            'success_count': 0,
            'total_count': 0
        }
        
        for dim_name, dim_options in suggested_dimensions.items():
            logger.info(f"ğŸ¯ ä¼˜åŒ–ç»´åº¦é€‰æ‹©ï¼š{dim_name}")
            
            # ä½¿ç”¨ç®€å•çš„é€‰é¡¹é€‰æ‹©é€»è¾‘
            best_option = self._select_best_option_for_dimension_simple(
                dim_name, dim_options, user_query
            )
            optimized_selection[dim_name] = best_option
            
            # è®°å½•åŸºç¡€è·Ÿè¸ªä¿¡æ¯
            semantic_tracking['total_count'] += 1
            semantic_tracking['success_count'] += 1  # ç®€åŒ–å®ç°ï¼Œå‡è®¾æˆåŠŸ
            semantic_tracking['semantic_matching_results'][dim_name] = {"method": "simple_selection"}
            semantic_tracking['semantic_raw_responses'][dim_name] = f"é€‰æ‹©äº† {best_option}"
            
            logger.debug(f"ğŸ¯ æœ€ç»ˆé€‰æ‹©: {dim_name} -> {optimized_selection[dim_name]}")
        
        # è®¡ç®—è¯­ä¹‰åŒ¹é…æˆåŠŸç‡
        if semantic_tracking['total_count'] > 0:
            semantic_tracking['success_rate'] = semantic_tracking['success_count'] / semantic_tracking['total_count']
        else:
            semantic_tracking['success_rate'] = 0.0
        
        logger.info(f"ğŸ‰ ä¼ ç»Ÿç»´åº¦é€‰æ‹©å®Œæˆ - æˆåŠŸç‡ï¼š{semantic_tracking['success_rate']:.3f}")
            
        return optimized_selection, semantic_tracking
    
    def _select_best_option_for_dimension_simple(self, dim_name: str, dim_options: Dict[str, str], user_query: str) -> str:
        """ç®€å•çš„ç»´åº¦é€‰é¡¹é€‰æ‹©æ–¹æ³•"""
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªé€‰é¡¹ï¼Œç›´æ¥è¿”å›
        if len(dim_options) == 1:
            return list(dim_options.keys())[0]
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…é€»è¾‘
        user_query_lower = user_query.lower()
        best_option = None
        best_score = 0
        
        for option_name, option_desc in dim_options.items():
            score = 0
            option_lower = (option_name + " " + option_desc).lower()
            
            # åŸºäºç®€å•çš„å…³é”®è¯åŒ¹é…è®¡åˆ†
            common_words = ['ç®€å•', 'å¿«é€Ÿ', 'æ ‡å‡†', 'åŸºç¡€', 'é»˜è®¤', 'å¸¸è§„']
            if any(word in user_query_lower for word in common_words):
                if any(word in option_lower for word in common_words):
                    score += 2
            
            # æŠ€æœ¯å…³é”®è¯åŒ¹é…
            tech_words = ['æŠ€æœ¯', 'ç®—æ³•', 'ç³»ç»Ÿ', 'æ¶æ„', 'å¼€å‘', 'å®ç°']
            if any(word in user_query_lower for word in tech_words):
                if any(word in option_lower for word in tech_words):
                    score += 1
            
            # æ€§èƒ½å…³é”®è¯åŒ¹é…
            perf_words = ['é«˜æ•ˆ', 'å¿«é€Ÿ', 'ä¼˜åŒ–', 'æ€§èƒ½']
            if any(word in user_query_lower for word in perf_words):
                if any(word in option_lower for word in perf_words):
                    score += 1.5
            
            if score > best_score:
                best_score = score
                best_option = option_name
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªé€‰é¡¹
        return best_option or list(dim_options.keys())[0]
    
    def _calculate_dimension_confidences(self, selected_dimensions: Dict[str, str], 
                                       dimension_result: Dict[str, Any],
                                       user_query: str) -> Dict[str, float]:
        """è®¡ç®—ç»´åº¦é€‰æ‹©çš„ç½®ä¿¡åº¦"""
        
        confidence_scores = {}
        base_confidence = 0.7  # DeepSeekæ¨ç†çš„åŸºç¡€ç½®ä¿¡åº¦
        
        for dim_name, selected_option in selected_dimensions.items():
            confidence = base_confidence
            
            # åŸºäºDeepSeekåˆ†æçš„å¤æ‚åº¦è°ƒæ•´
            task_complexity = dimension_result.get('task_analysis', {}).get('complexity', 0.5)
            complexity_adjustment = (1.0 - task_complexity) * 0.1  # å¤æ‚åº¦è¶Šä½ï¼Œç½®ä¿¡åº¦è¶Šé«˜
            
            # åŸºäºå†å²ä½¿ç”¨é¢‘ç‡è°ƒæ•´
            frequency_boost = min(0.2, self.dimension_usage_frequency[dim_name] * 0.02)
            
            # åŸºäºç»´åº¦è´¨é‡è¯„åˆ†è°ƒæ•´
            quality_boost = self.dimension_quality_scores.get(dim_name, 0.5) * 0.2
            
            # åŸºäºé€‰é¡¹åŒ¹é…åº¦è°ƒæ•´
            option_match_boost = self._calculate_option_match_confidence(
                dim_name, selected_option, user_query
            ) * 0.15
            
            # ç»¼åˆè®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
            final_confidence = confidence + complexity_adjustment + frequency_boost + quality_boost + option_match_boost
            confidence_scores[dim_name] = min(1.0, max(0.1, final_confidence))
            
            logger.debug(f"ğŸ¯ ç½®ä¿¡åº¦è®¡ç®—: {dim_name} = {final_confidence:.3f}")
        
        return confidence_scores
    
    def _calculate_option_match_confidence(self, dim_name: str, selected_option: str, user_query: str) -> float:
        """è®¡ç®—é€‰é¡¹ä¸æŸ¥è¯¢çš„åŒ¹é…ç½®ä¿¡åº¦"""
        
        query_words = set(user_query.lower().split())
        option_words = set(selected_option.lower().split())
        
        # è®¡ç®—è¯æ±‡é‡å åº¦
        if query_words and option_words:
            overlap = len(query_words.intersection(option_words))
            total = len(query_words.union(option_words))
            word_overlap = overlap / total if total > 0 else 0.0
        else:
            word_overlap = 0.0
        
        # è®¡ç®—è¯­ä¹‰åŒ¹é…åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        semantic_match = 0.5  # é»˜è®¤ä¸­ç­‰åŒ¹é…
        
        # åŸºäºå…³é”®è¯çš„è¯­ä¹‰åŒ¹é…
        if "å¿«é€Ÿ" in user_query.lower() and any(word in selected_option.lower() for word in ["å¿«é€Ÿ", "é«˜æ•ˆ", "é€Ÿåº¦"]):
            semantic_match = 0.9
        elif "ç¨³å®š" in user_query.lower() and any(word in selected_option.lower() for word in ["ç¨³å®š", "å¯é "]):
            semantic_match = 0.9
        elif "ç®€å•" in user_query.lower() and any(word in selected_option.lower() for word in ["ç®€æ´", "è½»é‡"]):
            semantic_match = 0.8
        
        return (word_overlap + semantic_match) / 2
    
    def _update_dimension_usage_history(self, selected_dimensions: Dict[str, str], 
                                      user_query: str, dimension_result: Dict[str, Any]):
        """æ›´æ–°ç»´åº¦ä½¿ç”¨å†å²"""
        
        # æ›´æ–°ä½¿ç”¨é¢‘ç‡
        for dim_name in selected_dimensions.keys():
            self.dimension_usage_frequency[dim_name] += 1
        
        # è®°å½•ä»»åŠ¡-ç»´åº¦æ˜ å°„
        task_key = self._generate_task_key(user_query)
        if task_key not in self.task_dimension_mapping:
            self.task_dimension_mapping[task_key] = []
        
        task_record = {
            'timestamp': time.time(),
            'selected_dimensions': selected_dimensions,
            'task_analysis': dimension_result.get('task_analysis', {}),
            'dimension_scores': {},  # å°†åœ¨åé¦ˆæ—¶æ›´æ–°
            'user_query_length': len(user_query)
        }
        
        self.task_dimension_mapping[task_key].append(task_record)
        
        # é™åˆ¶å†å²è®°å½•å¤§å°
        if len(self.task_dimension_mapping[task_key]) > 10:
            self.task_dimension_mapping[task_key] = self.task_dimension_mapping[task_key][-10:]
        
        # è®°å½•ç»´åº¦åˆ›å»ºæ¨¡å¼
        pattern_key = f"{len(selected_dimensions)}ç»´åº¦_{dimension_result.get('task_analysis', {}).get('domain', 'unknown')}"
        if pattern_key not in self.dimension_creation_patterns:
            self.dimension_creation_patterns[pattern_key] = []
        
        self.dimension_creation_patterns[pattern_key].append({
            'timestamp': time.time(),
            'dimensions': list(selected_dimensions.keys()),
            'complexity': dimension_result.get('task_analysis', {}).get('complexity', 0.5)
        })
        
        logger.info(f"ğŸ“Š æ›´æ–°ç»´åº¦ä½¿ç”¨å†å²: {len(selected_dimensions)}ä¸ªç»´åº¦")
    
    def _generate_task_key(self, user_query: str) -> str:
        """ç”Ÿæˆä»»åŠ¡å…³é”®å­—"""
        
        # æå–å…³é”®è¯ç”Ÿæˆä»»åŠ¡é”®
        words = user_query.lower().split()
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'æˆ–', 'ä½†', 'å¦‚æœ', 'è¯·', 'å¸®', 'æˆ‘', 'ä¸€ä¸ª', 'è¿™ä¸ª', 'é‚£ä¸ª'}
        keywords = [word for word in words if word not in stop_words and len(word) > 1]
        
        # å–å‰5ä¸ªå…³é”®è¯
        key_words = keywords[:5]
        return '_'.join(key_words) if key_words else 'general_task'
    
    def _create_reasoning_paths_from_analysis(self, dimension_result: Dict[str, Any], 
                                             user_query: str, execution_context: Optional[Dict]) -> List[ReasoningPath]:
        """åŸºäºDeepSeekåˆ†æç»“æœåˆ›å»ºæ€ç»´è·¯å¾„"""
        
        reasoning_paths = []
        task_analysis = dimension_result.get('task_analysis', {})
        domain = task_analysis.get('domain', 'general')
        complexity = task_analysis.get('complexity', 0.5)
        
        # åŸºäºä»»åŠ¡ç‰¹æ€§ç”Ÿæˆä¸åŒçš„æ€ç»´è·¯å¾„
        if complexity < 0.3:
            # ç®€å•ä»»åŠ¡ï¼šä½¿ç”¨ç›´æ¥å®ç”¨çš„è·¯å¾„
            reasoning_paths.append(ReasoningPath(
                path_id=f"practical_simple_{domain}_v1",
                path_type="å®ç”¨ç›´æ¥å‹",
                description="é€‚ç”¨äºç®€å•ä»»åŠ¡çš„ç›´æ¥å®ç”¨æ–¹æ³•",
                prompt_template="è¯·ç›´æ¥æä¾›è§£å†³æ–¹æ¡ˆï¼š{task}ã€‚è¦æ±‚ï¼š1) ç®€æ´æ˜äº† 2) ç«‹å³å¯ç”¨"
            ))
        elif complexity > 0.7:
            # å¤æ‚ä»»åŠ¡ï¼šä½¿ç”¨ç³»ç»Ÿåˆ†æè·¯å¾„
            reasoning_paths.append(ReasoningPath(
                path_id=f"systematic_complex_{domain}_v1", 
                path_type="ç³»ç»Ÿåˆ†æå‹",
                description="é€‚ç”¨äºå¤æ‚ä»»åŠ¡çš„ç³»ç»Ÿæ€§åˆ†ææ–¹æ³•",
                prompt_template="è¯·ç³»ç»Ÿæ€§åˆ†æä»»åŠ¡ï¼š{task}ã€‚æ­¥éª¤ï¼š1) åˆ†è§£é—®é¢˜ 2) åˆ†æä¾èµ– 3) åˆ¶å®šæ–¹æ¡ˆ 4) è¯„ä¼°é£é™©"
            ))
            reasoning_paths.append(ReasoningPath(
                path_id=f"creative_complex_{domain}_v1",
                path_type="åˆ›æ–°çªç ´å‹", 
                description="é€‚ç”¨äºå¤æ‚ä»»åŠ¡çš„åˆ›æ–°æ€ç»´æ–¹æ³•",
                prompt_template="è¯·åˆ›æ–°æ€§è§£å†³ï¼š{task}ã€‚è¦æ±‚ï¼š1) è·³å‡ºä¼ ç»Ÿæ€è·¯ 2) å¯»æ‰¾çªç ´ç‚¹ 3) æä¾›æ–°é¢–æ–¹æ¡ˆ"
            ))
        else:
            # ä¸­ç­‰å¤æ‚åº¦ï¼šå¹³è¡¡çš„æ–¹æ³•
            reasoning_paths.append(ReasoningPath(
                path_id=f"balanced_moderate_{domain}_v1",
                path_type="å¹³è¡¡ç»¼åˆå‹",
                description="é€‚ç”¨äºä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡çš„å¹³è¡¡æ–¹æ³•",
                prompt_template="è¯·ç»¼åˆåˆ†æè§£å†³ï¼š{task}ã€‚æ–¹æ³•ï¼š1) ç†è§£éœ€æ±‚ 2) è¯„ä¼°æ–¹æ¡ˆ 3) å®æ–½å»ºè®®"
            ))
        
        # æ€»æ˜¯æ·»åŠ ä¸€ä¸ªæ‰¹åˆ¤åˆ†æçš„è·¯å¾„ä½œä¸ºå¤‡é€‰
        reasoning_paths.append(ReasoningPath(
            path_id=f"critical_analysis_{domain}_v1",
            path_type="æ‰¹åˆ¤åˆ†æå‹",
            description="è´¨ç–‘å‡è®¾ã€æ·±åº¦åˆ†æçš„æ‰¹åˆ¤æ€§æ€ç»´æ–¹æ³•",
            prompt_template="è¯·æ‰¹åˆ¤æ€§åˆ†æï¼š{task}ã€‚è¦æ±‚ï¼š1) è´¨ç–‘åŸºæœ¬å‡è®¾ 2) åˆ†ææ½œåœ¨é—®é¢˜ 3) æä¾›æ”¹è¿›å»ºè®®"
        ))
        
        return reasoning_paths
    
    def _create_fallback_reasoning_paths(self, user_query: str, execution_context: Optional[Dict], error_msg: str) -> List[ReasoningPath]:
        """åˆ›å»ºå›é€€æ€ç»´è·¯å¾„"""
        
        logger.warning(f"ğŸ”„ ä½¿ç”¨å›é€€æ€ç»´è·¯å¾„åˆ›å»º: {error_msg}")
        
        # æä¾›åŸºç¡€çš„é€šç”¨æ€ç»´è·¯å¾„
        return [
            ReasoningPath(
                path_id="fallback_systematic_v1",
                path_type="ç³»ç»Ÿæ–¹æ³•å‹",
                description="é€šç”¨çš„ç³»ç»Ÿæ€§æ–¹æ³•ï¼ˆå›é€€ï¼‰",
                prompt_template="è¯·ç³»ç»Ÿæ€§å¤„ç†ä»¥ä¸‹ä»»åŠ¡ï¼š{task}ã€‚æ­¥éª¤ï¼š1) åˆ†æ 2) è®¡åˆ’ 3) æ‰§è¡Œ"
            ),
            ReasoningPath(
                path_id="fallback_practical_v1", 
                path_type="å®ç”¨è§£å†³å‹",
                description="é€šç”¨çš„å®ç”¨è§£å†³æ–¹æ³•ï¼ˆå›é€€ï¼‰",
                prompt_template="è¯·æä¾›å®ç”¨è§£å†³æ–¹æ¡ˆï¼š{task}ã€‚è¦æ±‚ï¼šç®€å•å¯è¡Œï¼Œç«‹å³å®æ–½"
            )
        ]


class ReasoningPathTemplates:
    """é¢„å®šä¹‰çš„æ€ç»´è·¯å¾„æ¨¡æ¿åº“ - æ¶µç›–å¤šç§æ€è€ƒèŒƒå¼"""
    
    @staticmethod
    def get_all_templates() -> Dict[str, ReasoningPath]:
        """è·å–æ‰€æœ‰é¢„å®šä¹‰çš„è·¯å¾„æ¨¡æ¿"""
        return {
            # ç³»ç»Ÿæ€§æ€ç»´
            "systematic_analytical": ReasoningPath(
                path_id="systematic_analytical_v1",
                path_type="ç³»ç»Ÿåˆ†æå‹",
                description="ç³»ç»Ÿæ€§åˆ†è§£å’Œåˆ†æé—®é¢˜ï¼Œé€‚ç”¨äºå¤æ‚ä»»åŠ¡å’ŒæŠ€æœ¯é—®é¢˜",
                prompt_template="""è¯·ç³»ç»Ÿæ€§åˆ†æä»»åŠ¡ï¼š{task}

ğŸ” **åˆ†ææ­¥éª¤**:
1. **é—®é¢˜åˆ†è§£**: å°†å¤æ‚é—®é¢˜æ‹†åˆ†ä¸ºå¯ç®¡ç†çš„å­é—®é¢˜
2. **å…³é”®è¦ç´ è¯†åˆ«**: æ‰¾å‡ºå½±å“æˆåŠŸçš„å…³é”®å› ç´ 
3. **ä¾èµ–å…³ç³»åˆ†æ**: åˆ†æå„éƒ¨åˆ†ä¹‹é—´çš„å…³è”å’Œä¾èµ–
4. **é£é™©è¯„ä¼°**: è¯†åˆ«æ½œåœ¨é£é™©å’ŒæŒ‘æˆ˜
5. **è§£å†³æ–¹æ¡ˆè®¾è®¡**: åŸºäºåˆ†æåˆ¶å®šç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆ

åŸºäºæ€ç»´ç§å­ï¼š{thinking_seed}
è¯·æä¾›ç»“æ„åŒ–ã€ç³»ç»Ÿæ€§çš„åˆ†æå’Œè§£å†³æ–¹æ¡ˆã€‚"""
            ),
            
            # åˆ›æ–°æ€§æ€ç»´
            "creative_innovative": ReasoningPath(
                path_id="creative_innovative_v1", 
                path_type="åˆ›æ–°çªç ´å‹",
                description="è·³å‡ºä¼ ç»Ÿæ€è·¯ï¼Œå¯»æ±‚åˆ›æ–°å’Œçªç ´ï¼Œé€‚ç”¨äºéœ€è¦åˆ›æ„çš„ä»»åŠ¡",
                prompt_template="""è¯·åˆ›æ–°æ€§è§£å†³ä»»åŠ¡ï¼š{task}

ğŸ’¡ **åˆ›æ–°æ–¹æ³•**:
1. **æ‰“ç ´å¸¸è§„**: è´¨ç–‘ä¼ ç»Ÿæ–¹æ³•å’Œå‡è®¾
2. **è·¨é¢†åŸŸæ€è€ƒ**: ä»å…¶ä»–é¢†åŸŸå¯»æ‰¾çµæ„Ÿå’Œæ–¹æ³•
3. **é€†å‘æ€ç»´**: è€ƒè™‘åå‘æˆ–éå¸¸è§„çš„è§£å†³è·¯å¾„  
4. **ç»„åˆåˆ›æ–°**: å°†ä¸åŒæ¦‚å¿µã€æŠ€æœ¯æˆ–æ–¹æ³•è¿›è¡Œåˆ›æ–°ç»„åˆ
5. **æœªæ¥å‰ç»**: è€ƒè™‘æ–°å…´æŠ€æœ¯å’Œè¶‹åŠ¿çš„åº”ç”¨

åŸºäºæ€ç»´ç§å­ï¼š{thinking_seed}
è¯·æä¾›åˆ›æ–°ã€ç‹¬ç‰¹ä¸”å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚"""
            ),
            
            # æ‰¹åˆ¤æ€§æ€ç»´
            "critical_questioning": ReasoningPath(
                path_id="critical_questioning_v1",
                path_type="æ‰¹åˆ¤è´¨ç–‘å‹", 
                description="æ·±åº¦è´¨ç–‘å’Œæ‰¹åˆ¤åˆ†æï¼Œé€‚ç”¨äºéœ€è¦ä¸¥è°¨è®ºè¯çš„ä»»åŠ¡",
                prompt_template="""è¯·æ‰¹åˆ¤æ€§åˆ†æä»»åŠ¡ï¼š{task}

ğŸ¤” **æ‰¹åˆ¤è¦ç‚¹**:
1. **å‡è®¾è´¨ç–‘**: è´¨ç–‘åŸºæœ¬å‡è®¾å’Œå‰ææ¡ä»¶
2. **è¯æ®è¯„ä¼°**: åˆ†æç°æœ‰è¯æ®çš„å¯é æ€§å’Œå……åˆ†æ€§
3. **é€»è¾‘æ£€éªŒ**: æ£€æŸ¥æ¨ç†è¿‡ç¨‹çš„é€»è¾‘ä¸¥å¯†æ€§
4. **å¤šè§’åº¦å®¡è§†**: ä»ä¸åŒç«‹åœºå’Œè§’åº¦å®¡è§†é—®é¢˜
5. **åé©³è®ºè¯**: è€ƒè™‘å¯èƒ½çš„åå¯¹æ„è§å’Œåé©³

åŸºäºæ€ç»´ç§å­ï¼š{thinking_seed}
è¯·æä¾›ä¸¥è°¨çš„æ‰¹åˆ¤æ€§åˆ†æå’Œè®ºè¯ã€‚"""
            ),
            
            # å®ç”¨æ€§æ€ç»´
            "practical_pragmatic": ReasoningPath(
                path_id="practical_pragmatic_v1",
                path_type="å®ç”¨åŠ¡å®å‹",
                description="æ³¨é‡å®é™…å¯è¡Œæ€§å’Œç«‹å³æ‰§è¡Œï¼Œé€‚ç”¨äºéœ€è¦å¿«é€Ÿè§£å†³çš„å®é™…é—®é¢˜",
                prompt_template="""è¯·åŠ¡å®åœ°è§£å†³ä»»åŠ¡ï¼š{task}

âš¡ **å®ç”¨ç­–ç•¥**:
1. **å¿«é€Ÿå¯è¡Œ**: ä¼˜å…ˆè€ƒè™‘ç«‹å³å¯å®æ–½çš„æ–¹æ¡ˆ
2. **èµ„æºçº¦æŸ**: åœ¨ç°æœ‰èµ„æºå’Œé™åˆ¶ä¸‹å¯»æ‰¾è§£å†³æ–¹æ¡ˆ
3. **é£é™©å¯æ§**: é€‰æ‹©é£é™©ä½ã€æˆåŠŸç‡é«˜çš„æ–¹æ³•
4. **æ•ˆæœå¯¼å‘**: ä¸“æ³¨äºèƒ½äº§ç”Ÿå®é™…æ•ˆæœçš„è¡ŒåŠ¨
5. **è¿­ä»£æ”¹è¿›**: é‡‡ç”¨å°æ­¥å¿«è·‘ã€æŒç»­æ”¹è¿›çš„æ–¹å¼

åŸºäºæ€ç»´ç§å­ï¼š{thinking_seed}
è¯·æä¾›ç®€å•ç›´æ¥ã€ç«‹å³å¯è¡Œçš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚"""
            ),
            
            # æ•´ä½“æ€§æ€ç»´
            "holistic_comprehensive": ReasoningPath(
                path_id="holistic_comprehensive_v1",
                path_type="æ•´ä½“ç»¼åˆå‹",
                description="ä»å…¨å±€å’Œæ•´ä½“è§’åº¦è€ƒè™‘é—®é¢˜ï¼Œé€‚ç”¨äºéœ€è¦å¹³è¡¡å¤šæ–¹å› ç´ çš„å¤æ‚æƒ…å†µ",
                prompt_template="""è¯·æ•´ä½“æ€§åˆ†æä»»åŠ¡ï¼š{task}

ğŸŒ **æ•´ä½“è§†è§’**:
1. **å…¨å±€è€ƒé‡**: ä»æ›´å¤§çš„èƒŒæ™¯å’Œç¯å¢ƒä¸­ç†è§£é—®é¢˜
2. **å¤šå…ƒå¹³è¡¡**: å¹³è¡¡ä¸åŒåˆ©ç›Šç›¸å…³è€…çš„éœ€æ±‚å’Œå…³åˆ‡
3. **é•¿è¿œå½±å“**: è€ƒè™‘å†³ç­–çš„é•¿æœŸå½±å“å’Œåæœ
4. **ç³»ç»Ÿäº’åŠ¨**: ç†è§£å„éƒ¨åˆ†ä¹‹é—´çš„å¤æ‚äº’åŠ¨å…³ç³»
5. **ç»¼åˆæƒè¡¡**: ç»¼åˆè€ƒè™‘å„ç§å› ç´ ï¼Œå¯»æ‰¾æœ€ä½³å¹³è¡¡ç‚¹

åŸºäºæ€ç»´ç§å­ï¼š{thinking_seed}
è¯·æä¾›å…¨é¢ã€å¹³è¡¡çš„æ•´ä½“æ€§åˆ†æå’Œå»ºè®®ã€‚"""
            ),
            
            # æ¢ç´¢æ€§æ€ç»´
            "exploratory_investigative": ReasoningPath(
                path_id="exploratory_investigative_v1",
                path_type="æ¢ç´¢è°ƒç ”å‹",
                description="æ·±å…¥è°ƒç ”å’Œæ¢ç´¢æœªçŸ¥é¢†åŸŸï¼Œé€‚ç”¨äºç ”ç©¶æ€§å’Œå­¦ä¹ æ€§ä»»åŠ¡",
                prompt_template="""è¯·æ¢ç´¢æ€§ç ”ç©¶ä»»åŠ¡ï¼š{task}

ğŸ”¬ **æ¢ç´¢æ–¹æ³•**:
1. **æ·±åº¦è°ƒç ”**: å¹¿æ³›æ”¶é›†å’Œåˆ†æç›¸å…³ä¿¡æ¯
2. **å¤šæºéªŒè¯**: ä»å¤šä¸ªæ¥æºéªŒè¯ä¿¡æ¯çš„å‡†ç¡®æ€§
3. **æ¨¡å¼è¯†åˆ«**: å¯»æ‰¾æ•°æ®ä¸­çš„æ¨¡å¼å’Œè§„å¾‹
4. **å‡è®¾éªŒè¯**: æå‡ºå‡è®¾å¹¶è®¾è®¡éªŒè¯æ–¹æ³•
5. **çŸ¥è¯†æ•´åˆ**: å°†å‘ç°æ•´åˆä¸ºç³»ç»Ÿæ€§çš„ç†è§£

åŸºäºæ€ç»´ç§å­ï¼š{thinking_seed}
è¯·æä¾›æ·±å…¥ã€å…¨é¢çš„æ¢ç´¢æ€§åˆ†æå’Œå‘ç°ã€‚"""
            ),
            
            # åä½œæ€§æ€ç»´
            "collaborative_consultative": ReasoningPath(
                path_id="collaborative_consultative_v1",
                path_type="åä½œå’¨è¯¢å‹",
                description="è€ƒè™‘å¤šæ–¹å‚ä¸å’Œåä½œï¼Œé€‚ç”¨äºéœ€è¦å›¢é˜Ÿåˆä½œçš„ä»»åŠ¡",
                prompt_template="""è¯·åä½œæ€§è§£å†³ä»»åŠ¡ï¼š{task}

ğŸ¤ **åä½œç­–ç•¥**:
1. **åˆ©ç›Šç›¸å…³è€…åˆ†æ**: è¯†åˆ«å…³é”®å‚ä¸è€…å’Œåˆ©ç›Šç›¸å…³è€…
2. **æ²Ÿé€šæœºåˆ¶è®¾è®¡**: å»ºç«‹æœ‰æ•ˆçš„æ²Ÿé€šå’Œåè°ƒæœºåˆ¶
3. **å…±è¯†å»ºç«‹**: å¯»æ‰¾å„æ–¹éƒ½èƒ½æ¥å—çš„è§£å†³æ–¹æ¡ˆ
4. **åˆ†å·¥åä½œ**: åˆç†åˆ†é…ä»»åŠ¡å’Œè´£ä»»
5. **å†²çªè§£å†³**: é¢„è§å¹¶è§£å†³å¯èƒ½çš„å†²çªå’Œåˆ†æ­§

åŸºäºæ€ç»´ç§å­ï¼š{thinking_seed}
è¯·æä¾›ä¿ƒè¿›åä½œã€å»ºç«‹å…±è¯†çš„è§£å†³æ–¹æ¡ˆã€‚"""
            ),
            
            # é€‚åº”æ€§æ€ç»´  
            "adaptive_flexible": ReasoningPath(
                path_id="adaptive_flexible_v1",
                path_type="é€‚åº”çµæ´»å‹",
                description="çµæ´»é€‚åº”å˜åŒ–ï¼Œé€‚ç”¨äºä¸ç¡®å®šæ€§é«˜çš„åŠ¨æ€ç¯å¢ƒ",
                prompt_template="""è¯·çµæ´»åœ°åº”å¯¹ä»»åŠ¡ï¼š{task}

ğŸ”„ **é€‚åº”ç­–ç•¥**:
1. **æƒ…å†µè¯„ä¼°**: åˆ†æå½“å‰æƒ…å†µçš„ä¸ç¡®å®šæ€§å’Œå˜åŒ–æ€§
2. **å¤šæ–¹æ¡ˆå‡†å¤‡**: å‡†å¤‡å¤šä¸ªå¤‡é€‰æ–¹æ¡ˆä»¥åº”å¯¹ä¸åŒæƒ…å†µ
3. **æ•æ·å“åº”**: å»ºç«‹å¿«é€Ÿå“åº”å’Œè°ƒæ•´çš„æœºåˆ¶
4. **å­¦ä¹ è¿­ä»£**: ä»å®è·µä¸­å­¦ä¹ å¹¶æŒç»­è°ƒæ•´ç­–ç•¥
5. **å¼¹æ€§è®¾è®¡**: è®¾è®¡å…·æœ‰å¼¹æ€§å’ŒéŸ§æ€§çš„è§£å†³æ–¹æ¡ˆ

åŸºäºæ€ç»´ç§å­ï¼š{thinking_seed}
è¯·æä¾›çµæ´»ã€å¯é€‚åº”çš„è§£å†³æ–¹æ¡ˆã€‚"""
            )
        }
    
    # ğŸ—‘ï¸ å·²åˆ é™¤ï¼šå…³é”®è¯æ˜ å°„æ–¹æ³• - æ”¹ç”¨LLMæ™ºèƒ½åˆ†æ
    # åŸæœ‰çš„ get_keyword_mapping() æ–¹æ³•å·²è¢« LLM è‡ªç„¶è¯­è¨€åˆ†ææ›¿ä»£
    # è¿™å¤§å¤§æé«˜äº†è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œæ— éœ€ç»´æŠ¤é™æ€å…³é”®è¯åˆ—è¡¨


@dataclass  
class PathGenerator:
    """è·¯å¾„ç”Ÿæˆå™¨ - åŸºäºæ€ç»´ç§å­ç”Ÿæˆå¤šæ ·åŒ–æ€ç»´è·¯å¾„ (é˜¶æ®µäºŒ)"""
    
    def __init__(self, api_key: str = "", llm_client=None):
        """
        åˆå§‹åŒ–è·¯å¾„ç”Ÿæˆå™¨
        
        Args:
            api_key: APIå¯†é’¥ï¼ˆå‘åå…¼å®¹ï¼‰
            llm_client: å…±äº«çš„LLMå®¢æˆ·ç«¯ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.api_key = api_key
        
        # ğŸ”§ ä¾èµ–æ³¨å…¥ï¼šä½¿ç”¨ä¼ å…¥çš„LLMå®¢æˆ·ç«¯ï¼ˆçº¯ä¾èµ–æ³¨å…¥æ¨¡å¼ï¼‰
        self.llm_analyzer = llm_client
        if self.llm_analyzer:
            logger.info("ğŸ§  LLMæ€ç»´ç§å­åˆ†æå™¨å·²å¯ç”¨ (ä½¿ç”¨å…±äº«å®¢æˆ·ç«¯)")
        else:
            logger.warning("âš ï¸ æœªæä¾›LLMå®¢æˆ·ç«¯ï¼Œæ€ç»´ç§å­åˆ†æå°†æ— æ³•ä½¿ç”¨AIåŠŸèƒ½")
            logger.info("ğŸ’¡ è¯·ç¡®ä¿ä»ä¸Šå±‚ï¼ˆMainControllerï¼‰ä¼ å…¥æœ‰æ•ˆçš„llm_client")
        
        # ğŸ”§ ä¾èµ–æ³¨å…¥ï¼šä¸ºç»´åº¦åˆ›å»ºå™¨ä¼ å…¥å…±äº«å®¢æˆ·ç«¯
        self.dimension_selector = None
        if llm_client:
            self.dimension_selector = LLMDrivenDimensionCreator(api_key, llm_client=llm_client)
        else:
            logger.warning("âš ï¸ ç»´åº¦åˆ›å»ºå™¨æ— æ³•åˆå§‹åŒ–ï¼Œç¼ºå°‘LLMå®¢æˆ·ç«¯")
        
        self.generation_cache = {}
        
        # æ–°å¢ï¼šæ€ç»´è·¯å¾„ç›¸å…³ç¼“å­˜å’Œç»Ÿè®¡
        self.path_generation_cache = {}
        self.path_templates = ReasoningPathTemplates.get_all_templates()
        # åˆ é™¤å…³é”®è¯æ˜ å°„ï¼Œæ”¹ç”¨LLMåˆ†æ
        # self.keyword_mapping = ReasoningPathTemplates.get_keyword_mapping()
        self.path_selection_stats = defaultdict(int)
        
        logger.info("ğŸ›¤ï¸ PathGenerator å·²åˆå§‹åŒ– (æ”¯æŒLLMå¢å¼ºçš„æ€ç»´ç§å­â†’è·¯å¾„ç”Ÿæˆ)")
        
    def generate_paths(self, thinking_seed: str, task: str = "", max_paths: int = 4, mode: str = 'normal') -> List[ReasoningPath]:
        """
        é˜¶æ®µäºŒæ ¸å¿ƒæ–¹æ³•ï¼šåŸºäºæ€ç»´ç§å­ç”Ÿæˆå¤šæ ·åŒ–æ€ç»´è·¯å¾„åˆ—è¡¨
        
        Args:
            thinking_seed: æ¥è‡ªé˜¶æ®µä¸€çš„æ€ç»´ç§å­
            task: åŸå§‹ä»»åŠ¡æè¿° (ç”¨äºå¡«å……è·¯å¾„æ¨¡æ¿)
            max_paths: æœ€å¤§ç”Ÿæˆè·¯å¾„æ•°
            mode: ç”Ÿæˆæ¨¡å¼ ('normal' | 'creative_bypass')
            
        Returns:
            å¤šæ ·åŒ–çš„æ€ç»´è·¯å¾„åˆ—è¡¨
        """
        # ğŸ’¡ Aha-Momentå†³ç­–ï¼šcreative_bypassæ¨¡å¼è·³è¿‡ç¼“å­˜ï¼Œç¡®ä¿åˆ›é€ æ€§
        use_cache = (mode != 'creative_bypass')
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"paths_{hash(thinking_seed)}_{hash(task)}_{max_paths}_{mode}"
        if use_cache and cache_key in self.path_generation_cache:
            logger.debug(f"ğŸ¯ ä½¿ç”¨ç¼“å­˜çš„è·¯å¾„ç”Ÿæˆ: {cache_key[:20]}...")
            return self.path_generation_cache[cache_key]
        
        if mode == 'creative_bypass':
            logger.info(f"ğŸ’¡ Aha-Momentåˆ›é€ æ€§ç»•é“æ¨¡å¼: {thinking_seed[:50]}...")
        else:
            logger.info(f"ğŸŒ± å¼€å§‹åŸºäºæ€ç»´ç§å­ç”Ÿæˆè·¯å¾„: {thinking_seed[:50]}...")
        
        try:
            # åˆ†ææ€ç»´ç§å­ï¼Œè¯†åˆ«å…³é”®ä¿¡æ¯
            seed_analysis = self._analyze_thinking_seed(thinking_seed)
            logger.debug(f"ğŸ” ç§å­åˆ†æç»“æœ: {seed_analysis}")
            
            # ğŸ’¡ æ ¹æ®æ¨¡å¼é€‰æ‹©è·¯å¾„ç±»å‹ç­–ç•¥
            if mode == 'creative_bypass':
                # Aha-Momentæ¨¡å¼ï¼šä¼˜å…ˆé€‰æ‹©åˆ›é€ æ€§å’Œçªç ´æ€§è·¯å¾„ç±»å‹
                selected_path_types = self._select_creative_bypass_path_types(seed_analysis, max_paths)
                logger.info(f"ğŸŒŸ åˆ›é€ æ€§ç»•é“è·¯å¾„ç±»å‹: {selected_path_types}")
            else:
                # å¸¸è§„æ¨¡å¼ï¼šæ ¹æ®åˆ†æç»“æœé€‰æ‹©åˆé€‚çš„è·¯å¾„æ¨¡æ¿
                selected_path_types = self._select_path_types(seed_analysis, max_paths)
                logger.info(f"ğŸ“‹ é€‰æ‹©çš„è·¯å¾„ç±»å‹: {selected_path_types}")
            
            # ç”Ÿæˆå…·ä½“çš„æ€ç»´è·¯å¾„å®ä¾‹
            reasoning_paths = self._instantiate_reasoning_paths(
                selected_path_types, thinking_seed, task
            )
            
            # ç¼“å­˜ç»“æœï¼ˆcreative_bypassæ¨¡å¼ä¸‹å¯ä»¥ç¼“å­˜ï¼Œä½†ç¼“å­˜é”®åŒ…å«æ¨¡å¼ä¿¡æ¯ï¼‰
            if use_cache or mode == 'creative_bypass':
                self.path_generation_cache[cache_key] = reasoning_paths
                self._manage_path_cache()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            for path in reasoning_paths:
                self.path_selection_stats[path.path_type] += 1
            
            logger.info(f"âœ… ç”Ÿæˆ {len(reasoning_paths)} æ¡æ€ç»´è·¯å¾„")
            return reasoning_paths
            
        except Exception as e:
            logger.error(f"âŒ æ€ç»´è·¯å¾„ç”Ÿæˆå¤±è´¥: {e}")
            # fallbackåˆ°é»˜è®¤è·¯å¾„
            return self._generate_fallback_paths(thinking_seed, task)
    
    def _analyze_thinking_seed(self, thinking_seed: str) -> Dict[str, Any]:
        """
         LLMå¢å¼ºçš„æ€ç»´ç§å­åˆ†æ - æ›¿ä»£å…³é”®è¯åŒ¹é…çš„æ™ºèƒ½åˆ†æ
        
        Args:
            thinking_seed: æ€ç»´ç§å­å­—ç¬¦ä¸²
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        logger.debug(f" å¼€å§‹LLMåˆ†ææ€ç»´ç§å­: {thinking_seed[:50]}...")
        
        # å¦‚æœLLMåˆ†æå™¨å¯ç”¨ï¼Œä½¿ç”¨æ™ºèƒ½åˆ†æ
        if self.llm_analyzer:
            try:
                return self._llm_analyze_thinking_seed(thinking_seed)
            except Exception as e:
                logger.warning(f"âš ï¸ LLMåˆ†æå¤±è´¥ï¼Œå›é€€åˆ°å¯å‘å¼åˆ†æ: {e}")
                return self._heuristic_analyze_thinking_seed(thinking_seed)
        else:
            logger.info("ğŸ”„ LLMåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨å¯å‘å¼åˆ†æ")
            return self._heuristic_analyze_thinking_seed(thinking_seed)
    
    def _llm_analyze_thinking_seed(self, thinking_seed: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ€ç»´ç§å­åˆ†æ
        
        Args:
            thinking_seed: æ€ç»´ç§å­
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # æ„å»ºLLMåˆ†ææç¤º
        analysis_prompt = self._build_seed_analysis_prompt(thinking_seed)
        
        # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
        llm_response = self.llm_analyzer.call_api(
            prompt=analysis_prompt,
            temperature=0.3,  # è¾ƒä½æ¸©åº¦ç¡®ä¿ç¨³å®šæ€§
            system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€ç»´æ¨¡å¼åˆ†æå¸ˆï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«æ–‡æœ¬ä¸­çš„æ€è€ƒç‰¹å¾å’Œéœ€æ±‚ã€‚"
        )
        
        # è§£æLLMå“åº”
        analysis_result = self._parse_llm_analysis_response(llm_response)
        
        logger.debug(f"âœ… LLMåˆ†æå®Œæˆ: {len(analysis_result['path_relevance'])}ä¸ªè·¯å¾„ç±»å‹è¢«è¯„ä¼°")
        return analysis_result
    
    def _build_seed_analysis_prompt(self, thinking_seed: str) -> str:
        """
        æ„å»ºæ€ç»´ç§å­åˆ†æçš„LLMæç¤º
        
        Args:
            thinking_seed: æ€ç»´ç§å­
            
        Returns:
            æ ¼å¼åŒ–çš„æç¤ºå­—ç¬¦ä¸²
        """
        # è·å–æ‰€æœ‰å¯ç”¨çš„æ€ç»´è·¯å¾„ç±»å‹åŠå…¶æè¿°
        path_descriptions = {
            "systematic_analytical": "ç³»ç»Ÿæ–¹æ³•å‹ - é€»è¾‘åˆ†æã€ç»“æ„åŒ–æ€è€ƒã€å·¥ç¨‹æ–¹æ³•",
            "creative_innovative": "åˆ›æ–°ç›´è§‰å‹ - åˆ›é€ æ€§æ€ç»´ã€çªç ´å¸¸è§„ã€è‰ºæœ¯çµæ„Ÿ",
            "critical_questioning": "æ‰¹åˆ¤è´¨ç–‘å‹ - è´¨ç–‘åˆ†æã€é£é™©è¯„ä¼°ã€å®¡è§†æ£€éªŒ",
            "practical_pragmatic": "å®ç”¨å¯¼å‘å‹ - å®é™…å¯è¡Œã€ç®€å•ç›´æ¥ã€æ•ˆç‡ä¼˜å…ˆ",
            "holistic_comprehensive": "ç»¼åˆå…¨é¢å‹ - æ•´ä½“è€ƒè™‘ã€å…¨å±€æ€ç»´ã€å¹³è¡¡åè°ƒ",
            "exploratory_investigative": "æ¢ç´¢ç ”ç©¶å‹ - ç ”ç©¶å­¦ä¹ ã€æ·±å…¥è°ƒæŸ¥ã€çŸ¥è¯†è·å–",
            "collaborative_consultative": "åä½œå’¨è¯¢å‹ - å›¢é˜Ÿåˆä½œã€æ²Ÿé€šäº¤æµã€é›†ä½“æ™ºæ…§",
            "adaptive_flexible": "é€‚åº”çµæ´»å‹ - çµæ´»åº”å˜ã€åŠ¨æ€è°ƒæ•´ã€æ•æ·å“åº”"
        }
        
        path_list = "\n".join([f"- {key}: {desc}" for key, desc in path_descriptions.items()])
        
        prompt = f"""
ä½œä¸ºæ€ç»´æ¨¡å¼åˆ†æä¸“å®¶ï¼Œè¯·æ·±åº¦åˆ†æä»¥ä¸‹æ€ç»´ç§å­ï¼Œè¯†åˆ«å…¶æ€è€ƒç‰¹å¾å’Œéœ€æ±‚ã€‚

 **å¾…åˆ†æçš„æ€ç»´ç§å­**:
{thinking_seed}

 **åˆ†æä»»åŠ¡**:
1. è¯„ä¼°è¿™ä¸ªæ€ç»´ç§å­ä¸ä»¥ä¸‹8ç§æ€ç»´è·¯å¾„ç±»å‹çš„ç›¸å…³ç¨‹åº¦ï¼ˆ0.0-1.0è¯„åˆ†ï¼‰
2. è¯†åˆ«æ€ç»´ç§å­ä¸­ä½“ç°çš„ç‰¹æ®Šéœ€æ±‚å’Œç‰¹å¾
3. åˆ¤æ–­é—®é¢˜çš„å¤æ‚åº¦å’Œç´§æ€¥ç¨‹åº¦

 **æ€ç»´è·¯å¾„ç±»å‹è¯´æ˜**:
{path_list}

 **è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼ä¸¥æ ¼è¿”å›åˆ†æç»“æœ**:
{{
    "path_relevance": {{
        "systematic_analytical": 0.0åˆ°1.0çš„ç›¸å…³åº¦è¯„åˆ†,
        "creative_innovative": 0.0åˆ°1.0çš„ç›¸å…³åº¦è¯„åˆ†,
        "critical_questioning": 0.0åˆ°1.0çš„ç›¸å…³åº¦è¯„åˆ†,
        "practical_pragmatic": 0.0åˆ°1.0çš„ç›¸å…³åº¦è¯„åˆ†,
        "holistic_comprehensive": 0.0åˆ°1.0çš„ç›¸å…³åº¦è¯„åˆ†,
        "exploratory_investigative": 0.0åˆ°1.0çš„ç›¸å…³åº¦è¯„åˆ†,
        "collaborative_consultative": 0.0åˆ°1.0çš„ç›¸å…³åº¦è¯„åˆ†,
        "adaptive_flexible": 0.0åˆ°1.0çš„ç›¸å…³åº¦è¯„åˆ†
    }},
    "characteristics": {{
        "urgency_level": "low|normal|high",
        "collaborative_need": trueæˆ–false,
        "innovation_requirement": trueæˆ–false,
        "critical_analysis_need": trueæˆ–false,
        "practical_focus": trueæˆ–false,
        "comprehensive_scope": trueæˆ–false,
        "research_oriented": trueæˆ–false,
        "adaptive_requirement": trueæˆ–false
    }},
    "complexity_assessment": {{
        "complexity_score": 0.0åˆ°1.0çš„å¤æ‚åº¦è¯„åˆ†,
        "complexity_indicators": ["æŒ‡æ ‡1", "æŒ‡æ ‡2", "æŒ‡æ ‡3"],
        "domain_hints": ["é¢†åŸŸ1", "é¢†åŸŸ2"]
    }},
    "reasoning": "ç®€è¦è§£é‡Šä½ çš„åˆ†æä¾æ®å’Œæ€è·¯"
}}

è¯·åŸºäºæ€ç»´ç§å­çš„è¯­ä¹‰å†…å®¹ã€éšå«éœ€æ±‚å’Œä¸Šä¸‹æ–‡è¿›è¡Œæ·±åº¦åˆ†æï¼Œè€Œä¸æ˜¯ç®€å•çš„å…³é”®è¯åŒ¹é…ã€‚
"""
        return prompt.strip()
    
    def _parse_llm_analysis_response(self, llm_response: str) -> Dict[str, Any]:
        """
        è§£æLLMçš„åˆ†æå“åº”ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        
        Args:
            llm_response: LLMå“åº”å­—ç¬¦ä¸²
            
        Returns:
            æ ‡å‡†æ ¼å¼çš„åˆ†æç»“æœ
        """
        try:
            # å°è¯•è§£æJSONå“åº”
            import json
            import re
            
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', llm_response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                llm_analysis = json.loads(json_str)
                
                # è½¬æ¢ä¸ºå…¼å®¹åŸæœ‰æ¥å£çš„æ ¼å¼
                analysis = {
                    'path_relevance': llm_analysis.get('path_relevance', {}),
                    'complexity_indicators': llm_analysis.get('complexity_assessment', {}).get('complexity_indicators', []),
                    'domain_hints': llm_analysis.get('complexity_assessment', {}).get('domain_hints', []),
                    'complexity_score': llm_analysis.get('complexity_assessment', {}).get('complexity_score', 0.5),
                    'llm_reasoning': llm_analysis.get('reasoning', ''),
                    
                    # ç‰¹æ®Šéœ€æ±‚ - ä»LLMåˆ†æä¸­æå–
                    'urgency_level': llm_analysis.get('characteristics', {}).get('urgency_level', 'normal'),
                    'collaborative_need': llm_analysis.get('characteristics', {}).get('collaborative_need', False),
                    'innovation_requirement': llm_analysis.get('characteristics', {}).get('innovation_requirement', False),
                    'critical_analysis_need': llm_analysis.get('characteristics', {}).get('critical_analysis_need', False),
                    'practical_focus': llm_analysis.get('characteristics', {}).get('practical_focus', False),
                    'comprehensive_scope': llm_analysis.get('characteristics', {}).get('comprehensive_scope', False),
                    'research_oriented': llm_analysis.get('characteristics', {}).get('research_oriented', False),
                    'adaptive_requirement': llm_analysis.get('characteristics', {}).get('adaptive_requirement', False),
                    
                    # å…¼å®¹æ€§å­—æ®µï¼šæ¨¡æ‹Ÿkeywords_foundæ ¼å¼
                    'keywords_found': self._convert_relevance_to_keywords(llm_analysis.get('path_relevance', {}))
                }
                
                return analysis
            else:
                logger.warning("âš ï¸ LLMå“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆJSONï¼Œä½¿ç”¨å¯å‘å¼åˆ†æ")
                return self._create_fallback_analysis(llm_response)
                
        except Exception as e:
            logger.error(f"âŒ è§£æLLMåˆ†æå“åº”å¤±è´¥: {e}")
            return self._create_fallback_analysis(llm_response)
    
    def _convert_relevance_to_keywords(self, path_relevance: Dict[str, float]) -> Dict[str, Dict]:
        """
        å°†LLMçš„ç›¸å…³åº¦è¯„åˆ†è½¬æ¢ä¸ºå…¼å®¹keywords_foundæ ¼å¼
        
        Args:
            path_relevance: è·¯å¾„ç›¸å…³åº¦å­—å…¸
            
        Returns:
            å…¼å®¹æ ¼å¼çš„å…³é”®è¯å­—å…¸
        """
        keywords_found = {}
        
        for path_type, relevance_score in path_relevance.items():
            if relevance_score > 0.1:  # åªä¿ç•™ç›¸å…³åº¦è¾ƒé«˜çš„è·¯å¾„
                keywords_found[path_type] = {
                    'keywords': ['llm_analyzed'],  # æ ‡è®°ä¸ºLLMåˆ†æ
                    'weight': relevance_score * 10,  # è½¬æ¢ä¸ºæƒé‡
                    'relevance_score': relevance_score
                }
        
        return keywords_found
    
    def _get_default_analysis(self, error_note: str = None) -> Dict[str, Any]:
        """
        è·å–é»˜è®¤çš„åˆ†æç»“æœ - é¿å…ä»£ç é‡å¤
        
        Args:
            error_note: é”™è¯¯è¯´æ˜ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            é»˜è®¤åˆ†æç»“æœå­—å…¸
        """
        # å‡åŒ€åˆ†é…ç›¸å…³åº¦è¯„åˆ†
        uniform_score = 0.4
        path_relevance = {
            'systematic_analytical': uniform_score,
            'creative_innovative': uniform_score,
            'critical_questioning': uniform_score,
            'practical_pragmatic': uniform_score,
            'holistic_comprehensive': uniform_score,
            'exploratory_investigative': uniform_score,
            'collaborative_consultative': uniform_score,
            'adaptive_flexible': uniform_score
        }
        
        analysis = {
            'path_relevance': path_relevance,
            'keywords_found': self._convert_relevance_to_keywords(path_relevance),
            'complexity_indicators': ['é»˜è®¤å¤æ‚åº¦'],
            'domain_hints': ['é€šç”¨é¢†åŸŸ'],
            'urgency_level': 'normal',
            'collaborative_need': False,
            'innovation_requirement': False,
            'critical_analysis_need': True,  # é»˜è®¤åŒ…å«æ‰¹åˆ¤æ€§åˆ†æ
            'practical_focus': True,       # é»˜è®¤å®ç”¨å¯¼å‘
            'comprehensive_scope': False,
            'research_oriented': False,
            'adaptive_requirement': False
        }
        
        if error_note:
            analysis['error_note'] = error_note
            
        return analysis

    def _heuristic_analyze_thinking_seed(self, thinking_seed: str) -> Dict[str, Any]:
        """
        ç®€åŒ–çš„å¤‡ç”¨åˆ†æ (LLMä¸å¯ç”¨æ—¶çš„é»˜è®¤æ–¹æ¡ˆ)
        
        Args:
            thinking_seed: æ€ç»´ç§å­
            
        Returns:
            é»˜è®¤åˆ†æç»“æœå­—å…¸
        """
        # ğŸ—‘ï¸ å·²åˆ é™¤æ‰€æœ‰å¯å‘å¼è§„åˆ™ï¼Œç›´æ¥ä½¿ç”¨é»˜è®¤åˆ†æ
        logger.info("ğŸ”„ LLMä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å‡åŒ€åˆ†é…ç­–ç•¥")
        return self._get_default_analysis()
    
    def _create_fallback_analysis(self, raw_response: str) -> Dict[str, Any]:
        """
        åˆ›å»ºé»˜è®¤çš„åˆ†æç»“æœ
        
        Args:
            raw_response: åŸå§‹å“åº”
            
        Returns:
            é»˜è®¤åˆ†æç»“æœ
        """
        error_note = f"LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®ï¼ŒåŸå§‹å“åº”é•¿åº¦: {len(raw_response)}"
        return self._get_default_analysis(error_note)
    
    def _select_path_types(self, seed_analysis: Dict[str, Any], max_paths: int) -> List[str]:
        """
        æ ¹æ®LLMåˆ†æç»“æœé€‰æ‹©åˆé€‚çš„è·¯å¾„ç±»å‹
        
        Args:
            seed_analysis: LLMåˆ†æçš„ç§å­ç»“æœ
            max_paths: æœ€å¤§è·¯å¾„æ•°
            
        Returns:
            é€‰æ‹©çš„è·¯å¾„ç±»å‹åˆ—è¡¨
        """
        path_scores = {}
        
        # 1. åŸºäºLLMç›¸å…³åº¦è¯„åˆ†çš„åŸºç¡€è¯„åˆ†ï¼ˆæ›¿ä»£å…³é”®è¯åŒ¹é…ï¼‰
        if 'path_relevance' in seed_analysis and seed_analysis['path_relevance']:
            # ä½¿ç”¨LLMçš„ç›´æ¥ç›¸å…³åº¦è¯„åˆ†
            for path_type, relevance_score in seed_analysis['path_relevance'].items():
                path_scores[path_type] = relevance_score * 10  # ç¼©æ”¾åˆ°åŸæœ‰æƒé‡èŒƒå›´
        else:
            # å›é€€åˆ°keywords_foundæ ¼å¼ï¼ˆå…¼å®¹æ€§ï¼‰
            for path_type, keyword_info in seed_analysis.get('keywords_found', {}).items():
                path_scores[path_type] = keyword_info['weight'] * keyword_info['relevance_score']
        
        # 2. åŸºäºç‰¹æ®Šéœ€æ±‚çš„åŠ æƒè°ƒæ•´
        adjustments = {
            'collaborative_consultative': seed_analysis['collaborative_need'] * 2,
            'creative_innovative': seed_analysis['innovation_requirement'] * 2,
            'critical_questioning': seed_analysis['critical_analysis_need'] * 2,
            'practical_pragmatic': seed_analysis['practical_focus'] * 2,
            'holistic_comprehensive': seed_analysis['comprehensive_scope'] * 2,
            'exploratory_investigative': seed_analysis['research_oriented'] * 2,
            'adaptive_flexible': seed_analysis['adaptive_requirement'] * 2,
            'systematic_analytical': len(seed_analysis['complexity_indicators']) * 0.5
        }
        
        for path_type, adjustment in adjustments.items():
            path_scores[path_type] = path_scores.get(path_type, 0) + adjustment
        
        # 3. ç´§æ€¥ç¨‹åº¦å½±å“è·¯å¾„é€‰æ‹©
        if seed_analysis['urgency_level'] == 'high':
            path_scores['practical_pragmatic'] = path_scores.get('practical_pragmatic', 0) + 1
        elif seed_analysis['urgency_level'] == 'low':
            path_scores['exploratory_investigative'] = path_scores.get('exploratory_investigative', 0) + 1
            path_scores['holistic_comprehensive'] = path_scores.get('holistic_comprehensive', 0) + 1
        
        # 4. ç¡®ä¿å¤šæ ·æ€§ï¼šé€‰æ‹©ä¸åŒç±»å‹çš„è·¯å¾„
        selected_paths = []
        
        # æŒ‰è¯„åˆ†æ’åº
        sorted_paths = sorted(path_scores.items(), key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„è·¯å¾„ï¼ŒåŒæ—¶ç¡®ä¿å¤šæ ·æ€§
        for path_type, score in sorted_paths:
            if len(selected_paths) >= max_paths:
                break
            if score > 0:  # åªé€‰æ‹©æœ‰ç›¸å…³æ€§çš„è·¯å¾„
                selected_paths.append(path_type)
        
        # 5. å¦‚æœé€‰æ‹©çš„è·¯å¾„ä¸è¶³ï¼Œæ·»åŠ é€šç”¨è·¯å¾„
        if len(selected_paths) < 2:
            default_paths = ['systematic_analytical', 'practical_pragmatic']
            for default_path in default_paths:
                if default_path not in selected_paths and len(selected_paths) < max_paths:
                    selected_paths.append(default_path)
        
        # 6. ç¡®ä¿æ€»æ˜¯åŒ…å«è‡³å°‘ä¸€ä¸ªæ‰¹åˆ¤æ€§è·¯å¾„ï¼ˆè´¨é‡ä¿è¯ï¼‰
        if 'critical_questioning' not in selected_paths and len(selected_paths) < max_paths:
            selected_paths.append('critical_questioning')
        
        return selected_paths[:max_paths]
    
    def _instantiate_reasoning_paths(self, path_types: List[str], thinking_seed: str, task: str) -> List[ReasoningPath]:
        """
        å®ä¾‹åŒ–é€‰æ‹©çš„æ€ç»´è·¯å¾„ - ğŸ¯ æ ¹æºä¿®å¤ï¼šåœ¨æºå¤´ç”Ÿæˆæ­£ç¡®çš„ç¡®å®šæ€§ç­–ç•¥ID
        
        Args:
            path_types: é€‰æ‹©çš„è·¯å¾„ç±»å‹åˆ—è¡¨
            thinking_seed: æ€ç»´ç§å­
            task: ä»»åŠ¡æè¿°
            
        Returns:
            å®ä¾‹åŒ–çš„æ€ç»´è·¯å¾„åˆ—è¡¨
        """
        reasoning_paths = []
        
        for path_type in path_types:
            if path_type in self.path_templates:
                template = self.path_templates[path_type]
                
                # ğŸ¯ æ ¹æºä¿®å¤ï¼šç›´æ¥ä½¿ç”¨æ¨¡æ¿é”®ä½œä¸ºç¡®å®šæ€§ç­–ç•¥ID
                strategy_id = path_type  # ä½¿ç”¨æ¨¡æ¿é”®ï¼Œç¡®ä¿ç¡®å®šæ€§å’Œå¹‚ç­‰æ€§
                
                # ç”Ÿæˆå®ä¾‹çº§åˆ«çš„å”¯ä¸€IDï¼Œç”¨äºä¼šè¯è¿½è¸ªå’Œè°ƒè¯•
                unique_suffix = f"{int(time.time() * 1000)}_{random.randint(1000, 9999)}"
                instance_id = f"{strategy_id}_{unique_suffix}"
                
                instantiated_path = ReasoningPath(
                    path_id=instance_id,  # ä¿æŒå‘åå…¼å®¹æ€§ï¼Œä½†ç°åœ¨è¿™æ˜¯å®ä¾‹ID
                    path_type=template.path_type,
                    description=template.description,
                    prompt_template=template.prompt_template.format(
                        task=task,
                        thinking_seed=thinking_seed
                    ),
                    # ğŸ¯ æºå¤´ç”Ÿæˆï¼šç›´æ¥èµ‹å€¼æ­£ç¡®çš„IDï¼Œæ— éœ€åå¤„ç†
                    strategy_id=strategy_id,  # ç¡®å®šæ€§ç­–ç•¥æ ‡è¯†ç¬¦ï¼Œç”¨äºMABå­¦ä¹ 
                    instance_id=instance_id   # å”¯ä¸€å®ä¾‹æ ‡è¯†ç¬¦ï¼Œç”¨äºè¿½è¸ªå’Œè°ƒè¯•
                )
                
                reasoning_paths.append(instantiated_path)
                logger.debug(f"âœ… å®ä¾‹åŒ–è·¯å¾„: {template.path_type}")
                logger.debug(f"   ç­–ç•¥ID: {strategy_id} (ç¡®å®šæ€§ï¼ŒMABå­¦ä¹ )")
                logger.debug(f"   å®ä¾‹ID: {instance_id} (å”¯ä¸€æ€§ï¼Œè¿½è¸ª)")
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è·¯å¾„æ¨¡æ¿: {path_type}")
        
        return reasoning_paths
    
    def _generate_fallback_paths(self, thinking_seed: str, task: str) -> List[ReasoningPath]:
        """
        ç”Ÿæˆfallbackæ€ç»´è·¯å¾„
        
        Args:
            thinking_seed: æ€ç»´ç§å­
            task: ä»»åŠ¡æè¿°
            
        Returns:
            åŸºç¡€æ€ç»´è·¯å¾„åˆ—è¡¨
        """
        logger.warning("ğŸ”„ ä½¿ç”¨fallbackè·¯å¾„ç”Ÿæˆ")
        
        fallback_types = ['systematic_analytical', 'practical_pragmatic']
        return self._instantiate_reasoning_paths(fallback_types, thinking_seed, task)
    
    def _manage_path_cache(self):
        """ç®¡ç†è·¯å¾„ç”Ÿæˆç¼“å­˜"""
        if len(self.path_generation_cache) > 100:
            # ç§»é™¤æœ€æ—§çš„50ä¸ªç¼“å­˜é¡¹
            oldest_keys = list(self.path_generation_cache.keys())[:50]
            for key in oldest_keys:
                del self.path_generation_cache[key]
            logger.debug("ğŸ§¹ æ¸…ç†è·¯å¾„ç”Ÿæˆç¼“å­˜")
        
    def generate_decision_path(self, user_query: str, complexity_info: Dict[str, Any], 
                             execution_context: Optional[Dict] = None) -> List[ReasoningPath]:
        """
        ç”Ÿæˆå†³ç­–è·¯å¾„å’Œç»´åº¦
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            complexity_info: å¤æ‚åº¦ä¿¡æ¯
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            åŠ¨æ€ç»´åº¦é€‰æ‹©ç»“æœ
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{user_query}_{hash(str(complexity_info))}_{hash(str(execution_context))}"
        if cache_key in self.generation_cache:
            logger.debug(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„è·¯å¾„ç”Ÿæˆ: {cache_key[:20]}...")
            return self.generation_cache[cache_key]
        
        try:
            if self.dimension_selector and self.api_key:
                result = self.dimension_selector.create_dynamic_dimensions(user_query, execution_context)
            else:
                logger.warning("âš ï¸ æœªé…ç½®APIå¯†é’¥æˆ–ç»´åº¦é€‰æ‹©å™¨ï¼Œä½¿ç”¨ç®€å•ç»´åº¦ç”Ÿæˆ")
                result = self._create_simple_dimensions(user_query, complexity_info)
            
            # ç¼“å­˜ç»“æœ
            self.generation_cache[cache_key] = result
            
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self.generation_cache) > 50:
                # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                oldest_key = next(iter(self.generation_cache))
                del self.generation_cache[oldest_key]
            
            logger.info(f"ğŸ›¤ï¸ è·¯å¾„ç”Ÿæˆå®Œæˆ: {len(result)}æ¡æ€ç»´è·¯å¾„")
            return result
            
        except Exception as e:
            logger.error(f"âŒ è·¯å¾„ç”Ÿæˆå¤±è´¥: {e}")
            return self._create_simple_reasoning_paths(user_query, complexity_info)
    
    def _create_simple_reasoning_paths(self, user_query: str, complexity_info: Dict[str, Any]) -> List[ReasoningPath]:
        """åˆ›å»ºç®€å•çš„æ€ç»´è·¯å¾„"""
        
        logger.info("ğŸ”„ ä½¿ç”¨ç®€å•æ€ç»´è·¯å¾„ç”Ÿæˆ")
        
        query_lower = user_query.lower()
        complexity_score = complexity_info.get('complexity_score', 0.5)
        
        reasoning_paths = []
        
        # åŸºäºå¤æ‚åº¦é€‰æ‹©ä¸»è¦æ€ç»´è·¯å¾„
        if complexity_score < 0.3:
            # ç®€å•ä»»åŠ¡ï¼šå®ç”¨ç›´æ¥
            reasoning_paths.append(ReasoningPath(
                path_id="simple_direct_v1",
                path_type="ç®€å•ç›´æ¥å‹",
                description="é€‚ç”¨äºç®€å•ä»»åŠ¡çš„ç›´æ¥æ–¹æ³•",
                prompt_template="è¯·ç›´æ¥è§£å†³ï¼š{task}ã€‚è¦æ±‚ç®€æ´å®ç”¨ï¼Œç«‹å³å¯è¡Œã€‚"
            ))
        elif complexity_score > 0.7:
            # å¤æ‚ä»»åŠ¡ï¼šç³»ç»Ÿåˆ†æ
            reasoning_paths.append(ReasoningPath(
                path_id="complex_systematic_v1",
                path_type="å¤æ‚ç³»ç»Ÿå‹",
                description="é€‚ç”¨äºå¤æ‚ä»»åŠ¡çš„ç³»ç»Ÿæ–¹æ³•",
                prompt_template="è¯·ç³»ç»Ÿæ€§åˆ†æï¼š{task}ã€‚æ­¥éª¤ï¼š1) åˆ†è§£é—®é¢˜ 2) åˆ¶å®šç­–ç•¥ 3) å®æ–½æ–¹æ¡ˆ"
            ))
        else:
            # ä¸­ç­‰å¤æ‚åº¦ï¼šå¹³è¡¡æ–¹æ³•
            reasoning_paths.append(ReasoningPath(
                path_id="moderate_balanced_v1",
                path_type="å¹³è¡¡é€‚ä¸­å‹",
                description="é€‚ç”¨äºä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡çš„å¹³è¡¡æ–¹æ³•",
                prompt_template="è¯·åˆç†åˆ†æè§£å†³ï¼š{task}ã€‚æ–¹æ³•ï¼š1) ç†è§£éœ€æ±‚ 2) è¯„ä¼°æ–¹æ¡ˆ 3) æä¾›å»ºè®®"
            ))
        
        # åŸºäºæŸ¥è¯¢å†…å®¹æ·»åŠ ç‰¹å®šçš„æ€ç»´è·¯å¾„
        if any(word in query_lower for word in ['åˆ›æ–°', 'åˆ›æ„', 'æ–°é¢–', 'ç‹¬ç‰¹']):
            reasoning_paths.append(ReasoningPath(
                path_id="creative_innovative_v1",
                path_type="åˆ›æ–°åˆ›æ„å‹",
                description="ä¸“æ³¨äºåˆ›æ–°å’Œåˆ›æ„çš„æ€ç»´æ–¹æ³•",
                prompt_template="è¯·åˆ›æ–°æ€§æ€è€ƒï¼š{task}ã€‚è¦æ±‚ï¼š1) è·³å‡ºä¼ ç»Ÿ 2) å¯»æ‰¾çªç ´ 3) æä¾›æ–°æ€è·¯"
            ))
        
        if any(word in query_lower for word in ['åˆ†æ', 'è¯„ä¼°', 'ç ”ç©¶', 'æ·±å…¥']):
            reasoning_paths.append(ReasoningPath(
                path_id="analytical_deep_v1",
                path_type="æ·±åº¦åˆ†æå‹",
                description="ä¸“æ³¨äºæ·±åº¦åˆ†æçš„æ€ç»´æ–¹æ³•",
                prompt_template="è¯·æ·±åº¦åˆ†æï¼š{task}ã€‚è¦æ±‚ï¼š1) å…¨é¢è°ƒç ” 2) å¤šè§’åº¦æ€è€ƒ 3) æ·±å…¥æ´å¯Ÿ"
            ))
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ€ç»´è·¯å¾„
        if not reasoning_paths:
            reasoning_paths.append(ReasoningPath(
                path_id="default_general_v1",
                path_type="é€šç”¨æ–¹æ³•å‹",
                description="é€šç”¨çš„é—®é¢˜è§£å†³æ–¹æ³•",
                prompt_template="è¯·å¤„ç†ä»¥ä¸‹ä»»åŠ¡ï¼š{task}ã€‚é‡‡ç”¨åˆé€‚çš„æ–¹æ³•è¿›è¡Œåˆ†æå’Œè§£å†³ã€‚"
            ))
        
        return reasoning_paths
    
    def get_generation_statistics(self) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        
        stats = {
            # ä¼ ç»Ÿç»´åº¦ç”Ÿæˆç»Ÿè®¡ (å‘åå…¼å®¹)
            'total_generations': len(self.generation_cache),
            'cache_hit_rate': 0.0,  # éœ€è¦é¢å¤–è¿½è¸ª
            'avg_paths_per_generation': 0.0,
            'fallback_usage_rate': 0.0,
            'avg_dimensions_per_generation': 0.0,  # å…¼å®¹æ€§å­—æ®µ
            
            # æ–°å¢ï¼šæ€ç»´è·¯å¾„ç”Ÿæˆç»Ÿè®¡
            'path_generation_stats': {
                'total_path_generations': len(self.path_generation_cache),
                'path_type_distribution': dict(self.path_selection_stats),
                'most_used_path_types': [],
                'avg_paths_per_seed': 0.0
            }
        }
        
        # ä¼ ç»Ÿç»Ÿè®¡
        if self.generation_cache:
            # ç°åœ¨ç¼“å­˜çš„æ˜¯List[ReasoningPath]ï¼Œæ‰€ä»¥è®¡ç®—å¹³å‡è·¯å¾„æ•°
            total_paths = sum(len(paths) for paths in self.generation_cache.values())
            stats['avg_paths_per_generation'] = total_paths / len(self.generation_cache)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å›é€€è·¯å¾„ï¼ˆé€šå¸¸IDåŒ…å«"fallback"ï¼‰
            fallback_count = sum(1 for paths in self.generation_cache.values() 
                               if any("fallback" in path.path_id for path in paths))
            stats['fallback_usage_rate'] = fallback_count / len(self.generation_cache)
            
            # å…¼å®¹æ€§å­—æ®µ
            stats['avg_dimensions_per_generation'] = stats['avg_paths_per_generation']
        
        # æ€ç»´è·¯å¾„ç»Ÿè®¡
        if self.path_generation_cache:
            total_paths = sum(len(paths) for paths in self.path_generation_cache.values())
            stats['path_generation_stats']['avg_paths_per_seed'] = total_paths / len(self.path_generation_cache)
            
            # æœ€å¸¸ç”¨çš„è·¯å¾„ç±»å‹ (å‰3å)
            if self.path_selection_stats:
                sorted_types = sorted(
                    self.path_selection_stats.items(), 
                    key=lambda x: x[1], 
                    reverse=True
                )
                stats['path_generation_stats']['most_used_path_types'] = sorted_types[:3]
        
        return stats
    
    def get_path_generation_insights(self) -> Dict[str, Any]:
        """è·å–è·¯å¾„ç”Ÿæˆæ·±åº¦æ´å¯Ÿ"""
        
        insights = {
            'template_coverage': {},  # æ¨¡æ¿ä½¿ç”¨è¦†ç›–ç‡
            'keyword_effectiveness': {},  # å…³é”®è¯åŒ¹é…æ•ˆæœ
            'path_diversity_score': 0.0,  # è·¯å¾„å¤šæ ·æ€§è¯„åˆ†
            'generation_efficiency': {
                'cache_hit_rate': 0.0,
                'avg_generation_time': 0.0,
                'fallback_rate': 0.0
            }
        }
        
        # æ¨¡æ¿ä½¿ç”¨è¦†ç›–ç‡
        total_templates = len(self.path_templates)
        used_templates = len(set(
            path_type for path_type in self.path_selection_stats.keys()
        ))
        insights['template_coverage'] = {
            'total_templates': total_templates,
            'used_templates': used_templates,
            'coverage_rate': used_templates / total_templates if total_templates > 0 else 0.0
        }
        
        # è·¯å¾„å¤šæ ·æ€§è¯„åˆ† (åŸºäºç†µè®¡ç®—)
        if self.path_selection_stats:
            total_selections = sum(self.path_selection_stats.values())
            diversity_score = 0.0
            for count in self.path_selection_stats.values():
                if count > 0:
                    p = count / total_selections
                    diversity_score -= p * (p**0.5)  # ç®€åŒ–çš„å¤šæ ·æ€§æŒ‡æ ‡
            insights['path_diversity_score'] = diversity_score
        
        return insights
    
    # ==================== ğŸ’¡ Aha-Momentåˆ›é€ æ€§ç»•é“è·¯å¾„ç”Ÿæˆ ====================
    
    def _select_creative_bypass_path_types(self, seed_analysis: Dict[str, Any], max_paths: int) -> List[str]:
        """
        Aha-Momentæ¨¡å¼ï¼šé€‰æ‹©åˆ›é€ æ€§å’Œçªç ´æ€§çš„è·¯å¾„ç±»å‹
        
        Args:
            seed_analysis: æ€ç»´ç§å­åˆ†æç»“æœ
            max_paths: æœ€å¤§è·¯å¾„æ•°
            
        Returns:
            åˆ›é€ æ€§è·¯å¾„ç±»å‹åˆ—è¡¨
        """
        # ğŸ’¡ ä¼˜å…ˆé€‰æ‹©çš„åˆ›é€ æ€§è·¯å¾„ç±»å‹ï¼ˆé€šå¸¸åœ¨å¸¸è§„æ¨¡å¼ä¸‹å¾—åˆ†è¾ƒä½ï¼‰
        creative_priority_paths = [
            "åˆ›æ–°çªç ´å‹",      # æœ€å…·åˆ›é€ æ€§
            "æ‰¹åˆ¤è´¨ç–‘å‹",      # æŒ‘æˆ˜å¸¸è§„æ€ç»´
            "è‰ºæœ¯åˆ›æ„å‹",      # è·³å‡ºé€»è¾‘æ¡†æ¶
            "å“²å­¦æ€è¾¨å‹",      # æ·±å±‚æ¬¡æ€è€ƒ
            "ç›´è§‰æ´å¯Ÿå‹",      # éçº¿æ€§æ€ç»´
            "é€†å‘æ€ç»´å‹",      # åå¸¸è§„è·¯å¾„
            "è·¨ç•Œèåˆå‹",      # å¤šé¢†åŸŸç»“åˆ
            "å®éªŒæ¢ç´¢å‹"       # å°è¯•æ–°æ–¹æ³•
        ]
        
        # ğŸ’¡ ä¸­ç­‰åˆ›é€ æ€§è·¯å¾„ï¼ˆå¹³è¡¡åˆ›é€ æ€§å’Œå®ç”¨æ€§ï¼‰
        balanced_creative_paths = [
            "åä½œå…±åˆ›å‹",
            "é€‚åº”æ¼”è¿›å‹",
            "ç»¼åˆé›†æˆå‹",
            "ç³»ç»Ÿä¼˜åŒ–å‹"
        ]
        
        # è·å–æ‰€æœ‰å¯ç”¨çš„è·¯å¾„ç±»å‹
        all_available_paths = list(self.path_templates.keys())
        
        selected_paths = []
        
        # Step 1: ä¼˜å…ˆé€‰æ‹©é«˜åˆ›é€ æ€§è·¯å¾„ï¼ˆè‡³å°‘50%ï¼‰
        high_creative_count = max(1, max_paths // 2)
        available_high_creative = [p for p in creative_priority_paths if p in all_available_paths]
        
        if available_high_creative:
            # éšæœºé€‰æ‹©ï¼Œå¢åŠ ä¸ç¡®å®šæ€§å’Œåˆ›é€ æ€§
            import random
            selected_high_creative = random.sample(
                available_high_creative, 
                min(high_creative_count, len(available_high_creative))
            )
            selected_paths.extend(selected_high_creative)
            
            logger.info(f"ğŸŒŸ é€‰æ‹©é«˜åˆ›é€ æ€§è·¯å¾„: {selected_high_creative}")
        
        # Step 2: è¡¥å……ä¸­ç­‰åˆ›é€ æ€§è·¯å¾„
        remaining_slots = max_paths - len(selected_paths)
        if remaining_slots > 0:
            available_balanced = [p for p in balanced_creative_paths 
                                if p in all_available_paths and p not in selected_paths]
            
            if available_balanced:
                import random
                selected_balanced = random.sample(
                    available_balanced,
                    min(remaining_slots, len(available_balanced))
                )
                selected_paths.extend(selected_balanced)
                
                logger.info(f"ğŸ”„ è¡¥å……å¹³è¡¡è·¯å¾„: {selected_balanced}")
        
        # Step 3: å¦‚æœè¿˜æœ‰ç©ºä½ï¼Œéšæœºé€‰æ‹©å…¶ä»–è·¯å¾„
        remaining_slots = max_paths - len(selected_paths)
        if remaining_slots > 0:
            other_paths = [p for p in all_available_paths 
                          if p not in selected_paths]
            
            if other_paths:
                import random
                additional_paths = random.sample(
                    other_paths,
                    min(remaining_slots, len(other_paths))
                )
                selected_paths.extend(additional_paths)
                
                logger.info(f"â• è¡¥å……å…¶ä»–è·¯å¾„: {additional_paths}")
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªè·¯å¾„
        if not selected_paths and all_available_paths:
            selected_paths = [all_available_paths[0]]
            logger.warning(f"âš ï¸ å›é€€åˆ°é»˜è®¤è·¯å¾„: {selected_paths}")
        
        logger.info(f"ğŸ’¡ Aha-Momentæœ€ç»ˆè·¯å¾„é€‰æ‹©: {selected_paths}")
        return selected_paths[:max_paths]
    
    def get_creative_bypass_stats(self) -> Dict[str, Any]:
        """
        è·å–åˆ›é€ æ€§ç»•é“æ¨¡å¼çš„ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # ç»Ÿè®¡åˆ›é€ æ€§è·¯å¾„ç±»å‹çš„ä½¿ç”¨é¢‘ç‡
        creative_paths = [
            "åˆ›æ–°çªç ´å‹", "æ‰¹åˆ¤è´¨ç–‘å‹", "è‰ºæœ¯åˆ›æ„å‹", "å“²å­¦æ€è¾¨å‹",
            "ç›´è§‰æ´å¯Ÿå‹", "é€†å‘æ€ç»´å‹", "è·¨ç•Œèåˆå‹", "å®éªŒæ¢ç´¢å‹"
        ]
        
        creative_usage = {}
        total_creative_usage = 0
        
        for path_type in creative_paths:
            usage_count = self.path_selection_stats.get(path_type, 0)
            creative_usage[path_type] = usage_count
            total_creative_usage += usage_count
        
        total_usage = sum(self.path_selection_stats.values())
        creative_ratio = total_creative_usage / max(total_usage, 1)
        
        return {
            'creative_path_usage': creative_usage,
            'total_creative_usage': total_creative_usage,
            'total_usage': total_usage,
            'creative_ratio': creative_ratio,
            'most_used_creative_path': max(creative_usage.items(), key=lambda x: x[1])[0] if creative_usage else None,
            'available_creative_paths': len([p for p in creative_paths if p in self.path_templates])
        }
    
    def clear_cache(self):
        """æ¸…é™¤ç”Ÿæˆç¼“å­˜"""
        old_generation_count = len(self.generation_cache)
        old_path_count = len(self.path_generation_cache)
        
        self.generation_cache.clear()
        self.path_generation_cache.clear()
        
        logger.info(f"ğŸ”„ ç¼“å­˜å·²æ¸…é™¤: ä¼ ç»Ÿç”Ÿæˆ({old_generation_count}), è·¯å¾„ç”Ÿæˆ({old_path_count})")
    
    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.path_selection_stats.clear()
        logger.info("ğŸ“Š è·¯å¾„ç”Ÿæˆç»Ÿè®¡å·²é‡ç½®")
