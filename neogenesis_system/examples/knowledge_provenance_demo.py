#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” çŸ¥è¯†æº¯æºç³»ç»Ÿæ¼”ç¤ºè„šæœ¬ - Knowledge Provenance System Demo

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å®Œæ•´çš„çŸ¥è¯†æº¯æº (Knowledge Provenance) ç³»ç»Ÿï¼š
- çŸ¥è¯†æ¥æºè¿½è¸ªå’Œç®¡ç†
- éªŒè¯å’Œç½®ä¿¡åº¦ç³»ç»Ÿ
- çŸ¥è¯†ç½‘ç»œå’Œå…³è”å»ºç«‹
- å­¦ä¹ è·¯å¾„çš„ç”Ÿå‘½å‘¨æœŸè¿½è¸ª
- çŸ¥è¯†è¿›åŒ–å’Œç‰ˆæœ¬ç®¡ç†

ä½œè€…: Neosgenesis Team
æ—¥æœŸ: 2024
"""

import sys
import os
import time
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from shared.data_structures import (
    KnowledgeProvenance, SourceReference, KnowledgeValidation, 
    KnowledgeUpdate, KnowledgeNetwork, KnowledgeSource, 
    CredibilityLevel, VerificationStatus
)
from cognitive_engine.data_structures import ReasoningPath
from utils.logging_setup import setup_logger


def create_sample_knowledge_sources():
    """åˆ›å»ºç¤ºä¾‹çŸ¥è¯†æ¥æº"""
    sources = []
    
    # å­¦æœ¯è®ºæ–‡æ¥æº
    academic_source = SourceReference(
        url="https://arxiv.org/abs/2024.12345",
        title="Advanced Reasoning Patterns in AI Systems",
        author="Dr. Jane Smith, Dr. Bob Wilson",
        published_date=time.time() - 86400 * 30,  # 30å¤©å‰
        source_type=KnowledgeSource.ACADEMIC_PAPER,
        metadata={
            "journal": "AI Research Quarterly",
            "citations": 45,
            "peer_reviewed": True,
            "impact_factor": 3.2
        }
    )
    sources.append(academic_source)
    
    # ç½‘ç»œçˆ¬å–æ¥æº
    web_source = SourceReference(
        url="https://www.example-ai-blog.com/reasoning-strategies",
        title="å®ç”¨AIæ¨ç†ç­–ç•¥æŒ‡å—",
        author="AIä¸“å®¶å›¢é˜Ÿ",
        published_date=time.time() - 86400 * 7,  # 7å¤©å‰
        source_type=KnowledgeSource.WEB_SCRAPING,
        metadata={
            "domain_authority": 85,
            "last_updated": "2024-01-15",
            "language": "zh-CN"
        }
    )
    sources.append(web_source)
    
    # APIæŸ¥è¯¢æ¥æº
    api_source = SourceReference(
        url="https://api.knowledge-base.com/v1/reasoning-methods",
        title="æ¨ç†æ–¹æ³•APIæ•°æ®",
        source_type=KnowledgeSource.API_QUERY,
        metadata={
            "api_version": "v1.2",
            "response_time": 0.15,
            "data_freshness": "real-time"
        }
    )
    sources.append(api_source)
    
    return sources


def demonstrate_basic_provenance(logger):
    """æ¼”ç¤ºåŸºç¡€çŸ¥è¯†æº¯æºåŠŸèƒ½"""
    logger.info("ğŸ” === åŸºç¡€çŸ¥è¯†æº¯æºåŠŸèƒ½æ¼”ç¤º ===")
    
    # 1. åˆ›å»ºçŸ¥è¯†æº¯æºè®°å½•
    print("\n1ï¸âƒ£ åˆ›å»ºçŸ¥è¯†æº¯æºè®°å½•...")
    provenance = KnowledgeProvenance(
        knowledge_id="reasoning_pattern_001",
        confidence_score=0.8
    )
    
    # 2. æ·»åŠ çŸ¥è¯†æ¥æº
    print("\n2ï¸âƒ£ æ·»åŠ çŸ¥è¯†æ¥æº...")
    sources = create_sample_knowledge_sources()
    
    # æ·»åŠ ä¸»è¦æ¥æº
    provenance.add_source(sources[0], is_primary=True)
    print(f"   âœ… ä¸»è¦æ¥æº: {sources[0].title}")
    
    # æ·»åŠ é¢å¤–æ¥æº
    for source in sources[1:]:
        provenance.add_source(source)
        print(f"   â• é¢å¤–æ¥æº: {source.title}")
    
    # 3. æ·»åŠ éªŒè¯è®°å½•
    print("\n3ï¸âƒ£ æ·»åŠ éªŒè¯è®°å½•...")
    validation = KnowledgeValidation(
        validation_method="expert_review",
        validator="AI Research Team",
        status=VerificationStatus.VERIFIED,
        confidence_score=0.85,
        notes="ç»ä¸“å®¶å›¢é˜ŸéªŒè¯ï¼Œæ¨ç†æ¨¡å¼æœ‰æ•ˆä¸”å®ç”¨"
    )
    validation.add_evidence("åœ¨5ä¸ªä¸åŒåœºæ™¯ä¸­æµ‹è¯•æˆåŠŸ")
    validation.add_evidence("è·å¾—3ä½ä¸“å®¶ä¸€è‡´è®¤å¯")
    
    provenance.add_validation(validation)
    print(f"   âœ… éªŒè¯çŠ¶æ€: {validation.status.value}")
    print(f"   ğŸ“Š éªŒè¯ç½®ä¿¡åº¦: {validation.confidence_score:.2f}")
    
    # 4. è®°å½•ä½¿ç”¨æƒ…å†µ
    print("\n4ï¸âƒ£ æ¨¡æ‹Ÿä½¿ç”¨æƒ…å†µ...")
    for i in range(10):
        success = i < 8  # 80% æˆåŠŸç‡
        provenance.record_usage(success)
        if i % 3 == 0:
            result = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            print(f"   ä½¿ç”¨ #{i+1}: {result}")
    
    # 5. æ·»åŠ ä¸Šä¸‹æ–‡æ ‡ç­¾
    print("\n5ï¸âƒ£ æ·»åŠ ä¸Šä¸‹æ–‡æ ‡ç­¾...")
    context_tags = ["AIæ¨ç†", "ç³»ç»Ÿæ€ç»´", "é—®é¢˜è§£å†³", "å­¦æœ¯éªŒè¯", "å®ç”¨æ–¹æ³•"]
    for tag in context_tags:
        provenance.add_context_tag(tag)
    print(f"   ğŸ·ï¸ æ ‡ç­¾: {', '.join(context_tags)}")
    
    # 6. æ˜¾ç¤ºæº¯æºæ‘˜è¦
    print("\n6ï¸âƒ£ çŸ¥è¯†æº¯æºæ‘˜è¦:")
    summary = provenance.get_provenance_summary()
    print(f"   ğŸ“‹ çŸ¥è¯†ID: {summary['knowledge_id']}")
    print(f"   ğŸ“Š ç½®ä¿¡åº¦: {summary['confidence_score']:.3f}")
    print(f"   ğŸ¯ å¯ä¿¡åº¦çº§åˆ«: {summary['credibility_level']}")
    print(f"   âœ… å·²éªŒè¯: {summary['is_verified']}")
    print(f"   ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.3f}")
    print(f"   â° å¹´é¾„: {summary['age_days']:.1f} å¤©")
    print(f"   ğŸŒŸ æ–°é²œåº¦: {summary['freshness_score']:.2f}")
    print(f"   ğŸ”— æ¥æºæ•°é‡: {summary['source_count']}")
    
    return provenance


def demonstrate_knowledge_network(logger):
    """æ¼”ç¤ºçŸ¥è¯†ç½‘ç»œåŠŸèƒ½"""
    logger.info("\nğŸ•¸ï¸ === çŸ¥è¯†ç½‘ç»œåŠŸèƒ½æ¼”ç¤º ===")
    
    # åˆ›å»ºä¸»çŸ¥è¯†èŠ‚ç‚¹
    main_provenance = KnowledgeProvenance(
        knowledge_id="systematic_reasoning",
        confidence_score=0.9
    )
    
    # åˆ›å»ºç›¸å…³çŸ¥è¯†èŠ‚ç‚¹
    related_knowledge_ids = [
        "analytical_thinking",
        "logical_deduction", 
        "creative_problem_solving",
        "critical_evaluation"
    ]
    
    print("1ï¸âƒ£ å»ºç«‹çŸ¥è¯†å…³è”ç½‘ç»œ...")
    
    # æ·»åŠ ç›¸å…³å…³ç³»
    main_provenance.knowledge_network.add_relationship(
        "analytical_thinking", "related", similarity_score=0.85,
        metadata={"relationship_strength": "strong", "domain": "logic"}
    )
    
    main_provenance.knowledge_network.add_relationship(
        "logical_deduction", "supporting", similarity_score=0.92,
        metadata={"support_type": "foundational", "evidence": "å…±åŒçš„é€»è¾‘åŸºç¡€"}
    )
    
    main_provenance.knowledge_network.add_relationship(
        "creative_problem_solving", "related", similarity_score=0.65,
        metadata={"relationship_strength": "moderate", "complementary": True}
    )
    
    main_provenance.knowledge_network.add_relationship(
        "critical_evaluation", "influences", similarity_score=0.78,
        metadata={"influence_type": "quality_control", "direction": "bidirectional"}
    )
    
    # æ˜¾ç¤ºç½‘ç»œç»Ÿè®¡
    network = main_provenance.knowledge_network
    print(f"   ğŸ”— æ€»è¿æ¥æ•°: {network.total_connections}")
    print(f"   ğŸ’ª è¿æ¥å¼ºåº¦: {network.connection_strength:.2f}")
    print(f"   ğŸ“Š ç›¸å…³çŸ¥è¯†: {len(network.related_knowledge)}")
    print(f"   ğŸ¤ æ”¯æŒæ€§çŸ¥è¯†: {len(network.supporting_knowledge)}")
    print(f"   â­ å½±å“å…³ç³»: {len(network.influences)}")
    
    # æ˜¾ç¤ºå…·ä½“å…³è”
    print("\n2ï¸âƒ£ çŸ¥è¯†å…³è”è¯¦æƒ…:")
    for knowledge_id, score in network.similarity_scores.items():
        relationship_type = "æœªçŸ¥"
        if knowledge_id in network.related_knowledge:
            relationship_type = "ç›¸å…³"
        elif knowledge_id in network.supporting_knowledge:
            relationship_type = "æ”¯æŒ"
        elif knowledge_id in network.influences:
            relationship_type = "å½±å“"
        
        metadata = network.relationship_metadata.get(knowledge_id, {})
        print(f"   ğŸ”— {knowledge_id} ({relationship_type}) - ç›¸ä¼¼åº¦: {score:.2f}")
        if metadata:
            print(f"      ğŸ’¬ å…ƒä¿¡æ¯: {metadata}")
    
    return main_provenance


def demonstrate_reasoning_path_provenance(logger):
    """æ¼”ç¤ºæ¨ç†è·¯å¾„çš„çŸ¥è¯†æº¯æºåŠŸèƒ½"""
    logger.info("\nğŸ§  === æ¨ç†è·¯å¾„çŸ¥è¯†æº¯æºæ¼”ç¤º ===")
    
    print("1ï¸âƒ£ åˆ›å»ºå¸¦æœ‰çŸ¥è¯†æº¯æºçš„æ¨ç†è·¯å¾„...")
    
    # åˆ›å»ºå­¦ä¹ å‹æ¨ç†è·¯å¾„
    learned_path = ReasoningPath(
        path_id="ai_enhanced_reasoning_v1",
        path_type="AIå¢å¼ºæ¨ç†å‹",
        description="ç»“åˆäººå·¥æ™ºèƒ½è¾…åŠ©çš„é«˜æ•ˆæ¨ç†æ–¹æ³•",
        prompt_template="ä½¿ç”¨AIåä½œè¿›è¡Œ{task}çš„ç³»ç»Ÿæ€§åˆ†æ...",
        name="AIå¢å¼ºæ¨ç†è·¯å¾„",
        steps=[
            "ğŸ¤– å¯åŠ¨AIåä½œæ¨¡å¼",
            "ğŸ§  äººæœºååŒé—®é¢˜åˆ†æ",
            "ğŸ’¡ AIç”Ÿæˆåˆ›æ–°è§£å†³æ–¹æ¡ˆ",
            "ğŸ” äººç±»ä¸“å®¶éªŒè¯å’Œä¼˜åŒ–",
            "ğŸ¯ è¾“å‡ºä¼˜åŒ–çš„æœ€ç»ˆæ–¹æ¡ˆ"
        ],
        keywords=["AIåä½œ", "äººæœºååŒ", "åˆ›æ–°è§£å†³", "ä¸“å®¶éªŒè¯"],
        complexity_level=4,
        estimated_steps=5,
        success_indicators=["æ–¹æ¡ˆåˆ›æ–°æ€§", "å®æ–½å¯è¡Œæ€§", "ä¸“å®¶è®¤å¯åº¦"],
        failure_patterns=["è¿‡åº¦ä¾èµ–AI", "ç¼ºä¹äººç±»æ´å¯Ÿ", "æ–¹æ¡ˆä¸åˆ‡å®é™…"],
        learning_source="learned_exploration",
        confidence_score=0.75,
        metadata={
            "source": "learned_exploration",
            "learned_from": "çŸ¥è¯†æ¢ç´¢æ¨¡å—",
            "confidence": 0.75,
            "discovery_method": "web_scraping_analysis"
        }
    )
    
    print(f"   âœ… è·¯å¾„åˆ›å»º: {learned_path.name}")
    print(f"   ğŸ¯ å­¦ä¹ æ¥æº: {learned_path.learning_source}")
    print(f"   ğŸ“Š åˆå§‹ç½®ä¿¡åº¦: {learned_path.confidence_score:.2f}")
    
    # 2. æ·»åŠ è¯¦ç»†çš„çŸ¥è¯†æ¥æº
    print("\n2ï¸âƒ£ æ·»åŠ çŸ¥è¯†æ¥æº...")
    success = learned_path.add_provenance_source(
        url="https://ai-research.example.com/human-ai-collaboration",
        title="äººæœºåä½œæ¨ç†æ–¹æ³•ç ”ç©¶",
        author="AIç ”ç©¶é™¢",
        content="äººæœºåä½œæ¨ç†çš„è¯¦ç»†æ–¹æ³•å’Œæ¡ˆä¾‹ç ”ç©¶..."
    )
    print(f"   ğŸ“š æ¥æºæ·»åŠ : {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
    
    # 3. æ¨¡æ‹Ÿä½¿ç”¨å’ŒéªŒè¯
    print("\n3ï¸âƒ£ æ¨¡æ‹Ÿè·¯å¾„ä½¿ç”¨å’Œåé¦ˆ...")
    for i in range(15):
        success = i < 12  # 80% æˆåŠŸç‡
        execution_time = 2.5 + (i * 0.1)  # é€æ¸ä¼˜åŒ–çš„æ‰§è¡Œæ—¶é—´
        learned_path.record_usage(success, execution_time)
        
        if i % 5 == 4:
            print(f"   ğŸ“Š ç¬¬ {i+1} æ¬¡ä½¿ç”¨å - æˆåŠŸç‡: {learned_path.success_rate:.2f}, å¹³å‡æ—¶é—´: {learned_path.avg_execution_time:.1f}s")
    
    # 4. æ·»åŠ ä¸Šä¸‹æ–‡æ ‡ç­¾
    print("\n4ï¸âƒ£ ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯...")
    context_tags = ["AIè¾…åŠ©", "åˆ›æ–°æ–¹æ³•", "åä½œæ¨ç†", "ä¸“å®¶éªŒè¯", "é«˜æ•ˆè§£å†³"]
    for tag in context_tags:
        learned_path.add_context_tag(tag)
    print(f"   ğŸ·ï¸ ä¸Šä¸‹æ–‡æ ‡ç­¾: {', '.join(context_tags)}")
    
    # 5. éªŒè¯è·¯å¾„
    print("\n5ï¸âƒ£ éªŒè¯è·¯å¾„æœ‰æ•ˆæ€§...")
    learned_path.mark_as_verified(
        verification_method="å®é™…ä½¿ç”¨éªŒè¯",
        confidence=0.88,
        notes="é€šè¿‡15æ¬¡å®é™…ä½¿ç”¨éªŒè¯ï¼Œè¡¨ç°è‰¯å¥½"
    )
    print(f"   âœ… éªŒè¯çŠ¶æ€: {learned_path.validation_status}")
    print(f"   ğŸ¯ å·²éªŒè¯: {learned_path.is_verified}")
    
    # 6. åˆ›å»ºè¿›åŒ–ç‰ˆæœ¬
    print("\n6ï¸âƒ£ åˆ›å»ºè·¯å¾„è¿›åŒ–ç‰ˆæœ¬...")
    changes = [
        "ä¼˜åŒ–AI-äººç±»äº¤äº’æµç¨‹",
        "å¢åŠ å®æ—¶åé¦ˆæœºåˆ¶", 
        "åŠ å¼ºä¸“å®¶éªŒè¯ç¯èŠ‚"
    ]
    evolved_path = learned_path.create_evolved_version(
        changes=changes,
        reason="åŸºäºä½¿ç”¨åé¦ˆè¿›è¡Œä¼˜åŒ–å‡çº§"
    )
    
    print(f"   ğŸ§¬ è¿›åŒ–è·¯å¾„: {evolved_path.path_id}")
    print(f"   ğŸ“ˆ è¿›åŒ–ä»£æ•°: {evolved_path.evolution_generation}")
    print(f"   ğŸ‘¨â€ğŸ’» çˆ¶è·¯å¾„: {evolved_path.parent_path_id}")
    print(f"   ğŸ“Š è¿›åŒ–åç½®ä¿¡åº¦: {evolved_path.confidence_score:.2f}")
    
    # 7. æ˜¾ç¤ºå®Œæ•´æº¯æºæ‘˜è¦
    print("\n7ï¸âƒ£ å®Œæ•´çŸ¥è¯†æº¯æºæ‘˜è¦:")
    summary = learned_path.get_provenance_summary()
    
    print(f"   ğŸ“‹ è·¯å¾„ID: {summary['path_id']}")
    print(f"   ğŸ“ å‹å¥½åç§°: {summary['name']}")
    print(f"   ğŸŒ± å­¦ä¹ æ¥æº: {summary['learning_source']}")
    print(f"   ğŸ“Š ç½®ä¿¡åº¦: {summary['confidence_score']:.3f}")
    print(f"   âœ… éªŒè¯çŠ¶æ€: {summary['validation_status']}")
    print(f"   ğŸ¯ ä½¿ç”¨æ¬¡æ•°: {summary['usage_count']}")
    print(f"   ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.3f}")
    print(f"   ğŸ” æ˜¯å¦å­¦ä¹ è·¯å¾„: {summary['is_learned_path']}")
    print(f"   âœ… æ˜¯å¦å·²éªŒè¯: {summary['is_verified']}")
    print(f"   âš ï¸ æ˜¯å¦æœ‰å†²çª: {summary['has_conflicts']}")
    print(f"   ğŸ§¬ è¿›åŒ–ä»£æ•°: {summary['evolution_generation']}")
    
    if 'detailed_provenance' in summary:
        detailed = summary['detailed_provenance']
        print(f"   ğŸ“… å¹´é¾„: {detailed['age_days']:.1f} å¤©")
        print(f"   ğŸŒŸ æ–°é²œåº¦: {detailed['freshness_score']:.2f}")
        print(f"   ğŸ”— ç½‘ç»œè¿æ¥: {detailed['network_connections']}")
        print(f"   ğŸ“š æ¥æºæ•°é‡: {detailed['source_count']}")
    
    print(f"   ğŸ·ï¸ ä¸Šä¸‹æ–‡æ ‡ç­¾: {', '.join(summary['context_tags'])}")
    
    return learned_path, evolved_path


def demonstrate_update_tracking(logger):
    """æ¼”ç¤ºçŸ¥è¯†æ›´æ–°è¿½è¸ª"""
    logger.info("\nğŸ“ === çŸ¥è¯†æ›´æ–°è¿½è¸ªæ¼”ç¤º ===")
    
    # åˆ›å»ºåˆå§‹çŸ¥è¯†
    provenance = KnowledgeProvenance(
        knowledge_id="dynamic_reasoning_method",
        confidence_score=0.7
    )
    
    print("1ï¸âƒ£ åˆå§‹çŸ¥è¯†åˆ›å»º...")
    print(f"   ğŸ“Š åˆå§‹ç½®ä¿¡åº¦: {provenance.confidence_score:.2f}")
    
    # æ¨¡æ‹Ÿå¤šæ¬¡æ›´æ–°
    updates = [
        {
            "type": "content_update",
            "reason": "å‘ç°æ–°çš„åº”ç”¨åœºæ™¯",
            "changes": ["å¢åŠ é‡‘èåˆ†æåº”ç”¨", "æ‰©å±•åˆ°åŒ»ç–—è¯Šæ–­é¢†åŸŸ"],
            "confidence_change": 0.1
        },
        {
            "type": "verification_update", 
            "reason": "ä¸“å®¶å›¢é˜ŸéªŒè¯é€šè¿‡",
            "changes": ["é€šè¿‡åŒè¡Œè¯„è®®", "è·å¾—æƒå¨è®¤è¯"],
            "confidence_change": 0.15
        },
        {
            "type": "confidence_update",
            "reason": "å®é™…ä½¿ç”¨åé¦ˆè‰¯å¥½",
            "changes": ["æˆåŠŸç‡æå‡åˆ°92%", "ç”¨æˆ·æ»¡æ„åº¦é«˜"],
            "confidence_change": 0.05
        }
    ]
    
    print("\n2ï¸âƒ£ è®°å½•çŸ¥è¯†æ›´æ–°å†å²...")
    for i, update_info in enumerate(updates, 1):
        update = KnowledgeUpdate(
            update_type=update_info["type"],
            reason=update_info["reason"],
            changes=update_info["changes"],
            confidence_change=update_info["confidence_change"]
        )
        
        provenance.add_update(update)
        provenance.confidence_score += update_info["confidence_change"]
        
        print(f"   ğŸ“ æ›´æ–° #{i}: {update_info['reason']}")
        print(f"      ç±»å‹: {update_info['type']}")
        print(f"      ç½®ä¿¡åº¦å˜åŒ–: +{update_info['confidence_change']:.2f}")
        print(f"      å½“å‰ç½®ä¿¡åº¦: {provenance.confidence_score:.2f}")
    
    # æ˜¾ç¤ºæ›´æ–°å†å²
    print(f"\n3ï¸âƒ£ æ›´æ–°å†å²æ‘˜è¦ (å…± {len(provenance.update_history)} æ¬¡æ›´æ–°):")
    for i, update in enumerate(provenance.update_history, 1):
        print(f"   #{i} - {update.reason} ({update.update_type})")
        for change in update.changes:
            print(f"        â€¢ {change}")
    
    return provenance


def demonstrate_conflict_detection(logger):
    """æ¼”ç¤ºå†²çªæ£€æµ‹åŠŸèƒ½"""
    logger.info("\nâš ï¸ === å†²çªæ£€æµ‹åŠŸèƒ½æ¼”ç¤º ===")
    
    # åˆ›å»ºå­˜åœ¨å†²çªçš„çŸ¥è¯†
    provenance = KnowledgeProvenance(
        knowledge_id="controversial_method",
        confidence_score=0.6
    )
    
    print("1ï¸âƒ£ åˆ›å»ºå…·æœ‰å†²çªçš„éªŒè¯è®°å½•...")
    
    # æ·»åŠ æ­£é¢éªŒè¯
    positive_validation = KnowledgeValidation(
        validation_method="academic_study",
        validator="University A Research Team",
        status=VerificationStatus.VERIFIED,
        confidence_score=0.8,
        notes="åœ¨æ§åˆ¶å®éªŒä¸­è¡¨ç°è‰¯å¥½"
    )
    positive_validation.add_evidence("å®éªŒç»„æ¯”å¯¹ç…§ç»„æå‡25%")
    positive_validation.add_evidence("é€šè¿‡ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ")
    
    provenance.add_validation(positive_validation)
    
    # æ·»åŠ è´Ÿé¢éªŒè¯
    negative_validation = KnowledgeValidation(
        validation_method="independent_replication",
        validator="University B Research Team",
        status=VerificationStatus.CONFLICTING,
        confidence_score=0.3,
        notes="æ— æ³•å¤ç°åŸå§‹ç ”ç©¶ç»“æœ"
    )
    negative_validation.add_conflict("å®éªŒç»“æœæ— æ³•å¤ç°")
    negative_validation.add_conflict("æ ·æœ¬å¤§å°å¯èƒ½ä¸è¶³")
    negative_validation.add_evidence("é‡å¤å®éªŒ3æ¬¡å‡æ— æ˜¾è‘—æ•ˆæœ")
    
    provenance.add_validation(negative_validation)
    
    print(f"   âœ… æ­£é¢éªŒè¯: {positive_validation.confidence_score:.2f} ç½®ä¿¡åº¦")
    print(f"   âš ï¸ å†²çªéªŒè¯: {negative_validation.confidence_score:.2f} ç½®ä¿¡åº¦")
    print(f"   ğŸ¯ æœ€ç»ˆçŠ¶æ€: {'æœ‰å†²çª' if provenance.has_conflicts else 'æ— å†²çª'}")
    
    # æ˜¾ç¤ºå†²çªè¯¦æƒ…
    print("\n2ï¸âƒ£ å†²çªè¯¦æƒ…åˆ†æ:")
    for i, validation in enumerate(provenance.validation_history, 1):
        print(f"   éªŒè¯ #{i} - {validation.validator}")
        print(f"      çŠ¶æ€: {validation.status.value}")
        print(f"      ç½®ä¿¡åº¦: {validation.confidence_score:.2f}")
        
        if validation.evidence:
            print("      è¯æ®:")
            for evidence in validation.evidence:
                print(f"        âœ“ {evidence}")
        
        if validation.conflicts:
            print("      å†²çª:")
            for conflict in validation.conflicts:
                print(f"        âš  {conflict}")
    
    print(f"\n3ï¸âƒ£ ç³»ç»Ÿå»ºè®®:")
    print(f"   ğŸ“Š æ•´ä½“ç½®ä¿¡åº¦: {provenance.confidence_score:.2f}")
    print(f"   ğŸ¯ å¯ä¿¡åº¦çº§åˆ«: {provenance.credibility_level.value}")
    print(f"   ğŸ’¡ å»ºè®®: éœ€è¦æ›´å¤šç‹¬ç«‹éªŒè¯æ¥è§£å†³å†²çª")
    
    return provenance


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger("KnowledgeProvenanceDemo", level="INFO")
    
    print("ğŸ” ========================================")
    print("ğŸ”      çŸ¥è¯†æº¯æºç³»ç»Ÿå®Œæ•´åŠŸèƒ½æ¼”ç¤º")
    print("ğŸ” ========================================")
    print("ğŸ” è¿™ä¸ªæ¼”ç¤ºå°†å±•ç¤ºçŸ¥è¯†æº¯æºç³»ç»Ÿçš„å„é¡¹æ ¸å¿ƒåŠŸèƒ½:")
    print("ğŸ” 1. åŸºç¡€çŸ¥è¯†æº¯æºå’Œæ¥æºç®¡ç†")
    print("ğŸ” 2. çŸ¥è¯†ç½‘ç»œå’Œå…³è”å»ºç«‹")
    print("ğŸ” 3. æ¨ç†è·¯å¾„çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸè¿½è¸ª")
    print("ğŸ” 4. çŸ¥è¯†æ›´æ–°å’Œç‰ˆæœ¬ç®¡ç†")
    print("ğŸ” 5. å†²çªæ£€æµ‹å’Œå¤„ç†")
    print("ğŸ” ========================================\n")
    
    try:
        # 1. åŸºç¡€æº¯æºæ¼”ç¤º
        basic_provenance = demonstrate_basic_provenance(logger)
        
        input("\nğŸ”„ æŒ‰ Enter é”®ç»§ç»­çŸ¥è¯†ç½‘ç»œæ¼”ç¤º...")
        
        # 2. çŸ¥è¯†ç½‘ç»œæ¼”ç¤º
        network_provenance = demonstrate_knowledge_network(logger)
        
        input("\nğŸ§  æŒ‰ Enter é”®ç»§ç»­æ¨ç†è·¯å¾„æº¯æºæ¼”ç¤º...")
        
        # 3. æ¨ç†è·¯å¾„æº¯æºæ¼”ç¤º
        learned_path, evolved_path = demonstrate_reasoning_path_provenance(logger)
        
        input("\nğŸ“ æŒ‰ Enter é”®ç»§ç»­æ›´æ–°è¿½è¸ªæ¼”ç¤º...")
        
        # 4. æ›´æ–°è¿½è¸ªæ¼”ç¤º
        update_provenance = demonstrate_update_tracking(logger)
        
        input("\nâš ï¸ æŒ‰ Enter é”®ç»§ç»­å†²çªæ£€æµ‹æ¼”ç¤º...")
        
        # 5. å†²çªæ£€æµ‹æ¼”ç¤º
        conflict_provenance = demonstrate_conflict_detection(logger)
        
        print("\nğŸ” ========================================")
        print("ğŸ”      çŸ¥è¯†æº¯æºç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
        print("ğŸ” ========================================")
        print("ğŸ¯ æ¼”ç¤ºçš„æ ¸å¿ƒåŠŸèƒ½:")
        print("   âœ… å®Œæ•´çš„çŸ¥è¯†æ¥æºè¿½è¸ªå’Œç®¡ç†")
        print("   âœ… å¤šå±‚æ¬¡çš„éªŒè¯å’Œç½®ä¿¡åº¦ç³»ç»Ÿ")
        print("   âœ… æ™ºèƒ½çš„çŸ¥è¯†ç½‘ç»œå…³è”")
        print("   âœ… å­¦ä¹ è·¯å¾„çš„ç”Ÿå‘½å‘¨æœŸè¿½è¸ª")
        print("   âœ… çŸ¥è¯†è¿›åŒ–å’Œç‰ˆæœ¬ç®¡ç†")
        print("   âœ… è‡ªåŠ¨çš„å†²çªæ£€æµ‹å’Œå¤„ç†")
        print("   âœ… å…¨é¢çš„ä½¿ç”¨ç»Ÿè®¡å’Œåˆ†æ")
        print("   âœ… çµæ´»çš„ä¸Šä¸‹æ–‡æ ‡ç­¾ç³»ç»Ÿ")
        
        print("\nğŸ“Š æ¼”ç¤ºæ€»ç»“:")
        print(f"   ğŸ” åŸºç¡€æº¯æºè®°å½•: ç½®ä¿¡åº¦ {basic_provenance.confidence_score:.2f}")
        print(f"   ğŸ•¸ï¸ çŸ¥è¯†ç½‘ç»œè¿æ¥: {network_provenance.knowledge_network.total_connections} ä¸ª")
        print(f"   ğŸ§  å­¦ä¹ è·¯å¾„ä½¿ç”¨: {learned_path.usage_count} æ¬¡")
        print(f"   ğŸ§¬ è·¯å¾„è¿›åŒ–ä»£æ•°: {evolved_path.evolution_generation}")
        print(f"   ğŸ“ çŸ¥è¯†æ›´æ–°æ¬¡æ•°: {len(update_provenance.update_history)}")
        print(f"   âš ï¸ å†²çªæ£€æµ‹: {'å‘ç°å†²çª' if conflict_provenance.has_conflicts else 'æ— å†²çª'}")
        
        print("\nğŸŒŸ çŸ¥è¯†æº¯æºç³»ç»Ÿä¸ºè®¤çŸ¥é£è½®æä¾›äº†é€æ˜ã€å¯è¿½æº¯ã€å¯éªŒè¯çš„çŸ¥è¯†åŸºç¡€ï¼")
        
        # å¯é€‰ï¼šä¿å­˜æ¼”ç¤ºæ•°æ®
        print(f"\nğŸ“‹ æƒ³è¦æŸ¥çœ‹å®Œæ•´çš„çŸ¥è¯†æº¯æºæ•°æ®å—ï¼Ÿ(y/N): ", end="")
        if input().lower().startswith('y'):
            print("\nğŸ” å®Œæ•´çŸ¥è¯†æº¯æºæ•°æ®:")
            
            print("\n1. åŸºç¡€æº¯æºæ‘˜è¦:")
            print(json.dumps(basic_provenance.get_provenance_summary(), 
                           indent=2, ensure_ascii=False, default=str))
            
            print("\n2. æ¨ç†è·¯å¾„æº¯æºæ‘˜è¦:")
            print(json.dumps(learned_path.get_provenance_summary(), 
                           indent=2, ensure_ascii=False, default=str))
        
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        logger.info("ğŸ‘‹ çŸ¥è¯†æº¯æºæ¼”ç¤ºç»“æŸ")


if __name__ == "__main__":
    main()
