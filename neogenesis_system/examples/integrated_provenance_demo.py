#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”— é›†æˆçŸ¥è¯†æº¯æºæ¼”ç¤ºè„šæœ¬ - Integrated Knowledge Provenance Demo

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†çŸ¥è¯†æº¯æºç³»ç»Ÿä¸è®¤çŸ¥é£è½®å„ç»„ä»¶çš„å®Œæ•´é›†æˆï¼š
- KnowledgeExplorer + çŸ¥è¯†æº¯æº
- PathGenerator + åŠ¨æ€è·¯å¾„åº“ + çŸ¥è¯†æº¯æº
- MABConverger è¯•ç‚¼åœº + çŸ¥è¯†æº¯æº
- ç«¯åˆ°ç«¯çš„å­¦ä¹ -éªŒè¯-è¿›åŒ–å¾ªç¯

ä½œè€…: Neosgenesis Team  
æ—¥æœŸ: 2024
"""

import sys
import os
import time
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from shared.data_structures import KnowledgeSource, SourceReference
from cognitive_engine.data_structures import ReasoningPath
from cognitive_engine.path_generator import PathGenerator
from cognitive_engine.mab_converger import MABConverger
from neogenesis_system.providers.knowledge_explorer import KnowledgeExplorer
from neogenesis_system.shared.logging_setup import setup_logger


def create_enhanced_knowledge_explorer():
    """åˆ›å»ºå¢å¼ºçš„çŸ¥è¯†æ¢ç´¢å™¨ï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""
    
    class MockKnowledgeExplorer:
        """æ¨¡æ‹ŸçŸ¥è¯†æ¢ç´¢å™¨"""
        
        def __init__(self):
            # æ¨¡æ‹Ÿæ¢ç´¢ç»“æœæ•°æ®åº“
            self.mock_results = {
                "åˆ›æ–°æ€ç»´": {
                    "title": "è®¾è®¡æ€ç»´ä¸åˆ›æ–°æ–¹æ³•è®º",
                    "content": "è®¾è®¡æ€ç»´æ˜¯ä¸€ç§ä»¥äººä¸ºä¸­å¿ƒçš„åˆ›æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬åŒç†å¿ƒã€å®šä¹‰ã€æ„æ€ã€åŸå‹å’Œæµ‹è¯•äº”ä¸ªé˜¶æ®µ...",
                    "source_url": "https://design-thinking.example.com/methodology",
                    "author": "åˆ›æ–°ç ”ç©¶é™¢",
                    "confidence": 0.85,
                    "source_type": KnowledgeSource.WEB_SCRAPING
                },
                "ç³»ç»Ÿåˆ†æ": {
                    "title": "å¤æ‚ç³»ç»Ÿåˆ†ææ–¹æ³•",
                    "content": "ç³»ç»Ÿåˆ†ææ˜¯ä¸€ç§ç»“æ„åŒ–çš„é—®é¢˜è§£å†³æ–¹æ³•ï¼Œé€šè¿‡åˆ†è§£å¤æ‚ç³»ç»Ÿæ¥ç†è§£å…¶ç»„æˆéƒ¨åˆ†...",
                    "source_url": "https://systems-analysis.example.com/methods",
                    "author": "ç³»ç»Ÿç§‘å­¦ç ”ç©¶ä¸­å¿ƒ",
                    "confidence": 0.78,
                    "source_type": KnowledgeSource.ACADEMIC_PAPER
                },
                "åä½œè§£å†³": {
                    "title": "å›¢é˜Ÿåä½œé—®é¢˜è§£å†³æ¡†æ¶", 
                    "content": "æœ‰æ•ˆçš„å›¢é˜Ÿåä½œéœ€è¦æ˜ç¡®çš„è§’è‰²åˆ†å·¥ã€å¼€æ”¾çš„æ²Ÿé€šæ¸ é“å’Œå…±åŒçš„ç›®æ ‡...",
                    "source_url": "https://collaboration.example.com/frameworks",
                    "author": "ç»„ç»‡è¡Œä¸ºå­¦ä¸“å®¶",
                    "confidence": 0.72,
                    "source_type": KnowledgeSource.EXPERT_SYSTEM
                }
            }
        
        def explore_knowledge(self, query, max_results=3):
            """æ¨¡æ‹ŸçŸ¥è¯†æ¢ç´¢"""
            results = []
            
            # åŸºäºæŸ¥è¯¢å…³é”®è¯åŒ¹é…ç»“æœ
            for key, data in self.mock_results.items():
                if any(keyword in query for keyword in key.split()):
                    result = {
                        "id": f"knowledge_{key}_{int(time.time())}",
                        "query": query,
                        "title": data["title"],
                        "content": data["content"],
                        "source_url": data["source_url"],
                        "author": data["author"], 
                        "confidence": data["confidence"],
                        "source_type": data["source_type"],
                        "discovered_at": time.time(),
                        "keywords": key.split() + query.split()
                    }
                    results.append(result)
                    
                    if len(results) >= max_results:
                        break
            
            return results
    
    return MockKnowledgeExplorer()


def create_reasoning_path_from_exploration(exploration_result, logger):
    """ä»çŸ¥è¯†æ¢ç´¢ç»“æœåˆ›å»ºæ¨ç†è·¯å¾„"""
    logger.info(f"ğŸŒ± ä»æ¢ç´¢ç»“æœåˆ›å»ºæ¨ç†è·¯å¾„: {exploration_result['title']}")
    
    # åŸºäºæ¢ç´¢ç»“æœç”Ÿæˆæ¨ç†æ­¥éª¤
    base_steps = [
        "ğŸ¯ æ˜ç¡®é—®é¢˜å’Œç›®æ ‡",
        "ğŸ” æ”¶é›†å’Œåˆ†æç›¸å…³ä¿¡æ¯", 
        "ğŸ’¡ ç”Ÿæˆå¤šä¸ªè§£å†³æ–¹æ¡ˆ",
        "âš–ï¸è¯„ä¼°å’Œé€‰æ‹©æœ€ä½³æ–¹æ¡ˆ",
        "ğŸš€ å®æ–½å¹¶ç›‘æ§æ•ˆæœ"
    ]
    
    # æ ¹æ®å†…å®¹æ·»åŠ ç‰¹å®šæ­¥éª¤
    content = exploration_result['content'].lower()
    specialized_steps = []
    
    if "è®¾è®¡æ€ç»´" in content or "åˆ›æ–°" in content:
        specialized_steps = [
            "ğŸ¤ å»ºç«‹ç”¨æˆ·åŒç†å¿ƒ",
            "ğŸ¨ è¿›è¡Œå¤´è„‘é£æš´",
            "ğŸ› ï¸ åˆ›å»ºå¿«é€ŸåŸå‹",
            "ğŸ§ª è¿›è¡Œç”¨æˆ·æµ‹è¯•"
        ]
    elif "ç³»ç»Ÿ" in content or "åˆ†æ" in content:
        specialized_steps = [
            "ğŸ“Š ç³»ç»Ÿè¾¹ç•Œå®šä¹‰",
            "ğŸ”— è¯†åˆ«å…³é”®è¦ç´ å’Œå…³ç³»", 
            "ğŸ“ˆ å»ºç«‹ç³»ç»Ÿæ¨¡å‹",
            "ğŸ”„ åˆ†æåé¦ˆå¾ªç¯"
        ]
    elif "åä½œ" in content or "å›¢é˜Ÿ" in content:
        specialized_steps = [
            "ğŸ‘¥ ç»„å»ºè·¨åŠŸèƒ½å›¢é˜Ÿ",
            "ğŸ’¬ å»ºç«‹æ²Ÿé€šæœºåˆ¶",
            "ğŸ¯ æ˜ç¡®è§’è‰²å’ŒèŒè´£", 
            "ğŸ¤ ä¿ƒè¿›åä½œå’Œå…±è¯†"
        ]
    
    # åˆå¹¶æ­¥éª¤
    all_steps = base_steps[:2] + specialized_steps + base_steps[2:]
    
    # åˆ›å»ºæ¨ç†è·¯å¾„
    path = ReasoningPath(
        path_id=f"learned_{exploration_result['id']}",
        path_type=f"{exploration_result['title']}å‹",
        description=f"åŸºäº{exploration_result['title']}çš„æ¨ç†æ–¹æ³•",
        prompt_template=f"è¿ç”¨{exploration_result['title']}çš„æ–¹æ³•æ¥è§£å†³{{task}}...",
        name=exploration_result['title'],
        steps=all_steps,
        keywords=exploration_result.get('keywords', []),
        complexity_level=4,
        estimated_steps=len(all_steps),
        success_indicators=["æ–¹æ³•é€‚ç”¨æ€§", "è§£å†³æ•ˆæœ", "å¯å¤ç”¨æ€§"],
        failure_patterns=["æ–¹æ³•ä¸åŒ¹é…", "æ‰§è¡Œå›°éš¾", "æ•ˆæœä¸ä½³"],
        learning_source="learned_exploration",
        confidence_score=exploration_result['confidence'],
        applicable_domains=["é—®é¢˜è§£å†³", "åˆ›æ–°è®¾è®¡", "å›¢é˜Ÿåä½œ"],
        metadata={
            "source": "learned_exploration",
            "learned_from": "çŸ¥è¯†æ¢ç´¢æ¨¡å—",
            "confidence": exploration_result['confidence'],
            "discovery_method": "knowledge_exploration",
            "exploration_query": exploration_result['query']
        }
    )
    
    # æ·»åŠ çŸ¥è¯†æº¯æºä¿¡æ¯
    if hasattr(path, 'add_provenance_source'):
        success = path.add_provenance_source(
            url=exploration_result['source_url'],
            title=exploration_result['title'],
            author=exploration_result.get('author'),
            source_type=exploration_result['source_type'],
            content=exploration_result['content']
        )
        
        if success:
            logger.info(f"   âœ… çŸ¥è¯†æº¯æºä¿¡æ¯å·²æ·»åŠ ")
            
            # æ·»åŠ æ¢ç´¢ç›¸å…³çš„ä¸Šä¸‹æ–‡æ ‡ç­¾
            path.add_context_tag("knowledge_exploration")
            path.add_context_tag("learned_method")
            for keyword in exploration_result.get('keywords', [])[:3]:  # æœ€å¤š3ä¸ªå…³é”®è¯æ ‡ç­¾
                path.add_context_tag(keyword)
        else:
            logger.warning(f"   âš ï¸ çŸ¥è¯†æº¯æºä¿¡æ¯æ·»åŠ å¤±è´¥")
    
    logger.info(f"   ğŸ¯ è·¯å¾„å¤æ‚åº¦: {path.complexity_level}")
    logger.info(f"   ğŸ“Š åˆå§‹ç½®ä¿¡åº¦: {path.confidence_score:.2f}")
    logger.info(f"   ğŸ“ æ­¥éª¤æ•°é‡: {len(path.steps)}")
    
    return path


def demonstrate_end_to_end_learning_cycle(logger):
    """æ¼”ç¤ºç«¯åˆ°ç«¯å­¦ä¹ å¾ªç¯"""
    logger.info("ğŸ”„ === ç«¯åˆ°ç«¯å­¦ä¹ å¾ªç¯æ¼”ç¤º ===")
    
    # 1. åˆå§‹åŒ–å„ä¸ªç»„ä»¶
    print("\n1ï¸âƒ£ åˆå§‹åŒ–è®¤çŸ¥é£è½®ç»„ä»¶...")
    
    knowledge_explorer = create_enhanced_knowledge_explorer()
    path_generator = PathGenerator()
    mab_converger = MABConverger()
    
    print("   âœ… çŸ¥è¯†æ¢ç´¢å™¨å·²å°±ç»ª")
    print("   âœ… è·¯å¾„ç”Ÿæˆå™¨å·²å°±ç»ª") 
    print("   âœ… MABè¯•ç‚¼åœºå·²å°±ç»ª")
    
    # 2. çŸ¥è¯†æ¢ç´¢é˜¶æ®µ
    print("\n2ï¸âƒ£ çŸ¥è¯†æ¢ç´¢é˜¶æ®µ...")
    
    exploration_queries = [
        "åˆ›æ–°æ€ç»´æ–¹æ³•",
        "ç³»ç»Ÿåˆ†ææŠ€æœ¯", 
        "å›¢é˜Ÿåä½œè§£å†³"
    ]
    
    learned_paths = []
    for query in exploration_queries:
        print(f"\n   ğŸ” æ¢ç´¢æŸ¥è¯¢: {query}")
        
        # æ‰§è¡ŒçŸ¥è¯†æ¢ç´¢
        results = knowledge_explorer.explore_knowledge(query, max_results=1)
        
        for result in results:
            print(f"   ğŸ“š å‘ç°: {result['title']}")
            print(f"   ğŸ¯ ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            print(f"   ğŸ“ æ¥æº: {result['source_url']}")
            
            # ä»æ¢ç´¢ç»“æœåˆ›å»ºæ¨ç†è·¯å¾„
            learned_path = create_reasoning_path_from_exploration(result, logger)
            learned_paths.append(learned_path)
            
            # å°†æ–°è·¯å¾„æ·»åŠ åˆ°è·¯å¾„ç”Ÿæˆå™¨
            if hasattr(path_generator, 'add_custom_path'):
                path_generator.add_custom_path(learned_path)
                print(f"   â• è·¯å¾„å·²æ·»åŠ åˆ°åŠ¨æ€åº“")
    
    print(f"\n   ğŸ“Š æ€»å…±å­¦ä¹ åˆ° {len(learned_paths)} ä¸ªæ–°çš„æ¨ç†è·¯å¾„")
    
    # 3. è¯•ç‚¼åœºéªŒè¯é˜¶æ®µ
    print("\n3ï¸âƒ£ è¯•ç‚¼åœºéªŒè¯é˜¶æ®µ...")
    
    print("   ğŸ­ å°†å­¦ä¹ è·¯å¾„æ³¨å…¥è¯•ç‚¼åœºè¿›è¡ŒéªŒè¯...")
    
    # è·å–æ‰€æœ‰å¯ç”¨è·¯å¾„ï¼ˆåŒ…æ‹¬æ–°å­¦ä¹ çš„ï¼‰
    all_paths = []
    try:
        # è·å–åŠ¨æ€è·¯å¾„
        if hasattr(path_generator, 'get_recommended_paths_by_context'):
            recommended_paths = path_generator.get_recommended_paths_by_context("ç»¼åˆé—®é¢˜è§£å†³")
            all_paths.extend(recommended_paths)
    except Exception as e:
        logger.warning(f"   âš ï¸ æ— æ³•è·å–åŠ¨æ€è·¯å¾„: {e}")
    
    # æ·»åŠ å­¦ä¹ åˆ°çš„è·¯å¾„
    all_paths.extend(learned_paths)
    
    print(f"   ğŸ“‹ å¯ç”¨è·¯å¾„æ€»æ•°: {len(all_paths)}")
    
    # æ¨¡æ‹Ÿå¤šè½®è¯•ç‚¼
    trial_rounds = 20
    print(f"\n   ğŸ¯ å¼€å§‹ {trial_rounds} è½®è¯•ç‚¼...")
    
    for round_num in range(1, trial_rounds + 1):
        if round_num % 5 == 1:
            print(f"\n   ğŸ”„ ç¬¬ {round_num}-{min(round_num+4, trial_rounds)} è½®è¯•ç‚¼")
        
        # é€‰æ‹©æœ€ä½³è·¯å¾„
        if all_paths:
            selected_path = mab_converger.select_best_path(all_paths)
            
            # æ¨¡æ‹Ÿæ‰§è¡Œç»“æœ
            # å­¦ä¹ è·¯å¾„åˆæœŸæˆåŠŸç‡è¾ƒä½ï¼Œä½†ä¼šé€æ¸æé«˜
            if hasattr(selected_path, 'is_learned_path') and selected_path.is_learned_path:
                # å­¦ä¹ è·¯å¾„ï¼šéšæ—¶é—´æ”¹å–„çš„æˆåŠŸç‡
                base_success_rate = selected_path.confidence_score * 0.6  # åˆå§‹è¾ƒä½
                improvement = min(round_num / trial_rounds, 1.0) * 0.3    # æœ€å¤šæå‡30%
                success_probability = base_success_rate + improvement
            else:
                # é™æ€è·¯å¾„ï¼šç›¸å¯¹ç¨³å®šçš„æˆåŠŸç‡
                success_probability = 0.75
            
            import random
            success = random.random() < success_probability
            reward = random.uniform(0.7, 1.0) if success else random.uniform(0.1, 0.4)
            
            # æ›´æ–°æ€§èƒ½
            mab_converger.update_path_performance(
                path_id=selected_path.path_id,
                success=success,
                reward=reward,
                source="trial_validation"
            )
            
            # åŒæ—¶æ›´æ–°è·¯å¾„æœ¬èº«çš„ç»Ÿè®¡ï¼ˆå¦‚æœæ”¯æŒï¼‰
            if hasattr(selected_path, 'record_usage'):
                execution_time = 2.0 + random.uniform(-0.5, 1.0)
                selected_path.record_usage(success, execution_time)
            
            if round_num % 5 == 0:
                result_icon = "âœ…" if success else "âŒ"
                boost = ""
                if hasattr(mab_converger, 'get_exploration_boost'):
                    boost_value = mab_converger.get_exploration_boost(selected_path.path_id)
                    if boost_value > 1.0:
                        boost = f" (æ¢ç´¢å¢å¼º: {boost_value:.1f}x)"
                
                print(f"      {result_icon} ç¬¬{round_num}è½®: {selected_path.name[:20]}... å¥–åŠ±: {reward:.2f}{boost}")
    
    # 4. è¯•ç‚¼ç»“æœåˆ†æ
    print("\n4ï¸âƒ£ è¯•ç‚¼ç»“æœåˆ†æ...")
    
    # è·å–è¯•ç‚¼åœºåˆ†æ
    if hasattr(mab_converger, 'get_trial_ground_analytics'):
        analytics = mab_converger.get_trial_ground_analytics()
        
        print(f"   ğŸ“Š è¯•ç‚¼åœºçŠ¶æ€:")
        print(f"      æ€»æ´»è·ƒè·¯å¾„: {analytics['overview']['total_active_paths']}")
        print(f"      å­¦ä¹ è·¯å¾„: {analytics['overview']['learned_paths_count']}")
        print(f"      æ¢ç´¢å¢å¼ºä¸­: {analytics['overview']['exploration_boost_active']}")
        print(f"      æ·˜æ±°å€™é€‰: {analytics['overview']['culling_candidates']}")
        print(f"      é»„é‡‘æ¨¡æ¿: {analytics['overview']['golden_templates']}")
        
        print(f"   ğŸ¯ ç³»ç»Ÿå¥åº·: {analytics['performance_trends']['overall_system_health']}")
        print(f"   ğŸ“ˆ å¹³å‡æˆåŠŸç‡: {analytics['performance_trends']['avg_success_rate']:.3f}")
        
        # åˆ†æå­¦ä¹ è·¯å¾„è¡¨ç°
        if analytics['learned_paths']['active_learned_paths']:
            print(f"\n   ğŸŒ± å­¦ä¹ è·¯å¾„è¯¦ç»†è¡¨ç°:")
            for path_info in analytics['learned_paths']['active_learned_paths']:
                print(f"      ğŸ“‹ {path_info['strategy_id'][:30]}...")
                print(f"         æˆåŠŸç‡: {path_info['success_rate']:.3f}")
                print(f"         æ¿€æ´»æ¬¡æ•°: {path_info['activations']}")
                print(f"         è¯•ç‚¼æ—¶é•¿: {path_info['trial_duration_hours']:.2f} å°æ—¶")
                print(f"         æ¢ç´¢å¢å¼º: {'âœ…' if path_info['has_exploration_boost'] else 'âŒ'}")
                print(f"         æå‡å€™é€‰: {'ğŸ†' if path_info['is_promotion_candidate'] else 'â­'}")
    
    # 5. çŸ¥è¯†æº¯æºè¿½è¸ª
    print("\n5ï¸âƒ£ çŸ¥è¯†æº¯æºè¿½è¸ª...")
    
    print("   ğŸ” å­¦ä¹ è·¯å¾„çš„å®Œæ•´æº¯æºä¿¡æ¯:")
    for i, path in enumerate(learned_paths, 1):
        print(f"\n   è·¯å¾„ #{i}: {path.name}")
        
        summary = path.get_provenance_summary()
        
        key_metrics = [
            ("å­¦ä¹ æ¥æº", summary['learning_source']),
            ("ç½®ä¿¡åº¦", f"{summary['confidence_score']:.3f}"),
            ("éªŒè¯çŠ¶æ€", summary['validation_status']),
            ("ä½¿ç”¨æ¬¡æ•°", summary['usage_count']),
            ("æˆåŠŸç‡", f"{summary['success_rate']:.3f}"),
            ("æ˜¯å¦å·²éªŒè¯", "âœ…" if summary['is_verified'] else "âŒ")
        ]
        
        for label, value in key_metrics:
            print(f"      {label}: {value}")
        
        if summary.get('context_tags'):
            print(f"      ä¸Šä¸‹æ–‡æ ‡ç­¾: {', '.join(list(summary['context_tags'])[:3])}...")
        
        # æ˜¾ç¤ºè¯¦ç»†æº¯æºä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if 'detailed_provenance' in summary:
            detailed = summary['detailed_provenance']
            print(f"      å¹´é¾„: {detailed['age_days']:.1f} å¤©")
            print(f"      æ–°é²œåº¦: {detailed['freshness_score']:.2f}")
            print(f"      æ¥æºæ•°é‡: {detailed['source_count']}")
    
    # 6. è¿›åŒ–å’Œä¼˜åŒ–
    print("\n6ï¸âƒ£ çŸ¥è¯†è¿›åŒ–å’Œä¼˜åŒ–...")
    
    # é€‰æ‹©è¡¨ç°æœ€å¥½çš„å­¦ä¹ è·¯å¾„è¿›è¡Œè¿›åŒ–
    best_learned_path = None
    best_success_rate = 0.0
    
    for path in learned_paths:
        if hasattr(path, 'success_rate') and path.success_rate > best_success_rate:
            best_success_rate = path.success_rate
            best_learned_path = path
    
    if best_learned_path and best_success_rate > 0.6:
        print(f"   ğŸŒŸ æœ€ä½³å­¦ä¹ è·¯å¾„: {best_learned_path.name}")
        print(f"   ğŸ“Š æˆåŠŸç‡: {best_success_rate:.3f}")
        
        # åˆ›å»ºè¿›åŒ–ç‰ˆæœ¬
        if hasattr(best_learned_path, 'create_evolved_version'):
            evolved_changes = [
                "åŸºäºè¯•ç‚¼åé¦ˆä¼˜åŒ–æ­¥éª¤æµç¨‹",
                "å¢å¼ºé”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ",
                "æå‡æ‰§è¡Œæ•ˆç‡å’Œç”¨æˆ·ä½“éªŒ"
            ]
            
            evolved_path = best_learned_path.create_evolved_version(
                changes=evolved_changes,
                reason="åŸºäºè¯•ç‚¼åœºéªŒè¯ç»“æœè¿›è¡Œä¼˜åŒ–å‡çº§"
            )
            
            print(f"   ğŸ§¬ è¿›åŒ–è·¯å¾„: {evolved_path.path_id}")
            print(f"   ğŸ“ˆ è¿›åŒ–ä»£æ•°: {evolved_path.evolution_generation}")
            print(f"   ğŸ“Š è¿›åŒ–åç½®ä¿¡åº¦: {evolved_path.confidence_score:.3f}")
            
            # å°†è¿›åŒ–è·¯å¾„æ·»åŠ å›ç³»ç»Ÿ
            if hasattr(path_generator, 'add_custom_path'):
                path_generator.add_custom_path(evolved_path)
                print(f"   â• è¿›åŒ–è·¯å¾„å·²æ·»åŠ åˆ°åŠ¨æ€åº“")
    
    # 7. ç³»ç»Ÿç»´æŠ¤
    print("\n7ï¸âƒ£ ç³»ç»Ÿç»´æŠ¤...")
    
    if hasattr(mab_converger, 'trigger_trial_ground_maintenance'):
        maintenance_result = mab_converger.trigger_trial_ground_maintenance()
        print(f"   ğŸ”§ æ‰§è¡Œç»´æŠ¤ä»»åŠ¡: {len(maintenance_result['tasks_executed'])} ä¸ª")
        
        if maintenance_result['cleanup_results'].get('culling'):
            culling = maintenance_result['cleanup_results']['culling']
            if culling.get('paths_culled'):
                print(f"   ğŸ—¡ï¸ æ·˜æ±°è·¯å¾„: {len(culling['paths_culled'])} ä¸ª")
            
        if maintenance_result['cleanup_results'].get('expired_boosts'):
            boosts = maintenance_result['cleanup_results']['expired_boosts']
            print(f"   ğŸ§¹ æ¸…ç†è¿‡æœŸæ¢ç´¢å¢å¼º: {boosts['cleaned_count']} ä¸ª")
    
    return {
        'learned_paths': learned_paths,
        'trial_rounds': trial_rounds,
        'system_health': analytics.get('performance_trends', {}).get('overall_system_health', 'unknown') if 'analytics' in locals() else 'unknown'
    }


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger("IntegratedProvenanceDemo", level="INFO")
    
    print("ğŸ”— ========================================")
    print("ğŸ”—    é›†æˆçŸ¥è¯†æº¯æºç³»ç»Ÿå®Œæ•´æ¼”ç¤º")
    print("ğŸ”— ========================================")
    print("ğŸ”— è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºçŸ¥è¯†æº¯æºç³»ç»Ÿä¸è®¤çŸ¥é£è½®çš„å®Œæ•´é›†æˆ:")
    print("ğŸ”— 1. çŸ¥è¯†æ¢ç´¢ â†’ åˆ›å»ºå¸¦æº¯æºçš„æ¨ç†è·¯å¾„")
    print("ğŸ”— 2. åŠ¨æ€è·¯å¾„åº“ â†’ æŒä¹…åŒ–å­¦ä¹ æˆæœ")
    print("ğŸ”— 3. MABè¯•ç‚¼åœº â†’ éªŒè¯å’Œä¼˜é€‰è·¯å¾„")
    print("ğŸ”— 4. çŸ¥è¯†æº¯æº â†’ å…¨ç¨‹è¿½è¸ªå’Œç®¡ç†")
    print("ğŸ”— 5. è·¯å¾„è¿›åŒ– â†’ æŒç»­ä¼˜åŒ–æ”¹è¿›")
    print("ğŸ”— ========================================\n")
    
    try:
        # æ‰§è¡Œç«¯åˆ°ç«¯æ¼”ç¤º
        results = demonstrate_end_to_end_learning_cycle(logger)
        
        print("\nğŸ”— ========================================")
        print("ğŸ”—      é›†æˆæ¼”ç¤ºå®Œæˆï¼")
        print("ğŸ”— ========================================")
        print("ğŸ¯ æ¼”ç¤ºæˆæœæ€»ç»“:")
        print(f"   ğŸŒ± å­¦ä¹ åˆ°æ–°è·¯å¾„: {len(results['learned_paths'])} ä¸ª")
        print(f"   ğŸ­ å®Œæˆè¯•ç‚¼è½®æ¬¡: {results['trial_rounds']} è½®")
        print(f"   ğŸ’Š ç³»ç»Ÿå¥åº·çŠ¶å†µ: {results['system_health']}")
        
        print("\nğŸŒŸ é›†æˆæ•ˆæœ:")
        integration_benefits = [
            "ğŸ” å®Œæ•´çš„çŸ¥è¯†æ¥æºè¿½è¸ªå’ŒéªŒè¯",
            "ğŸ§  ä»æ¢ç´¢åˆ°åº”ç”¨çš„è‡ªåŠ¨åŒ–æµç¨‹",
            "ğŸ­ åŸºäºå®é™…è¡¨ç°çš„è·¯å¾„ä¼˜é€‰",
            "ğŸ“Š å…¨æ–¹ä½çš„æ€§èƒ½åˆ†æå’Œç›‘æ§",
            "ğŸ§¬ æ”¯æŒçŸ¥è¯†è¿›åŒ–å’ŒæŒç»­æ”¹è¿›",
            "ğŸ”— å„ç»„ä»¶é—´çš„æ— ç¼æ•°æ®æµè½¬",
            "âš¡ é«˜æ•ˆçš„å­¦ä¹ -éªŒè¯-ä¼˜åŒ–å¾ªç¯",
            "ğŸ›¡ï¸ å¯é çš„è´¨é‡æ§åˆ¶å’Œé£é™©ç®¡ç†"
        ]
        
        for benefit in integration_benefits:
            print(f"   {benefit}")
        
        print(f"\nğŸ’¡ è¿™ä¸ªé›†æˆç³»ç»Ÿå®ç°äº†çœŸæ­£çš„è‡ªä¸»å­¦ä¹ å’ŒæŒç»­è¿›åŒ–ï¼")
        print(f"ğŸ”„ è®¤çŸ¥é£è½®ç°åœ¨å…·å¤‡äº†å®Œæ•´çš„çŸ¥è¯†ç”Ÿå‘½å‘¨æœŸç®¡ç†èƒ½åŠ›ã€‚")
        
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        logger.info("ğŸ‘‹ é›†æˆæ¼”ç¤ºç»“æŸ")


if __name__ == "__main__":
    main()
