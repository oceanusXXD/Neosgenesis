#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»Ÿä¸€LLMå®¢æˆ·ç«¯æ¥å£ - æ‰€æœ‰LLMå®¢æˆ·ç«¯çš„åŸºç±»å’Œæ•°æ®ç»“æ„å®šä¹‰
Unified LLM Client Interface - Base classes and data structures for all LLM clients

è®¾è®¡ç†å¿µï¼š
- æä¾›ç»Ÿä¸€çš„æ¥å£æŠ½è±¡ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†
- å®šä¹‰æ ‡å‡†çš„å“åº”æ ¼å¼å’Œé”™è¯¯å¤„ç†
- æ”¯æŒå¼‚æ­¥æ“ä½œå’Œæµå¼å“åº”
- å…¼å®¹ç°æœ‰DeepSeekå®ç°çš„åŒæ—¶æ”¯æŒæ‰©å±•
"""

import time
import logging
import asyncio
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Union, Callable, AsyncIterator, Awaitable
from dataclasses import dataclass, field
from enum import Enum

logger = logging.getLogger(__name__)


class LLMProvider(Enum):
    """æ”¯æŒçš„LLMæä¾›å•†æšä¸¾"""
    DEEPSEEK = "deepseek"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    AZURE_OPENAI = "azure_openai"
    HUGGINGFACE = "huggingface"
    OLLAMA = "ollama"
    TOGETHER_AI = "together"
    COHERE = "cohere"
    GOOGLE = "google"
    GROQ = "groq"


class LLMErrorType(Enum):
    """LLMé”™è¯¯ç±»å‹æšä¸¾"""
    AUTHENTICATION = "authentication_error"
    RATE_LIMIT = "rate_limit_error"
    SERVER_ERROR = "server_error"
    NETWORK_ERROR = "network_error"
    TIMEOUT_ERROR = "timeout_error"
    PARSE_ERROR = "parse_error"
    MODEL_ERROR = "model_error"
    QUOTA_EXCEEDED = "quota_exceeded"
    INVALID_REQUEST = "invalid_request"
    UNKNOWN_ERROR = "unknown_error"


@dataclass
class LLMMessage:
    """æ ‡å‡†åŒ–çš„æ¶ˆæ¯æ ¼å¼"""
    role: str  # "system", "user", "assistant"
    content: str
    name: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class LLMUsage:
    """Tokenä½¿ç”¨æƒ…å†µ"""
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    
    @property
    def cost_estimate(self) -> float:
        """ä¼°ç®—æˆæœ¬ï¼ˆåŸºäºé€šç”¨å®šä»·ï¼Œå¯è¢«å­ç±»è¦†ç›–ï¼‰"""
        # åŸºç¡€ä¼°ç®—ï¼šè¾“å…¥0.001/1K tokensï¼Œè¾“å‡º0.002/1K tokens
        return (self.prompt_tokens * 0.001 + self.completion_tokens * 0.002) / 1000


@dataclass
class LLMResponse:
    """ç»Ÿä¸€çš„LLMå“åº”æ ¼å¼"""
    # åŸºç¡€å­—æ®µ
    success: bool
    content: str = ""
    
    # å…ƒæ•°æ®
    provider: str = ""
    model: str = ""
    response_time: float = 0.0
    timestamp: float = field(default_factory=time.time)
    
    # Tokenä½¿ç”¨æƒ…å†µ
    usage: Optional[LLMUsage] = None
    
    # é”™è¯¯ä¿¡æ¯
    error_type: Optional[LLMErrorType] = None
    error_message: str = ""
    error_details: Optional[Dict[str, Any]] = None
    
    # åŸå§‹å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    raw_response: Optional[Dict[str, Any]] = None
    
    # æ‰©å±•å­—æ®µ
    finish_reason: Optional[str] = None
    choices: Optional[List[Dict[str, Any]]] = None
    
    def __post_init__(self):
        """å“åº”åå¤„ç†"""
        if not self.timestamp:
            self.timestamp = time.time()
    
    @property
    def is_error(self) -> bool:
        """æ˜¯å¦ä¸ºé”™è¯¯å“åº”"""
        return not self.success or self.error_type is not None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "success": self.success,
            "content": self.content,
            "provider": self.provider,
            "model": self.model,
            "response_time": self.response_time,
            "timestamp": self.timestamp,
            "usage": self.usage.__dict__ if self.usage else None,
            "error_type": self.error_type.value if self.error_type else None,
            "error_message": self.error_message,
            "finish_reason": self.finish_reason
        }


@dataclass
class LLMConfig:
    """ç»Ÿä¸€çš„LLMé…ç½®åŸºç±»"""
    # åŸºç¡€é…ç½®
    provider: LLMProvider
    api_key: str
    model_name: str = ""
    
    # ç”Ÿæˆå‚æ•°
    temperature: float = 0.7
    max_tokens: int = 2000
    top_p: float = 1.0
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    
    # ç½‘ç»œé…ç½®
    base_url: Optional[str] = None
    timeout: tuple = (30, 180)  # (connect_timeout, read_timeout)
    max_retries: int = 3
    retry_delay_base: float = 2.0
    proxies: Optional[Dict[str, str]] = None
    
    # æ€§èƒ½é…ç½®
    enable_cache: bool = True
    cache_ttl: int = 300
    request_interval: float = 1.0
    
    # æ‰©å±•é…ç½®
    extra_headers: Optional[Dict[str, str]] = None
    extra_params: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        """é…ç½®åå¤„ç†"""
        if not self.model_name:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
            default_models = {
                LLMProvider.DEEPSEEK: "deepseek-chat",
                LLMProvider.OPENAI: "gpt-3.5-turbo",
                LLMProvider.ANTHROPIC: "claude-3-sonnet-20240229",
                LLMProvider.AZURE_OPENAI: "gpt-35-turbo",
                LLMProvider.HUGGINGFACE: "gpt2",
                LLMProvider.OLLAMA: "llama2",
                LLMProvider.TOGETHER_AI: "togethercomputer/llama-2-7b-chat",
                LLMProvider.COHERE: "command",
                LLMProvider.GOOGLE: "gemini-pro",
                LLMProvider.GROQ: "llama2-70b-4096"
            }
            self.model_name = default_models.get(self.provider, "unknown")


class BaseLLMClient(ABC):
    """
    ç»Ÿä¸€LLMå®¢æˆ·ç«¯æŠ½è±¡åŸºç±»
    
    æ‰€æœ‰LLMå®¢æˆ·ç«¯éƒ½åº”è¯¥ç»§æ‰¿æ­¤ç±»å¹¶å®ç°å…¶æŠ½è±¡æ–¹æ³•ã€‚
    è¿™ä¸ªè®¾è®¡å—åˆ°äº†LangChainå’Œunified-llm-clientçš„å¯å‘ï¼Œ
    ä½†é’ˆå¯¹Meta MABç³»ç»Ÿçš„ç‰¹å®šéœ€æ±‚è¿›è¡Œäº†ä¼˜åŒ–ã€‚
    """
    
    def __init__(self, config: LLMConfig):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            config: LLMé…ç½®å¯¹è±¡
        """
        self.config = config
        self.provider = config.provider
        self.model_name = config.model_name
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_response_time": 0.0,
            "total_tokens": 0,
            "error_counts": {},
            "last_request_time": 0.0
        }
        
        logger.info(f"ğŸ¤– åˆå§‹åŒ–{self.provider.value}å®¢æˆ·ç«¯: {self.model_name}")
    
    @abstractmethod
    def chat_completion(self, 
                       messages: Union[str, List[LLMMessage]], 
                       temperature: Optional[float] = None,
                       max_tokens: Optional[int] = None,
                       **kwargs) -> LLMResponse:
        """
        èŠå¤©å®Œæˆæ¥å£ - æ ¸å¿ƒæŠ½è±¡æ–¹æ³•
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: ç»Ÿä¸€çš„å“åº”å¯¹è±¡
        """
        pass
    
    @abstractmethod
    async def achat_completion(self, 
                              messages: Union[str, List[LLMMessage]], 
                              temperature: Optional[float] = None,
                              max_tokens: Optional[int] = None,
                              **kwargs) -> LLMResponse:
        """
        ğŸš€ å¼‚æ­¥èŠå¤©å®Œæˆæ¥å£ - æ ¸å¿ƒå¼‚æ­¥æŠ½è±¡æ–¹æ³•
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: ç»Ÿä¸€çš„å“åº”å¯¹è±¡
        """
        pass
    
    def call_api(self, prompt: str, 
                 system_message: Optional[str] = None,
                 temperature: Optional[float] = None,
                 **kwargs) -> str:
        """
        ç®€åŒ–çš„APIè°ƒç”¨æ¥å£ - å…¼å®¹ç°æœ‰ä»£ç 
        
        è¿™ä¸ªæ–¹æ³•ä¸ºäº†ä¿æŒä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§è€Œå­˜åœ¨ã€‚
        å†…éƒ¨è°ƒç”¨chat_completionæ–¹æ³•ã€‚
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            temperature: æ¸©åº¦å‚æ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            str: å“åº”å†…å®¹
            
        Raises:
            ConnectionError: å½“æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥æ—¶
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_message:
            messages.append(LLMMessage(role="system", content=system_message))
        messages.append(LLMMessage(role="user", content=prompt))
        
        # è°ƒç”¨chat_completion
        response = self.chat_completion(
            messages=messages,
            temperature=temperature,
            **kwargs
        )
        
        # å…¼å®¹æ€§å¤„ç†
        if response.success:
            return response.content
        else:
            error_msg = f"{self.provider.value} APIè°ƒç”¨å¤±è´¥: {response.error_message}"
            logger.error(error_msg)
            raise ConnectionError(error_msg)
    
    async def acall_api(self, prompt: str, 
                       system_message: Optional[str] = None,
                       temperature: Optional[float] = None,
                       **kwargs) -> str:
        """
        ğŸš€ å¼‚æ­¥ç®€åŒ–çš„APIè°ƒç”¨æ¥å£ - å…¼å®¹ç°æœ‰ä»£ç 
        
        è¿™ä¸ªæ–¹æ³•ä¸ºäº†ä¿æŒä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§è€Œå­˜åœ¨ã€‚
        å†…éƒ¨è°ƒç”¨achat_completionæ–¹æ³•ã€‚
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            temperature: æ¸©åº¦å‚æ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            str: å“åº”å†…å®¹
            
        Raises:
            ConnectionError: å½“æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥æ—¶
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_message:
            messages.append(LLMMessage(role="system", content=system_message))
        messages.append(LLMMessage(role="user", content=prompt))
        
        # ä½¿ç”¨achat_completionæ‰§è¡Œ
        response = await self.achat_completion(messages, temperature=temperature, **kwargs)
        
        if response.success:
            return response.content
        else:
            error_msg = f"{self.provider.value} å¼‚æ­¥APIè°ƒç”¨å¤±è´¥: {response.error_message}"
            logger.error(error_msg)
            raise ConnectionError(error_msg)
    
    @abstractmethod
    def validate_config(self) -> bool:
        """
        éªŒè¯é…ç½®æ˜¯å¦æœ‰æ•ˆ
        
        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        pass
    
    @abstractmethod
    def get_available_models(self) -> List[str]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Returns:
            List[str]: å¯ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
        """
        pass
    
    def get_provider_info(self) -> Dict[str, Any]:
        """
        è·å–æä¾›å•†ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: æä¾›å•†ä¿¡æ¯
        """
        return {
            "provider": self.provider.value,
            "model": self.model_name,
            "base_url": self.config.base_url,
            "supported_features": self.get_supported_features(),
            "stats": self.get_stats()
        }
    
    def get_supported_features(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨
        
        Returns:
            List[str]: æ”¯æŒçš„åŠŸèƒ½
        """
        # åŸºç¡€åŠŸèƒ½
        features = ["chat_completion", "text_generation"]
        
        # å­ç±»å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•æ·»åŠ æ›´å¤šåŠŸèƒ½
        return features
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        stats = self.stats.copy()
        
        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        if stats["total_requests"] > 0:
            stats["success_rate"] = stats["successful_requests"] / stats["total_requests"]
            stats["average_response_time"] = stats["total_response_time"] / stats["successful_requests"] if stats["successful_requests"] > 0 else 0
        else:
            stats["success_rate"] = 0.0
            stats["average_response_time"] = 0.0
        
        return stats
    
    def _update_stats(self, response: LLMResponse):
        """
        æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        
        Args:
            response: LLMå“åº”å¯¹è±¡
        """
        self.stats["total_requests"] += 1
        self.stats["last_request_time"] = time.time()
        
        if response.success:
            self.stats["successful_requests"] += 1
            self.stats["total_response_time"] += response.response_time
            
            if response.usage:
                self.stats["total_tokens"] += response.usage.total_tokens
        else:
            self.stats["failed_requests"] += 1
            
            if response.error_type:
                error_key = response.error_type.value
                self.stats["error_counts"][error_key] = self.stats["error_counts"].get(error_key, 0) + 1
    
    def _prepare_messages(self, messages: Union[str, List[LLMMessage]]) -> List[LLMMessage]:
        """
        å‡†å¤‡æ¶ˆæ¯æ ¼å¼
        
        Args:
            messages: è¾“å…¥æ¶ˆæ¯
            
        Returns:
            List[LLMMessage]: æ ‡å‡†åŒ–çš„æ¶ˆæ¯åˆ—è¡¨
        """
        if isinstance(messages, str):
            return [LLMMessage(role="user", content=messages)]
        elif isinstance(messages, list):
            # ç¡®ä¿éƒ½æ˜¯LLMMessageå¯¹è±¡
            result = []
            for msg in messages:
                if isinstance(msg, LLMMessage):
                    result.append(msg)
                elif isinstance(msg, dict):
                    result.append(LLMMessage(
                        role=msg.get("role", "user"),
                        content=msg.get("content", ""),
                        name=msg.get("name"),
                        metadata=msg.get("metadata")
                    ))
                else:
                    # å‡è®¾æ˜¯å­—ç¬¦ä¸²
                    result.append(LLMMessage(role="user", content=str(msg)))
            return result
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¶ˆæ¯æ ¼å¼: {type(messages)}")
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_response_time": 0.0,
            "total_tokens": 0,
            "error_counts": {},
            "last_request_time": 0.0
        }
        logger.info(f"ğŸ”„ {self.provider.value}å®¢æˆ·ç«¯ç»Ÿè®¡å·²é‡ç½®")
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"{self.provider.value}Client(model={self.model_name})"
    
    def __repr__(self) -> str:
        """è°ƒè¯•è¡¨ç¤º"""
        return f"{self.__class__.__name__}(provider={self.provider.value}, model={self.model_name})"


# å·¥å…·å‡½æ•°

def create_llm_config(provider: str, api_key: str, **kwargs) -> LLMConfig:
    """
    ä¾¿æ·çš„é…ç½®åˆ›å»ºå‡½æ•°
    
    Args:
        provider: æä¾›å•†åç§°
        api_key: APIå¯†é’¥
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        LLMConfig: é…ç½®å¯¹è±¡
    """
    provider_enum = LLMProvider(provider.lower())
    
    return LLMConfig(
        provider=provider_enum,
        api_key=api_key,
        **kwargs
    )


def create_error_response(provider: str, error_type: LLMErrorType, 
                         error_message: str, **kwargs) -> LLMResponse:
    """
    åˆ›å»ºé”™è¯¯å“åº”çš„ä¾¿æ·å‡½æ•°
    
    Args:
        provider: æä¾›å•†åç§°
        error_type: é”™è¯¯ç±»å‹
        error_message: é”™è¯¯æ¶ˆæ¯
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        LLMResponse: é”™è¯¯å“åº”å¯¹è±¡
    """
    return LLMResponse(
        success=False,
        provider=provider,
        error_type=error_type,
        error_message=error_message,
        **kwargs
    )


def parse_messages_from_dict(messages_data: List[Dict[str, Any]]) -> List[LLMMessage]:
    """
    ä»å­—å…¸åˆ—è¡¨è§£ææ¶ˆæ¯
    
    Args:
        messages_data: å­—å…¸æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        
    Returns:
        List[LLMMessage]: è§£æåçš„æ¶ˆæ¯åˆ—è¡¨
    """
    return [
        LLMMessage(
            role=msg.get("role", "user"),
            content=msg.get("content", ""),
            name=msg.get("name"),
            metadata=msg.get("metadata")
        )
        for msg in messages_data
    ]
