#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è‡ªä¸»çŸ¥è¯†æ¢å‹˜æ¨¡å— - Knowledge Explorer
Agentè¿æ¥å¤–éƒ¨æ™ºæ…§çš„æ¡¥æ¢

è¿™ä¸ªæ¨¡å—å®ç°äº†è®¤çŸ¥é£è½®çš„"æ¢ç´¢è§¦è§’"ï¼Œè´Ÿè´£ï¼š
1. ä¸»åŠ¨æ¢å‹˜å¤–éƒ¨ä¸–ç•Œçš„æ–°çŸ¥è¯†å’Œæ™ºæ…§
2. å¤šæºä¿¡æ¯çš„è·å–ã€æ•´åˆå’Œè´¨é‡è¯„ä¼°
3. åŸºäºæ¢ç´¢ç»“æœç”Ÿæˆæ–°çš„æ€ç»´ç§å­
4. ä¸ºè®¤çŸ¥é£è½®æä¾›æŒç»­çš„"è¥å…»è¾“å…¥"

æ ¸å¿ƒç†å¿µï¼šè®©AIä»å†…å‘å‹æ€ç»´æ‰©å±•ä¸ºå¤–å‘å‹è®¤çŸ¥æ¢ç´¢è€…
"""

import time
import random
import logging
import asyncio
import hashlib
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
import json

try:
    from .search_tools import WebSearchClient
except ImportError:
    WebSearchClient = None

try:
    from ..cognitive_engine.semantic_analyzer import SemanticAnalyzer, AnalysisTaskType
except ImportError:
    SemanticAnalyzer = None
    AnalysisTaskType = None
    logger.error("âŒ SemanticAnalyzer å¯¼å…¥å¤±è´¥ï¼ŒKnowledgeExplorerå°†ä½¿ç”¨é»˜è®¤ç­–ç•¥è¿è¡Œ")

logger = logging.getLogger(__name__)


class ExplorationStrategy(Enum):
    """æ¢ç´¢ç­–ç•¥æšä¸¾"""
    DOMAIN_EXPANSION = "domain_expansion"           # é¢†åŸŸæ‰©å±•æ¢ç´¢
    TREND_MONITORING = "trend_monitoring"           # è¶‹åŠ¿ç›‘æ§æ¢ç´¢
    GAP_ANALYSIS = "gap_analysis"                   # çŸ¥è¯†ç¼ºå£åˆ†æ
    CROSS_DOMAIN_LEARNING = "cross_domain_learning" # è·¨åŸŸå­¦ä¹ æ¢ç´¢
    SERENDIPITY_DISCOVERY = "serendipity_discovery" # å¶ç„¶å‘ç°æ¢ç´¢
    EXPERT_KNOWLEDGE = "expert_knowledge"           # ä¸“å®¶çŸ¥è¯†è·å–
    COMPETITIVE_INTELLIGENCE = "competitive_intelligence" # ç«äº‰æƒ…æŠ¥åˆ†æ


class KnowledgeQuality(Enum):
    """çŸ¥è¯†è´¨é‡ç­‰çº§"""
    EXCELLENT = "excellent"     # ä¼˜ç§€ï¼šé«˜å¯ä¿¡åº¦ã€é«˜åˆ›æ–°æ€§
    GOOD = "good"              # è‰¯å¥½ï¼šä¸­ç­‰è´¨é‡ï¼Œæœ‰ä¸€å®šä»·å€¼
    FAIR = "fair"              # ä¸€èˆ¬ï¼šåŸºç¡€ä¿¡æ¯ï¼Œå‚è€ƒä»·å€¼
    POOR = "poor"              # è¾ƒå·®ï¼šè´¨é‡å­˜ç–‘ï¼Œéœ€è¦éªŒè¯
    UNRELIABLE = "unreliable"   # ä¸å¯é ï¼šä¸å»ºè®®ä½¿ç”¨


@dataclass
class ExplorationTarget:
    """æ¢ç´¢ç›®æ ‡æ•°æ®ç»“æ„"""
    target_id: str
    target_type: str  # "concept", "trend", "technology", "methodology", "domain"
    description: str
    keywords: List[str] = field(default_factory=list)
    priority: float = 0.5  # 0-1, 1ä¸ºæœ€é«˜ä¼˜å…ˆçº§
    exploration_depth: int = 1  # æ¢ç´¢æ·±åº¦å±‚çº§
    context: Dict[str, Any] = field(default_factory=dict)
    created_at: float = field(default_factory=time.time)


@dataclass
class KnowledgeItem:
    """çŸ¥è¯†é¡¹æ•°æ®ç»“æ„"""
    knowledge_id: str
    content: str
    source: str
    source_type: str  # "web_search", "api_call", "database", "expert_system"
    quality: KnowledgeQuality
    confidence_score: float = 0.5  # 0-1, ç½®ä¿¡åº¦åˆ†æ•°
    relevance_score: float = 0.5   # 0-1, ç›¸å…³æ€§åˆ†æ•°
    novelty_score: float = 0.5     # 0-1, æ–°é¢–æ€§åˆ†æ•°
    
    # å…ƒæ•°æ®
    extraction_method: str = ""
    language: str = "zh"
    discovered_at: float = field(default_factory=time.time)
    tags: List[str] = field(default_factory=list)
    related_concepts: List[str] = field(default_factory=list)
    
    # éªŒè¯çŠ¶æ€
    is_verified: bool = False
    verification_method: str = ""
    verification_score: float = 0.0


@dataclass
class ThinkingSeed:
    """æ€ç»´ç§å­æ•°æ®ç»“æ„"""
    seed_id: str
    seed_content: str
    source_knowledge: List[str] = field(default_factory=list)  # æ¥æºçŸ¥è¯†é¡¹IDåˆ—è¡¨
    creativity_level: str = "medium"  # "low", "medium", "high"
    potential_applications: List[str] = field(default_factory=list)
    generated_strategy: str = ""
    confidence: float = 0.5
    
    # è®¤çŸ¥è·¯å¾„ç›¸å…³
    suggested_reasoning_paths: List[str] = field(default_factory=list)
    cross_domain_connections: List[str] = field(default_factory=list)
    
    # å…ƒæ•°æ®
    generated_at: float = field(default_factory=time.time)
    generation_context: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ExplorationResult:
    """æ¢ç´¢ç»“æœæ•°æ®ç»“æ„"""
    exploration_id: str
    strategy: ExplorationStrategy
    targets: List[ExplorationTarget]
    
    # æ¢ç´¢æˆæœ
    discovered_knowledge: List[KnowledgeItem] = field(default_factory=list)
    generated_seeds: List[ThinkingSeed] = field(default_factory=list)
    identified_trends: List[Dict[str, Any]] = field(default_factory=list)
    cross_domain_insights: List[Dict[str, Any]] = field(default_factory=list)
    
    # æ‰§è¡Œç»Ÿè®¡
    execution_time: float = 0.0
    success_rate: float = 0.0
    quality_score: float = 0.0
    
    # å…ƒæ•°æ®
    timestamp: float = field(default_factory=time.time)
    context: Dict[str, Any] = field(default_factory=dict)


class KnowledgeExplorer:
    """
    ğŸŒ è‡ªä¸»çŸ¥è¯†æ¢å‹˜æ¨¡å— - Agentçš„"å¤–éƒ¨æ™ºæ…§è¿æ¥å™¨"
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. ä¸»åŠ¨æ¢å‹˜å¤–éƒ¨ä¸–ç•Œçš„æ–°çŸ¥è¯†å’Œè¶‹åŠ¿
    2. å¤šæºä¿¡æ¯è·å–ï¼šç½‘ç»œæœç´¢ã€APIè°ƒç”¨ã€ä¸“å®¶ç³»ç»Ÿ
    3. æ™ºèƒ½çŸ¥è¯†è´¨é‡è¯„ä¼°å’Œè¿‡æ»¤æœºåˆ¶
    4. åŸºäºæ¢ç´¢å‘ç°ç”Ÿæˆé«˜è´¨é‡æ€ç»´ç§å­
    5. ä¸ºè®¤çŸ¥é£è½®æä¾›æŒç»­çš„çŸ¥è¯†"è¥å…»è¾“å…¥"
    
    è®¾è®¡åŸåˆ™ï¼š
    - ä¸»åŠ¨æ€§ï¼šä¸ç­‰å¾…æŒ‡ä»¤ï¼Œä¸»åŠ¨å‘ç°æœºä¼š
    - å¤šæ ·æ€§ï¼šæ”¯æŒå¤šç§æ¢ç´¢ç­–ç•¥å’Œä¿¡æ¯æº
    - è´¨é‡ä¼˜å…ˆï¼šä¸¥æ ¼çš„çŸ¥è¯†è´¨é‡è¯„ä¼°ä½“ç³»
    - é€‚åº”æ€§ï¼šæ ¹æ®æ¢ç´¢æ•ˆæœåŠ¨æ€è°ƒæ•´ç­–ç•¥
    """
    
    def __init__(self, 
                 llm_client=None,
                 web_search_client=None,
                 semantic_analyzer=None,
                 config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–çŸ¥è¯†æ¢å‹˜æ¨¡å—
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºæ™ºèƒ½åˆ†æï¼‰
            web_search_client: ç½‘ç»œæœç´¢å®¢æˆ·ç«¯
            semantic_analyzer: è¯­ä¹‰åˆ†æå™¨ï¼ˆç”¨äºæ™ºèƒ½æ„å›¾ç†è§£å’Œç­–ç•¥é€‰æ‹©ï¼‰
            config: æ¢å‹˜é…ç½®å‚æ•°
        """
        self.llm_client = llm_client
        self.web_search_client = web_search_client
        self.semantic_analyzer = semantic_analyzer
        
        # é…ç½®å‚æ•°
        self.config = {
            # æ¢ç´¢ç­–ç•¥é…ç½®
            "exploration_strategies": {
                "default_strategy": ExplorationStrategy.DOMAIN_EXPANSION,
                "strategy_rotation": True,              # æ˜¯å¦è½®æ¢ç­–ç•¥
                "max_parallel_explorations": 3,        # æœ€å¤§å¹¶è¡Œæ¢ç´¢æ•°
                "exploration_timeout": 120.0           # æ¢ç´¢è¶…æ—¶æ—¶é—´
            },
            
            # è´¨é‡æ§åˆ¶é…ç½®
            "quality_control": {
                "min_confidence_threshold": 0.4,       # æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
                "min_relevance_threshold": 0.3,        # æœ€å°ç›¸å…³æ€§é˜ˆå€¼
                "enable_cross_validation": True,       # å¯ç”¨äº¤å‰éªŒè¯
                "quality_decay_factor": 0.1            # è´¨é‡è¡°å‡å› å­
            },
            
            # ç§å­ç”Ÿæˆé…ç½®
            "seed_generation": {
                "max_seeds_per_exploration": 5,        # æ¯æ¬¡æ¢ç´¢æœ€å¤§ç§å­æ•°
                "creativity_boost_factor": 1.2,        # åˆ›æ„æå‡å› å­
                "cross_domain_bonus": 0.3,            # è·¨åŸŸè¿æ¥å¥–åŠ±
                "enable_serendipity": True             # å¯ç”¨å¶ç„¶å‘ç°
            },
            
            # ä¿¡æ¯æºé…ç½®
            "information_sources": {
                "enable_web_search": True,             # å¯ç”¨ç½‘ç»œæœç´¢
                "enable_api_calls": False,             # å¯ç”¨APIè°ƒç”¨
                "enable_database_query": False,       # å¯ç”¨æ•°æ®åº“æŸ¥è¯¢
                "max_results_per_source": 10          # æ¯ä¸ªä¿¡æ¯æºæœ€å¤§ç»“æœæ•°
            }
        }
        
        # åˆå¹¶ç”¨æˆ·é…ç½®
        if config:
            self._merge_config(self.config, config)
        
        # åˆå§‹åŒ–ä¿¡æ¯æºå®¢æˆ·ç«¯
        if not self.web_search_client and self.config["information_sources"]["enable_web_search"]:
            try:
                if WebSearchClient:
                    self.web_search_client = WebSearchClient(
                        max_results=self.config["information_sources"]["max_results_per_source"]
                    )
                    logger.info("ğŸŒ ç½‘ç»œæœç´¢å®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
                else:
                    logger.warning("âš ï¸ WebSearchClient ä¸å¯ç”¨ï¼Œç½‘ç»œæœç´¢åŠŸèƒ½å°†è¢«ç¦ç”¨")
            except Exception as e:
                logger.warning(f"âš ï¸ ç½‘ç»œæœç´¢å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # æ¢ç´¢å†å²å’Œç»Ÿè®¡
        self.exploration_history: List[ExplorationResult] = []
        self.knowledge_cache: Dict[str, KnowledgeItem] = {}
        self.seed_cache: Dict[str, ThinkingSeed] = {}
        
        # ç­–ç•¥æ€§èƒ½ç»Ÿè®¡
        self.strategy_performance: Dict[str, Dict[str, float]] = defaultdict(
            lambda: {"success_rate": 0.0, "avg_quality": 0.0, "total_seeds": 0}
        )
        
        # æ¢ç´¢ç›®æ ‡ç®¡ç†
        self.active_targets: List[ExplorationTarget] = []
        self.completed_targets: List[ExplorationTarget] = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_explorations": 0,
            "successful_explorations": 0,
            "total_knowledge_discovered": 0,
            "total_seeds_generated": 0,
            "average_quality_score": 0.0,
            "average_execution_time": 0.0
        }
        
        logger.info("ğŸŒ KnowledgeExplorer åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ç½‘ç»œæœç´¢: {'å¯ç”¨' if self.web_search_client else 'ç¦ç”¨'}")
        logger.info(f"   LLMåˆ†æ: {'å¯ç”¨' if self.llm_client else 'ç¦ç”¨'}")
        logger.info("ğŸ” å¤–éƒ¨æ™ºæ…§è¿æ¥å™¨å·²å°±ç»ª - ä¸»åŠ¨æ¢ç´¢æ¨¡å¼å¯åŠ¨")
    
    def explore_knowledge(self, 
                         targets: List[ExplorationTarget],
                         strategy: Optional[ExplorationStrategy] = None,
                         user_context: Optional[Dict[str, Any]] = None) -> ExplorationResult:
        """
        æ‰§è¡ŒçŸ¥è¯†æ¢å‹˜ä»»åŠ¡ - åŒè½¨æ¢ç´¢ç³»ç»Ÿæ ¸å¿ƒå…¥å£
        
        Args:
            targets: æ¢ç´¢ç›®æ ‡åˆ—è¡¨
            strategy: æ¢ç´¢ç­–ç•¥ï¼ˆå¯é€‰ï¼‰
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆç”¨äºç”¨æˆ·æŒ‡ä»¤é©±åŠ¨æ¨¡å¼ï¼‰
            
        Returns:
            å®Œæ•´çš„æ¢ç´¢ç»“æœ
        """
        start_time = time.time()
        exploration_id = f"exploration_{int(time.time() * 1000)}"
        
        # ğŸ¯ æ£€æµ‹æ¢ç´¢æ¨¡å¼
        is_user_directed = user_context and user_context.get("exploration_mode") == "user_directed"
        exploration_mode = "ç”¨æˆ·æŒ‡ä»¤é©±åŠ¨" if is_user_directed else "ç³»ç»Ÿè‡ªä¸»æ¢ç´¢"
        
        # ç¡®å®šæ¢ç´¢ç­–ç•¥
        if not strategy:
            if is_user_directed:
                strategy = self._select_user_directed_strategy(targets, user_context)
            else:
                strategy = self._select_optimal_strategy(targets)
        
        logger.info(f"ğŸŒ å¼€å§‹çŸ¥è¯†æ¢å‹˜: {exploration_id} ({exploration_mode})")
        logger.info(f"   ç­–ç•¥: {strategy.value}")
        logger.info(f"   ç›®æ ‡æ•°é‡: {len(targets)}")
        
        if is_user_directed:
            user_query = user_context.get("user_query", "")
            logger.info(f"   ğŸ¯ ç”¨æˆ·æŸ¥è¯¢: {user_query[:50]}...")
        
        try:
            # åˆ›å»ºæ¢ç´¢ç»“æœå¯¹è±¡
            result = ExplorationResult(
                exploration_id=exploration_id,
                strategy=strategy,
                targets=targets
            )
            
            # æ‰§è¡Œæ¢ç´¢æµç¨‹
            self._execute_exploration_pipeline(result)
            
            # è®¡ç®—æ‰§è¡Œç»Ÿè®¡
            result.execution_time = time.time() - start_time
            result.success_rate = self._calculate_success_rate(result)
            result.quality_score = self._calculate_quality_score(result)
            
            # æ›´æ–°ç¼“å­˜å’Œç»Ÿè®¡
            self._update_caches_and_stats(result)
            
            logger.info(f"âœ… çŸ¥è¯†æ¢å‹˜å®Œæˆ: {exploration_id}")
            logger.info(f"   æ‰§è¡Œæ—¶é—´: {result.execution_time:.2f}s")
            logger.info(f"   å‘ç°çŸ¥è¯†: {len(result.discovered_knowledge)} é¡¹")
            logger.info(f"   ç”Ÿæˆç§å­: {len(result.generated_seeds)} ä¸ª")
            logger.info(f"   è´¨é‡è¯„åˆ†: {result.quality_score:.2f}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†æ¢å‹˜å¤±è´¥: {exploration_id} - {e}")
            # è¿”å›ç©ºç»“æœä½†åŒ…å«é”™è¯¯ä¿¡æ¯
            return ExplorationResult(
                exploration_id=exploration_id,
                strategy=strategy,
                targets=targets,
                execution_time=time.time() - start_time,
                context={"error": str(e)}
            )
    
    def _execute_exploration_pipeline(self, result: ExplorationResult):
        """æ‰§è¡Œå®Œæ•´çš„æ¢ç´¢æµæ°´çº¿"""
        logger.info("ğŸ”„ æ‰§è¡Œæ¢ç´¢æµæ°´çº¿...")
        
        # é˜¶æ®µ1: ä¿¡æ¯æ”¶é›†
        logger.info("ğŸ“¡ é˜¶æ®µ1: ä¿¡æ¯æ”¶é›†")
        raw_information = self._collect_information(result.targets, result.strategy)
        
        # é˜¶æ®µ2: çŸ¥è¯†æå–å’Œè´¨é‡è¯„ä¼°
        logger.info("ğŸ” é˜¶æ®µ2: çŸ¥è¯†æå–å’Œè´¨é‡è¯„ä¼°")
        result.discovered_knowledge = self._extract_and_evaluate_knowledge(
            raw_information, result.targets
        )
        
        # é˜¶æ®µ3: æ€ç»´ç§å­ç”Ÿæˆ
        logger.info("ğŸŒ± é˜¶æ®µ3: æ€ç»´ç§å­ç”Ÿæˆ")
        result.generated_seeds = self._generate_thinking_seeds(
            result.discovered_knowledge, result.strategy
        )
        
        # é˜¶æ®µ4: è¶‹åŠ¿åˆ†æ
        logger.info("ğŸ“ˆ é˜¶æ®µ4: è¶‹åŠ¿åˆ†æ")
        result.identified_trends = self._analyze_trends(
            result.discovered_knowledge, result.targets
        )
        
        # é˜¶æ®µ5: è·¨åŸŸæ´å¯Ÿå‘ç°
        logger.info("ğŸ”— é˜¶æ®µ5: è·¨åŸŸæ´å¯Ÿå‘ç°")
        result.cross_domain_insights = self._discover_cross_domain_insights(
            result.discovered_knowledge, result.generated_seeds
        )
        
        logger.info("âœ… æ¢ç´¢æµæ°´çº¿æ‰§è¡Œå®Œæˆ")
    
    def _collect_information(self, 
                           targets: List[ExplorationTarget], 
                           strategy: ExplorationStrategy) -> List[Dict[str, Any]]:
        """ä¿¡æ¯æ”¶é›†é˜¶æ®µ - ä»å¤šä¸ªä¿¡æ¯æºè·å–åŸå§‹ä¿¡æ¯"""
        raw_information = []
        
        for target in targets[:self.config["exploration_strategies"]["max_parallel_explorations"]]:
            logger.debug(f"ğŸ¯ æ”¶é›†ç›®æ ‡ä¿¡æ¯: {target.target_id}")
            
            # ç½‘ç»œæœç´¢
            if (self.web_search_client and 
                self.config["information_sources"]["enable_web_search"]):
                web_results = self._search_web_information(target, strategy)
                raw_information.extend(web_results)
            
            # APIè°ƒç”¨ï¼ˆé¢„ç•™æ¥å£ï¼‰
            if self.config["information_sources"]["enable_api_calls"]:
                api_results = self._query_api_sources(target, strategy)
                raw_information.extend(api_results)
            
            # æ•°æ®åº“æŸ¥è¯¢ï¼ˆé¢„ç•™æ¥å£ï¼‰
            if self.config["information_sources"]["enable_database_query"]:
                db_results = self._query_database_sources(target, strategy)
                raw_information.extend(db_results)
        
        logger.info(f"ğŸ“¡ ä¿¡æ¯æ”¶é›†å®Œæˆ: è·å– {len(raw_information)} æ¡åŸå§‹ä¿¡æ¯")
        return raw_information
    
    def _search_web_information(self, 
                              target: ExplorationTarget, 
                              strategy: ExplorationStrategy) -> List[Dict[str, Any]]:
        """æ‰§è¡Œç½‘ç»œæœç´¢"""
        if not self.web_search_client:
            return []
        
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_queries = self._build_search_queries(target, strategy)
            web_results = []
            
            for query in search_queries[:3]:  # é™åˆ¶æŸ¥è¯¢æ•°é‡
                logger.debug(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
                
                results = self.web_search_client.search(query)
                for result in results:
                    web_results.append({
                        "content": result.get("snippet", ""),
                        "title": result.get("title", ""),
                        "url": result.get("link", ""),
                        "source": "web_search",
                        "query": query,
                        "target_id": target.target_id,
                        "collected_at": time.time()
                    })
            
            logger.debug(f"ğŸŒ ç½‘ç»œæœç´¢å®Œæˆ: {len(web_results)} æ¡ç»“æœ")
            return web_results
            
        except Exception as e:
            logger.error(f"âŒ ç½‘ç»œæœç´¢å¤±è´¥: {e}")
            return []
    
    def _build_search_queries(self, 
                            target: ExplorationTarget, 
                            strategy: ExplorationStrategy) -> List[str]:
        """ğŸ” æ„å»ºè¯­ä¹‰å¢å¼ºçš„æœç´¢æŸ¥è¯¢ - ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰ç†è§£ï¼Œå›é€€åˆ°å…³é”®è¯æ„å»º"""
        base_keywords = target.keywords + [target.description]
        queries = []
        
        # ğŸ¯ æ ¹æ®æ¢ç´¢æ¨¡å¼å’Œä¼˜å…ˆçº§è°ƒæ•´æœç´¢ç­–ç•¥
        is_user_directed = target.metadata.get("exploration_mode") == "user_directed"
        priority_level = target.priority
        target_type = target.metadata.get("target_type", "general")
        user_query = target.metadata.get("user_query", "")
        
        logger.debug(f"ğŸ” æ„å»ºæœç´¢æŸ¥è¯¢: æ¨¡å¼={target.metadata.get('exploration_mode')}, "
                    f"ä¼˜å…ˆçº§={priority_level}, æŸ¥è¯¢='{user_query[:30]}...'")
        
        # ğŸ§  ä¼˜å…ˆå°è¯•è¯­ä¹‰å¢å¼ºçš„æŸ¥è¯¢æ„å»º
        if is_user_directed and self.semantic_analyzer and user_query:
            try:
                semantic_queries = self._build_semantic_enhanced_queries(user_query, target, strategy, base_keywords)
                if semantic_queries:
                    queries.extend(semantic_queries)
                    logger.debug(f"âœ… ä½¿ç”¨è¯­ä¹‰å¢å¼ºæŸ¥è¯¢: ç”Ÿæˆ {len(semantic_queries)} ä¸ªæŸ¥è¯¢")
                else:
                    # è¯­ä¹‰å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
                    queries.extend(self._build_traditional_queries(target, strategy, base_keywords))
                    logger.debug("ğŸ”„ è¯­ä¹‰å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»ŸæŸ¥è¯¢æ„å»º")
            except Exception as e:
                logger.warning(f"âš ï¸ è¯­ä¹‰å¢å¼ºæŸ¥è¯¢æ„å»ºå¤±è´¥: {e}")
                queries.extend(self._build_traditional_queries(target, strategy, base_keywords))
        else:
            # ä½¿ç”¨ä¼ ç»ŸæŸ¥è¯¢æ„å»ºæ–¹æ³•
            queries.extend(self._build_traditional_queries(target, strategy, base_keywords))
        
        # æ ¹æ®ä¼˜å…ˆçº§é™åˆ¶æŸ¥è¯¢æ•°é‡
        if priority_level == ExplorationPriority.HIGH:
            max_queries = 8  # é«˜ä¼˜å…ˆçº§å…è®¸æ›´å¤šæŸ¥è¯¢
        elif priority_level == ExplorationPriority.MEDIUM:
            max_queries = 6
        else:
            max_queries = 4  # ä½ä¼˜å…ˆçº§é™åˆ¶æŸ¥è¯¢æ•°é‡
        
        final_queries = queries[:max_queries]
        logger.debug(f"ğŸ” æœ€ç»ˆç”Ÿæˆ {len(final_queries)} ä¸ªæœç´¢æŸ¥è¯¢ (æœ€å¤§: {max_queries})")
        return final_queries
    
    def _build_semantic_enhanced_queries(self, user_query: str, target: ExplorationTarget, 
                                       strategy: ExplorationStrategy, base_keywords: List[str]) -> List[str]:
        """ğŸ§  åŸºäºè¯­ä¹‰åˆ†ææ„å»ºæ™ºèƒ½æœç´¢æŸ¥è¯¢"""
        try:
            # æ‰§è¡Œè¯­ä¹‰åˆ†æ
            analysis_tasks = ['intent_detection', 'domain_classification', 'keyword_extraction']
            response = self.semantic_analyzer.analyze(user_query, analysis_tasks)
            
            if not response.overall_success:
                logger.warning("âš ï¸ è¯­ä¹‰åˆ†æå¤±è´¥ï¼Œæ— æ³•æ„å»ºè¯­ä¹‰å¢å¼ºæŸ¥è¯¢")
                return []
            
            # æå–åˆ†æç»“æœ
            intent_result = response.analysis_results.get('intent_detection')
            domain_result = response.analysis_results.get('domain_classification')
            keyword_result = response.analysis_results.get('keyword_extraction')
            
            intent = None
            domain = None
            semantic_keywords = []
            
            confidence_threshold = 0.7
            
            if intent_result and intent_result.success and intent_result.confidence > confidence_threshold:
                intent = intent_result.result.get('primary_intent')
                
            if domain_result and domain_result.success and domain_result.confidence > confidence_threshold:
                domain = domain_result.result.get('domain')
                
            if keyword_result and keyword_result.success:
                semantic_keywords = keyword_result.result.get('keywords', [])[:5]
            
            logger.debug(f"ğŸ§  è¯­ä¹‰åˆ†ææå–: intent={intent}, domain={domain}, keywords={semantic_keywords}")
            
            # åŸºäºè¯­ä¹‰åˆ†æç»“æœç”ŸæˆæŸ¥è¯¢
            semantic_queries = self._generate_queries_from_semantics(
                user_query, intent, domain, semantic_keywords, strategy, target.metadata
            )
            
            return semantic_queries
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰å¢å¼ºæŸ¥è¯¢æ„å»ºå¼‚å¸¸: {e}")
            return []
    
    def _generate_queries_from_semantics(self, user_query: str, intent: Optional[str], 
                                       domain: Optional[str], semantic_keywords: List[str],
                                       strategy: ExplorationStrategy, metadata: Dict[str, Any]) -> List[str]:
        """ğŸ¯ åŸºäºè¯­ä¹‰è¦ç´ ç”Ÿæˆæœç´¢æŸ¥è¯¢"""
        queries = []
        
        # ä½¿ç”¨åŸå§‹æŸ¥è¯¢ä½œä¸ºåŸºç¡€
        queries.append(user_query)
        
        # åŸºäºæ„å›¾ç”Ÿæˆä¸“é—¨åŒ–æŸ¥è¯¢
        if intent:
            intent_queries = self._get_intent_specific_queries(user_query, intent, semantic_keywords)
            queries.extend(intent_queries)
        
        # åŸºäºé¢†åŸŸç”Ÿæˆä¸“ä¸šåŒ–æŸ¥è¯¢
        if domain:
            domain_queries = self._get_domain_specific_queries(user_query, domain, semantic_keywords)
            queries.extend(domain_queries)
        
        # åŸºäºç­–ç•¥ç”Ÿæˆå¢å¼ºæŸ¥è¯¢
        strategy_queries = self._get_strategy_enhanced_queries(user_query, strategy, semantic_keywords)
        queries.extend(strategy_queries)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        unique_queries = []
        seen = set()
        for query in queries:
            if query and query not in seen:
                unique_queries.append(query)
                seen.add(query)
        
        logger.debug(f"ğŸ§  è¯­ä¹‰ç”ŸæˆæŸ¥è¯¢: {len(unique_queries)} ä¸ª")
        return unique_queries
    
    def _get_intent_specific_queries(self, user_query: str, intent: str, keywords: List[str]) -> List[str]:
        """ğŸ¯ åŸºäºæ„å›¾ç”Ÿæˆç‰¹å®šæŸ¥è¯¢"""
        queries = []
        main_keywords = keywords[:3] if keywords else user_query.split()[:3]
        
        intent_templates = {
            'solution_seeking': [
                "{} è§£å†³æ–¹æ¡ˆ", "{} æœ€ä½³å®è·µ", "{} å®ç°æ–¹æ³•", 
                "{} ä¸“ä¸šæŒ‡å—", "{} è¯¦ç»†æ•™ç¨‹"
            ],
            'comparison_analysis': [
                "{} å¯¹æ¯”åˆ†æ", "{} ä¼˜åŠ£æ¯”è¾ƒ", "{} é€‰æ‹©æŒ‡å—",
                "{} vs æ›¿ä»£æ–¹æ¡ˆ", "{} æ€§èƒ½å¯¹æ¯”"
            ],
            'trend_monitoring': [
                "{} æœ€æ–°è¶‹åŠ¿", "{} 2024å‘å±•", "{} æœªæ¥æ–¹å‘",
                "{} åˆ›æ–°åŠ¨æ€", "{} æŠ€æœ¯æ¼”è¿›"
            ],
            'learning_request': [
                "{} åŸºç¡€çŸ¥è¯†", "{} å­¦ä¹ æŒ‡å—", "{} å…¥é—¨æ•™ç¨‹",
                "{} æ·±å…¥ç†è§£", "{} æ ¸å¿ƒæ¦‚å¿µ"
            ],
            'problem_diagnosis': [
                "{} å¸¸è§é—®é¢˜", "{} æ•…éšœæ’é™¤", "{} é—®é¢˜åˆ†æ",
                "{} è¯Šæ–­æ–¹æ³•", "{} è§£å†³æ–¹æ¡ˆ"
            ]
        }
        
        templates = intent_templates.get(intent, ["{} è¯¦ç»†ä¿¡æ¯"])
        
        # ä¸ºæ¯ä¸ªä¸»è¦å…³é”®è¯ç”ŸæˆæŸ¥è¯¢
        for keyword in main_keywords:
            for template in templates[:2]:  # é™åˆ¶æ¯ä¸ªæ„å›¾çš„æŸ¥è¯¢æ•°é‡
                query = template.format(keyword)
                queries.append(query)
        
        return queries[:4]  # é™åˆ¶æ€»æ•°
    
    def _get_domain_specific_queries(self, user_query: str, domain: str, keywords: List[str]) -> List[str]:
        """ğŸ¯ åŸºäºé¢†åŸŸç”Ÿæˆç‰¹å®šæŸ¥è¯¢"""
        queries = []
        main_keywords = keywords[:3] if keywords else user_query.split()[:3]
        
        domain_templates = {
            'technology': [
                "{} æŠ€æœ¯åŸç†", "{} æ¶æ„è®¾è®¡", "{} æ€§èƒ½ä¼˜åŒ–",
                "{} å®ç°ç»†èŠ‚", "{} æœ€æ–°æŠ€æœ¯"
            ],
            'business': [
                "{} å•†ä¸šæ¨¡å¼", "{} å¸‚åœºåˆ†æ", "{} æŠ•èµ„ä»·å€¼",
                "{} å•†ä¸šæ¡ˆä¾‹", "{} ç›ˆåˆ©ç­–ç•¥"
            ],
            'academic': [
                "{} ç ”ç©¶ç°çŠ¶", "{} å­¦æœ¯è§‚ç‚¹", "{} ç†è®ºåŸºç¡€",
                "{} ç ”ç©¶æ–¹æ³•", "{} å‰æ²¿ç ”ç©¶"
            ],
            'health': [
                "{} å¥åº·å½±å“", "{} åŒ»ç–—åº”ç”¨", "{} å®‰å…¨æ€§",
                "{} ä¸´åºŠç ”ç©¶", "{} ä¸“ä¸šå»ºè®®"
            ]
        }
        
        templates = domain_templates.get(domain, ["{} ä¸“ä¸šåˆ†æ"])
        
        # ä¸ºæ¯ä¸ªå…³é”®è¯ç”Ÿæˆé¢†åŸŸç‰¹å®šæŸ¥è¯¢
        for keyword in main_keywords:
            for template in templates[:2]:
                query = template.format(keyword)
                queries.append(query)
        
        return queries[:3]  # é™åˆ¶æ•°é‡
    
    def _get_strategy_enhanced_queries(self, user_query: str, strategy: ExplorationStrategy, keywords: List[str]) -> List[str]:
        """ğŸ¯ åŸºäºæ¢ç´¢ç­–ç•¥ç”Ÿæˆå¢å¼ºæŸ¥è¯¢"""
        queries = []
        main_keywords = keywords[:2] if keywords else user_query.split()[:2]
        
        strategy_templates = {
            ExplorationStrategy.EXPERT_KNOWLEDGE: [
                "{} ä¸“å®¶è§‚ç‚¹", "{} æƒå¨æŒ‡å—", "{} ä¸“ä¸šæ–¹æ³•è®º"
            ],
            ExplorationStrategy.TREND_MONITORING: [
                "{} æœ€æ–°è¶‹åŠ¿", "{} å‘å±•åŠ¨æ€", "{} æœªæ¥å±•æœ›"
            ],
            ExplorationStrategy.COMPETITIVE_INTELLIGENCE: [
                "{} ç«äº‰åˆ†æ", "{} å¸‚åœºå¯¹æ¯”", "{} ä¼˜åŠ¿è¯„ä¼°"
            ],
            ExplorationStrategy.CROSS_DOMAIN_LEARNING: [
                "{} è·¨é¢†åŸŸåº”ç”¨", "{} åˆ›æ–°èåˆ", "{} è·¨ç•Œæ¡ˆä¾‹"
            ],
            ExplorationStrategy.DOMAIN_EXPANSION: [
                "{} åº”ç”¨é¢†åŸŸ", "{} ç›¸å…³æŠ€æœ¯", "{} æ‰©å±•åº”ç”¨"
            ],
            ExplorationStrategy.GAP_ANALYSIS: [
                "{} æŠ€æœ¯ç“¶é¢ˆ", "{} æŒ‘æˆ˜é—®é¢˜", "{} è§£å†³æ–¹æ¡ˆ"
            ]
        }
        
        templates = strategy_templates.get(strategy, ["{} æ·±å…¥åˆ†æ"])
        
        for keyword in main_keywords:
            for template in templates:
                query = template.format(keyword)
                queries.append(query)
        
        return queries[:3]  # é™åˆ¶æ•°é‡
    
    def _build_traditional_queries(self, target: ExplorationTarget, strategy: ExplorationStrategy, 
                                 base_keywords: List[str]) -> List[str]:
        """ğŸ”„ ä¼ ç»ŸæŸ¥è¯¢æ„å»ºæ–¹æ³•ï¼ˆå›é€€æœºåˆ¶ï¼‰"""
        is_user_directed = target.metadata.get("exploration_mode") == "user_directed"
        
        if is_user_directed:
            # ç”¨æˆ·æŒ‡ä»¤é©±åŠ¨ï¼šæ·±å…¥ã€å¤šæ ·åŒ–ã€é«˜è´¨é‡æœç´¢
            return self._build_user_directed_queries(target, strategy, base_keywords)
        else:
            # ç³»ç»Ÿè‡ªä¸»æ¢ç´¢ï¼šå®½æ³›ã€å‘æ•£ã€æ¢ç´¢æ€§æœç´¢
            return self._build_autonomous_queries(target, strategy, base_keywords)
    
    def _build_user_directed_queries(self, target: ExplorationTarget, 
                                   strategy: ExplorationStrategy, 
                                   base_keywords: List[str]) -> List[str]:
        """ğŸ¯ æ„å»ºç”¨æˆ·æŒ‡ä»¤é©±åŠ¨çš„æ·±å…¥ã€å¤šæ ·åŒ–æœç´¢æŸ¥è¯¢"""
        queries = []
        search_depth = target.metadata.get("search_depth", "comprehensive")
        target_type = target.metadata.get("target_type", "primary_focus")
        
        # è·å–ç”¨æˆ·æŸ¥è¯¢åŸæ–‡
        user_query = target.metadata.get("user_query", "")
        core_keywords = base_keywords[:4]  # ä½¿ç”¨å‰4ä¸ªå…³é”®è¯
        
        if target_type == "primary_focus":
            # ğŸ¯ ä¸»è¦ç›®æ ‡ï¼šæœ€æ·±å…¥ã€æœ€å…¨é¢çš„æœç´¢
            queries.extend(self._build_comprehensive_queries(core_keywords, strategy, user_query))
            
        elif target_type == "contextual_expansion":
            # ğŸ¯ ä¸Šä¸‹æ–‡ç›®æ ‡ï¼šç›¸å…³èƒŒæ™¯å’Œæ‰©å±•ä¿¡æ¯
            queries.extend(self._build_contextual_queries(core_keywords, strategy))
            
        elif target_type == "verification_focused":
            # ğŸ¯ éªŒè¯ç›®æ ‡ï¼šå¯è¡Œæ€§å’Œé£é™©è¯„ä¼°
            queries.extend(self._build_verification_queries(core_keywords, strategy))
        
        return queries
    
    def _build_autonomous_queries(self, target: ExplorationTarget, 
                                strategy: ExplorationStrategy, 
                                base_keywords: List[str]) -> List[str]:
        """ğŸ”„ æ„å»ºç³»ç»Ÿè‡ªä¸»çš„å®½æ³›ã€æ¢ç´¢æ€§æœç´¢æŸ¥è¯¢"""
        queries = []
        target_type = target.metadata.get("target_type", "general")
        core_keywords = base_keywords[:3]  # è‡ªä¸»æ¢ç´¢ä½¿ç”¨è¾ƒå°‘å…³é”®è¯
        
        if target_type == "knowledge_gap_filling":
            # ğŸ”„ çŸ¥è¯†ç¼ºå£ï¼šå¹¿æ³›æ”¶é›†åŸºç¡€ä¿¡æ¯
            queries.extend(self._build_gap_filling_queries(core_keywords, strategy))
            
        elif target_type == "serendipitous_discovery":
            # ğŸ”„ å¶ç„¶å‘ç°ï¼šæœ€å¤§åŒ–æœç´¢å¤šæ ·æ€§
            queries.extend(self._build_serendipity_queries(core_keywords, strategy))
            
        elif target_type == "trend_monitoring":
            # ğŸ”„ è¶‹åŠ¿ç›‘æ§ï¼šå…³æ³¨æœ€æ–°å‘å±•
            queries.extend(self._build_trend_queries(core_keywords, strategy))
        
        return queries
    
    def _build_comprehensive_queries(self, keywords: List[str], 
                                   strategy: ExplorationStrategy, 
                                   user_query: str) -> List[str]:
        """ğŸ¯ æ„å»ºå…¨é¢æ·±å…¥çš„æŸ¥è¯¢ - ç”¨æˆ·ä¸»è¦ç›®æ ‡"""
        queries = []
        
        # ç›´æ¥ä½¿ç”¨ç”¨æˆ·æŸ¥è¯¢
        if user_query:
            queries.append(user_query)
        
        # æ ¹æ®ç­–ç•¥ç”Ÿæˆç²¾ç¡®æŸ¥è¯¢
        if strategy == ExplorationStrategy.EXPERT_KNOWLEDGE:
            for keyword in keywords[:2]:
                queries.extend([
                    f"{keyword} ä¸“å®¶æ·±åº¦åˆ†æ",
                    f"{keyword} æƒå¨æŒ‡å—è¯¦è§£",
                    f"{keyword} ä¸“ä¸šå®è·µæ–¹æ¡ˆ",
                    f"{keyword} è¡Œä¸šæœ€ä½³å®è·µæ¡ˆä¾‹"
                ])
        
        elif strategy == ExplorationStrategy.TREND_MONITORING:
            for keyword in keywords[:2]:
                queries.extend([
                    f"{keyword} 2024æœ€æ–°å‘å±•è¶‹åŠ¿",
                    f"{keyword} æœªæ¥å‘å±•æ–¹å‘é¢„æµ‹",
                    f"{keyword} æŠ€æœ¯æ¼”è¿›è·¯çº¿å›¾",
                    f"{keyword} åˆ›æ–°çªç ´è¿›å±•"
                ])
        
        elif strategy == ExplorationStrategy.COMPETITIVE_INTELLIGENCE:
            for keyword in keywords[:2]:
                queries.extend([
                    f"{keyword} ç«äº‰å¯¹æ‰‹åˆ†æ",
                    f"{keyword} å¸‚åœºæ ¼å±€å¯¹æ¯”",
                    f"{keyword} ä¼˜åŠ¿åŠ£åŠ¿è¯„ä¼°",
                    f"{keyword} ç«äº‰ç­–ç•¥ç ”ç©¶"
                ])
        
        else:
            # é»˜è®¤æ·±åº¦æŸ¥è¯¢
            for keyword in keywords[:3]:
                queries.extend([
                    f"{keyword} è¯¦ç»†è¯´æ˜",
                    f"{keyword} æ·±å…¥åˆ†æ",
                    f"{keyword} å®Œæ•´æŒ‡å—"
                ])
        
        return queries
    
    def _build_contextual_queries(self, keywords: List[str], 
                                strategy: ExplorationStrategy) -> List[str]:
        """ğŸ¯ æ„å»ºä¸Šä¸‹æ–‡ç›¸å…³æŸ¥è¯¢ - ç”¨æˆ·æ‰©å±•ç›®æ ‡"""
        queries = []
        
        for keyword in keywords[:2]:
                queries.extend([
                f"{keyword} ç›¸å…³èƒŒæ™¯çŸ¥è¯†",
                f"{keyword} åº”ç”¨åœºæ™¯æ¡ˆä¾‹",
                f"{keyword} å®æ–½å‰ææ¡ä»¶",
                f"{keyword} ç›¸å…³æŠ€æœ¯æ ˆ"
            ])
        
        return queries
    
    def _build_verification_queries(self, keywords: List[str], 
                                  strategy: ExplorationStrategy) -> List[str]:
        """ğŸ¯ æ„å»ºéªŒè¯ç›¸å…³æŸ¥è¯¢ - ç”¨æˆ·éªŒè¯ç›®æ ‡"""
        queries = []
        
        for keyword in keywords[:2]:
            queries.extend([
                f"{keyword} å¯è¡Œæ€§è¯„ä¼°æŠ¥å‘Š",
                f"{keyword} æŠ€æœ¯é£é™©åˆ†æ",
                f"{keyword} å®æ–½æŒ‘æˆ˜ä¸å¯¹ç­–",
                f"{keyword} æˆåŠŸå¤±è´¥æ¡ˆä¾‹å¯¹æ¯”"
            ])
        
        return queries
    
    def _build_gap_filling_queries(self, keywords: List[str], 
                                 strategy: ExplorationStrategy) -> List[str]:
        """ğŸ”„ æ„å»ºçŸ¥è¯†ç¼ºå£æŸ¥è¯¢ - ç³»ç»Ÿè‡ªä¸»å­¦ä¹ """
        queries = []
        
        # æ›´å¹¿æ³›çš„æ¢ç´¢æ€§æŸ¥è¯¢
        for keyword in keywords[:2]:
            queries.extend([
                f"{keyword} åŸºç¡€æ¦‚å¿µ",
                f"{keyword} ç›¸å…³é¢†åŸŸ",
                f"{keyword} åº”ç”¨å®ä¾‹"
            ])
        
        return queries
    
    def _build_serendipity_queries(self, keywords: List[str], 
                                 strategy: ExplorationStrategy) -> List[str]:
        """ğŸ”„ æ„å»ºå¶ç„¶å‘ç°æŸ¥è¯¢ - ç³»ç»Ÿåˆ›æ–°æ¢ç´¢"""
        queries = []
        
        # æœ€å¤§åŒ–å¤šæ ·æ€§çš„æ¢ç´¢æ€§æŸ¥è¯¢
        for keyword in keywords[:2]:
            queries.extend([
                f"{keyword} æ„æƒ³ä¸åˆ°çš„åº”ç”¨",
                f"{keyword} åˆ›æ–°çªç ´",
                f"{keyword} è·¨ç•Œèåˆ",
                f"{keyword} æœªæ¥å¯èƒ½æ€§"
            ])
        
        return queries
    
    def _build_trend_queries(self, keywords: List[str], 
                           strategy: ExplorationStrategy) -> List[str]:
        """ğŸ”„ æ„å»ºè¶‹åŠ¿ç›‘æ§æŸ¥è¯¢ - ç³»ç»Ÿè¶‹åŠ¿è·Ÿè¸ª"""
        queries = []
        
        # å…³æ³¨æœ€æ–°å‘å±•çš„æŸ¥è¯¢
        for keyword in keywords[:2]:
            queries.extend([
                f"{keyword} æœ€æ–°åŠ¨æ€",
                f"{keyword} å‘å±•è¶‹åŠ¿",
                f"{keyword} æ–°å…´æ–¹å‘"
            ])
        
        return queries
    
    def _query_api_sources(self, 
                         target: ExplorationTarget, 
                         strategy: ExplorationStrategy) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢APIä¿¡æ¯æºï¼ˆé¢„ç•™æ¥å£ï¼‰"""
        # è¿™é‡Œå¯ä»¥é›†æˆå„ç§APIï¼š
        # - å­¦æœ¯è®ºæ–‡API (arXiv, PubMedç­‰)
        # - æ–°é—»API (News APIç­‰)
        # - æŠ€æœ¯æ–‡æ¡£API (GitHub, Stack Overflowç­‰)
        # - ä¸“ä¸šæ•°æ®åº“API
        
        return []  # å½“å‰ä¸ºç©ºå®ç°
    
    def _query_database_sources(self, 
                              target: ExplorationTarget, 
                              strategy: ExplorationStrategy) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æ•°æ®åº“ä¿¡æ¯æºï¼ˆé¢„ç•™æ¥å£ï¼‰"""
        # è¿™é‡Œå¯ä»¥é›†æˆï¼š
        # - å†…éƒ¨çŸ¥è¯†åº“
        # - è¡Œä¸šæ•°æ®åº“
        # - å†å²æ¢ç´¢ç¼“å­˜
        
        return []  # å½“å‰ä¸ºç©ºå®ç°
    
    def _extract_and_evaluate_knowledge(self, 
                                      raw_information: List[Dict[str, Any]], 
                                      targets: List[ExplorationTarget]) -> List[KnowledgeItem]:
        """çŸ¥è¯†æå–å’Œè´¨é‡è¯„ä¼°"""
        knowledge_items = []
        
        for info in raw_information:
            try:
                # æå–çŸ¥è¯†å†…å®¹
                knowledge_item = self._extract_knowledge_from_info(info, targets)
                if knowledge_item:
                    # è¯„ä¼°çŸ¥è¯†è´¨é‡
                    self._evaluate_knowledge_quality(knowledge_item)
                    
                    # è´¨é‡è¿‡æ»¤
                    if self._passes_quality_filter(knowledge_item):
                        knowledge_items.append(knowledge_item)
                        logger.debug(f"âœ… çŸ¥è¯†æå–æˆåŠŸ: {knowledge_item.knowledge_id}")
                    else:
                        logger.debug(f"âŒ çŸ¥è¯†è´¨é‡ä¸åˆæ ¼: {knowledge_item.knowledge_id}")
            
            except Exception as e:
                logger.debug(f"âš ï¸ çŸ¥è¯†æå–å¤±è´¥: {e}")
                continue
        
        logger.info(f"ğŸ” çŸ¥è¯†æå–å®Œæˆ: {len(knowledge_items)} é¡¹é«˜è´¨é‡çŸ¥è¯†")
        return knowledge_items
    
    def _extract_knowledge_from_info(self, 
                                   info: Dict[str, Any], 
                                   targets: List[ExplorationTarget]) -> Optional[KnowledgeItem]:
        """ä»åŸå§‹ä¿¡æ¯ä¸­æå–çŸ¥è¯†é¡¹"""
        content = info.get("content", "")
        if not content or len(content.strip()) < 10:
            return None
        
        # ç”ŸæˆçŸ¥è¯†ID
        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
        knowledge_id = f"knowledge_{content_hash}_{int(time.time())}"
        
        # åˆ›å»ºçŸ¥è¯†é¡¹
        knowledge_item = KnowledgeItem(
            knowledge_id=knowledge_id,
            content=content,
            source=info.get("url", info.get("source", "unknown")),
            source_type=info.get("source", "unknown"),
            quality=KnowledgeQuality.FAIR,  # åˆå§‹è´¨é‡ç­‰çº§
            extraction_method="automatic_extraction",
            tags=self._extract_tags_from_content(content),
            related_concepts=self._extract_concepts_from_content(content, targets)
        )
        
        return knowledge_item
    
    def _evaluate_knowledge_quality(self, knowledge_item: KnowledgeItem):
        """è¯„ä¼°çŸ¥è¯†è´¨é‡ - å¤šç»´åº¦è¯„ä¼°ä½“ç³»"""
        
        # 1. ç½®ä¿¡åº¦è¯„ä¼°ï¼ˆåŸºäºæ¥æºå¯ä¿¡åº¦ï¼‰
        confidence_score = self._assess_source_credibility(knowledge_item.source_type)
        
        # 2. ç›¸å…³æ€§è¯„ä¼°ï¼ˆåŸºäºå†…å®¹ç›¸å…³åº¦ï¼‰
        relevance_score = self._assess_content_relevance(knowledge_item.content)
        
        # 3. æ–°é¢–æ€§è¯„ä¼°ï¼ˆåŸºäºå†…å®¹ç‹¬ç‰¹æ€§ï¼‰
        novelty_score = self._assess_content_novelty(knowledge_item.content)
        
        # 4. ç»¼åˆè´¨é‡è¯„ä¼°
        overall_score = (confidence_score * 0.4 + 
                        relevance_score * 0.4 + 
                        novelty_score * 0.2)
        
        # æ›´æ–°çŸ¥è¯†é¡¹è¯„åˆ†
        knowledge_item.confidence_score = confidence_score
        knowledge_item.relevance_score = relevance_score
        knowledge_item.novelty_score = novelty_score
        
        # ç¡®å®šè´¨é‡ç­‰çº§
        if overall_score >= 0.8:
            knowledge_item.quality = KnowledgeQuality.EXCELLENT
        elif overall_score >= 0.6:
            knowledge_item.quality = KnowledgeQuality.GOOD
        elif overall_score >= 0.4:
            knowledge_item.quality = KnowledgeQuality.FAIR
        elif overall_score >= 0.2:
            knowledge_item.quality = KnowledgeQuality.POOR
        else:
            knowledge_item.quality = KnowledgeQuality.UNRELIABLE
        
        logger.debug(f"ğŸ“Š è´¨é‡è¯„ä¼°å®Œæˆ: {knowledge_item.knowledge_id} - {knowledge_item.quality.value}")
    
    def _assess_source_credibility(self, source_type: str) -> float:
        """è¯„ä¼°ä¿¡æ¯æºå¯ä¿¡åº¦"""
        credibility_scores = {
            "web_search": 0.6,
            "academic_paper": 0.9,
            "expert_system": 0.8,
            "database": 0.7,
            "api_call": 0.6,
            "unknown": 0.3
        }
        return credibility_scores.get(source_type, 0.3)
    
    def _assess_content_relevance(self, content: str) -> float:
        """è¯„ä¼°å†…å®¹ç›¸å…³æ€§ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # åŸºäºå†…å®¹é•¿åº¦å’Œå…³é”®è¯å¯†åº¦çš„ç®€å•è¯„ä¼°
        content_length = len(content)
        if content_length < 50:
            return 0.3
        elif content_length < 200:
            return 0.5
        elif content_length < 500:
            return 0.7
        else:
            return 0.8
    
    def _assess_content_novelty(self, content: str) -> float:
        """è¯„ä¼°å†…å®¹æ–°é¢–æ€§"""
        # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰çŸ¥è¯†é‡å¤
        for cached_item in list(self.knowledge_cache.values())[-10:]:  # æ£€æŸ¥æœ€è¿‘10é¡¹
            if self._calculate_content_similarity(content, cached_item.content) > 0.8:
                return 0.2  # é«˜åº¦ç›¸ä¼¼ï¼Œæ–°é¢–æ€§ä½
        
        return 0.6  # é»˜è®¤ä¸­ç­‰æ–°é¢–æ€§
    
    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        """è®¡ç®—å†…å®¹ç›¸ä¼¼æ€§ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _passes_quality_filter(self, knowledge_item: KnowledgeItem) -> bool:
        """çŸ¥è¯†è´¨é‡è¿‡æ»¤å™¨"""
        min_confidence = self.config["quality_control"]["min_confidence_threshold"]
        min_relevance = self.config["quality_control"]["min_relevance_threshold"]
        
        return (knowledge_item.confidence_score >= min_confidence and
                knowledge_item.relevance_score >= min_relevance and
                knowledge_item.quality != KnowledgeQuality.UNRELIABLE)
    
    def _generate_thinking_seeds(self, 
                               knowledge_items: List[KnowledgeItem], 
                               strategy: ExplorationStrategy) -> List[ThinkingSeed]:
        """åŸºäºå‘ç°çš„çŸ¥è¯†ç”Ÿæˆæ€ç»´ç§å­"""
        thinking_seeds = []
        max_seeds = self.config["seed_generation"]["max_seeds_per_exploration"]
        
        # æŒ‰è´¨é‡æ’åºçŸ¥è¯†é¡¹
        sorted_knowledge = sorted(knowledge_items, 
                                key=lambda k: (k.confidence_score + k.relevance_score + k.novelty_score) / 3, 
                                reverse=True)
        
        # ä¸ºé«˜è´¨é‡çŸ¥è¯†ç”Ÿæˆç§å­
        for knowledge_item in sorted_knowledge[:max_seeds]:
            seed = self._create_thinking_seed_from_knowledge(knowledge_item, strategy)
            if seed:
                thinking_seeds.append(seed)
        
        # ç”Ÿæˆè·¨çŸ¥è¯†èåˆç§å­
        if len(knowledge_items) >= 2:
            fusion_seeds = self._create_fusion_thinking_seeds(knowledge_items[:3], strategy)
            thinking_seeds.extend(fusion_seeds)
        
        # é™åˆ¶ç§å­æ•°é‡
        thinking_seeds = thinking_seeds[:max_seeds]
        
        logger.info(f"ğŸŒ± æ€ç»´ç§å­ç”Ÿæˆå®Œæˆ: {len(thinking_seeds)} ä¸ª")
        return thinking_seeds
    
    def _create_thinking_seed_from_knowledge(self, 
                                           knowledge_item: KnowledgeItem, 
                                           strategy: ExplorationStrategy) -> Optional[ThinkingSeed]:
        """ä»å•ä¸ªçŸ¥è¯†é¡¹åˆ›å»ºæ€ç»´ç§å­"""
        
        # åŸºäºç­–ç•¥è°ƒæ•´ç§å­å†…å®¹
        if strategy == ExplorationStrategy.TREND_MONITORING:
            seed_content = f"åŸºäºè¶‹åŠ¿ç›‘æ§å‘ç°ï¼š{knowledge_item.content[:100]}..."
            creativity_level = "medium"
        
        elif strategy == ExplorationStrategy.CROSS_DOMAIN_LEARNING:
            seed_content = f"è·¨åŸŸå­¦ä¹ æ´å¯Ÿï¼š{knowledge_item.content[:100]}..."
            creativity_level = "high"
        
        elif strategy == ExplorationStrategy.GAP_ANALYSIS:
            seed_content = f"ç¼ºå£åˆ†æå‘ç°ï¼š{knowledge_item.content[:100]}..."
            creativity_level = "medium"
        
        elif strategy == ExplorationStrategy.EXPERT_KNOWLEDGE:
            seed_content = f"ä¸“å®¶çŸ¥è¯†æ´å¯Ÿï¼š{knowledge_item.content[:100]}..."
            creativity_level = "high"
        
        else:
            seed_content = f"æ¢ç´¢å‘ç°ï¼š{knowledge_item.content[:100]}..."
            creativity_level = "medium"
        
        # ç”Ÿæˆç§å­ID
        seed_id = f"seed_{knowledge_item.knowledge_id}_{int(time.time())}"
        
        # åˆ›å»ºæ€ç»´ç§å­
        thinking_seed = ThinkingSeed(
            seed_id=seed_id,
            seed_content=seed_content,
            source_knowledge=[knowledge_item.knowledge_id],
            creativity_level=creativity_level,
            potential_applications=self._suggest_applications(knowledge_item),
            generated_strategy=strategy.value,
            confidence=min(knowledge_item.confidence_score * 1.1, 1.0),  # è½»å¾®æå‡
            suggested_reasoning_paths=self._suggest_reasoning_paths(knowledge_item, strategy),
            generation_context={
                "strategy": strategy.value,
                "source_quality": knowledge_item.quality.value,
                "generated_from": "single_knowledge_item"
            }
        )
        
        return thinking_seed
    
    def _create_fusion_thinking_seeds(self, 
                                    knowledge_items: List[KnowledgeItem], 
                                    strategy: ExplorationStrategy) -> List[ThinkingSeed]:
        """åˆ›å»ºèåˆæ€ç»´ç§å­ - æ•´åˆå¤šä¸ªçŸ¥è¯†é¡¹çš„åˆ›æ–°ç§å­"""
        fusion_seeds = []
        
        if len(knowledge_items) < 2:
            return fusion_seeds
        
        # åˆ›å»ºçŸ¥è¯†èåˆç§å­
        fusion_content = self._fuse_knowledge_contents(knowledge_items)
        fusion_id = f"fusion_seed_{int(time.time())}"
        
        fusion_seed = ThinkingSeed(
            seed_id=fusion_id,
            seed_content=f"èåˆæ´å¯Ÿï¼š{fusion_content}",
            source_knowledge=[k.knowledge_id for k in knowledge_items],
            creativity_level="high",  # èåˆç§å­é€šå¸¸æ›´æœ‰åˆ›æ„
            potential_applications=self._suggest_fusion_applications(knowledge_items),
            generated_strategy=strategy.value,
            confidence=sum(k.confidence_score for k in knowledge_items) / len(knowledge_items),
            cross_domain_connections=self._identify_cross_domain_connections(knowledge_items),
            generation_context={
                "strategy": strategy.value,
                "generated_from": "knowledge_fusion",
                "fusion_count": len(knowledge_items)
            }
        )
        
        fusion_seeds.append(fusion_seed)
        return fusion_seeds
    
    def _fuse_knowledge_contents(self, knowledge_items: List[KnowledgeItem]) -> str:
        """èåˆå¤šä¸ªçŸ¥è¯†é¡¹çš„å†…å®¹"""
        contents = [k.content[:50] for k in knowledge_items[:3]]  # å–å‰50å­—ç¬¦
        return "ã€".join(contents) + " çš„ç»¼åˆåˆ›æ–°æ€è·¯"
    
    def _suggest_applications(self, knowledge_item: KnowledgeItem) -> List[str]:
        """å»ºè®®çŸ¥è¯†åº”ç”¨é¢†åŸŸ"""
        base_applications = [
            "é—®é¢˜è§£å†³ç­–ç•¥",
            "åˆ›æ–°æ€ç»´è·¯å¾„",
            "å†³ç­–ä¼˜åŒ–æ–¹æ¡ˆ"
        ]
        
        # åŸºäºçŸ¥è¯†æ ‡ç­¾å¢åŠ ç‰¹å®šåº”ç”¨
        specific_applications = []
        for tag in knowledge_item.tags[:2]:
            specific_applications.append(f"{tag}ç›¸å…³åº”ç”¨")
        
        return base_applications + specific_applications
    
    def _suggest_reasoning_paths(self, 
                               knowledge_item: KnowledgeItem, 
                               strategy: ExplorationStrategy) -> List[str]:
        """å»ºè®®æ¨ç†è·¯å¾„"""
        base_paths = ["analytical_reasoning", "creative_synthesis"]
        
        strategy_specific_paths = {
            ExplorationStrategy.TREND_MONITORING: ["trend_analysis_path", "predictive_reasoning"],
            ExplorationStrategy.CROSS_DOMAIN_LEARNING: ["analogical_reasoning", "cross_domain_transfer"],
            ExplorationStrategy.GAP_ANALYSIS: ["problem_solving_path", "systematic_analysis"],
            ExplorationStrategy.DOMAIN_EXPANSION: ["exploratory_reasoning", "domain_bridging"],
            ExplorationStrategy.EXPERT_KNOWLEDGE: ["expert_reasoning_path", "professional_methodology"]
        }
        
        specific_paths = strategy_specific_paths.get(strategy, [])
        return base_paths + specific_paths
    
    def _suggest_fusion_applications(self, knowledge_items: List[KnowledgeItem]) -> List[str]:
        """å»ºè®®èåˆçŸ¥è¯†çš„åº”ç”¨"""
        return [
            "è·¨é¢†åŸŸåˆ›æ–°è§£å†³æ–¹æ¡ˆ",
            "ç»¼åˆå†³ç­–ä¼˜åŒ–ç­–ç•¥",
            "å¤šç»´åº¦é—®é¢˜åˆ†ææ–¹æ³•",
            "ç³»ç»Ÿæ€§æ€ç»´å‡çº§è·¯å¾„"
        ]
    
    def _identify_cross_domain_connections(self, knowledge_items: List[KnowledgeItem]) -> List[str]:
        """è¯†åˆ«è·¨åŸŸè¿æ¥"""
        connections = []
        domains = set()
        
        # ä»æ ‡ç­¾ä¸­æå–é¢†åŸŸä¿¡æ¯
        for item in knowledge_items:
            domains.update(item.tags[:2])  # å–å‰2ä¸ªæ ‡ç­¾ä½œä¸ºé¢†åŸŸ
        
        # ç”Ÿæˆè·¨åŸŸè¿æ¥æè¿°
        domain_list = list(domains)
        for i in range(len(domain_list)):
            for j in range(i+1, len(domain_list)):
                connections.append(f"{domain_list[i]}ä¸{domain_list[j]}çš„èåˆåˆ›æ–°")
        
        return connections[:3]  # é™åˆ¶è¿æ¥æ•°é‡
    
    def _analyze_trends(self, 
                       knowledge_items: List[KnowledgeItem], 
                       targets: List[ExplorationTarget]) -> List[Dict[str, Any]]:
        """åˆ†æè¯†åˆ«çš„è¶‹åŠ¿"""
        trends = []
        
        # åŸºäºçŸ¥è¯†å†…å®¹è¯†åˆ«è¶‹åŠ¿å…³é”®è¯
        trend_keywords = self._extract_trend_keywords(knowledge_items)
        
        for keyword in trend_keywords[:3]:  # é™åˆ¶è¶‹åŠ¿æ•°é‡
            trend = {
                "trend_id": f"trend_{keyword}_{int(time.time())}",
                "trend_name": f"{keyword}ç›¸å…³è¶‹åŠ¿",
                "confidence": 0.6,
                "supporting_knowledge": [k.knowledge_id for k in knowledge_items 
                                       if keyword.lower() in k.content.lower()],
                "time_horizon": "short_term",
                "impact_prediction": f"{keyword}å°†åœ¨ç›¸å…³é¢†åŸŸäº§ç”Ÿé‡è¦å½±å“",
                "identified_at": time.time()
            }
            trends.append(trend)
        
        logger.info(f"ğŸ“ˆ è¶‹åŠ¿åˆ†æå®Œæˆ: è¯†åˆ« {len(trends)} ä¸ªè¶‹åŠ¿")
        return trends
    
    def _extract_trend_keywords(self, knowledge_items: List[KnowledgeItem]) -> List[str]:
        """æå–è¶‹åŠ¿å…³é”®è¯"""
        keyword_frequency = defaultdict(int)
        
        for item in knowledge_items:
            words = item.content.lower().split()
            for word in words:
                if len(word) > 3:  # è¿‡æ»¤çŸ­è¯
                    keyword_frequency[word] += 1
        
        # è¿”å›é¢‘æ¬¡æœ€é«˜çš„å…³é”®è¯
        sorted_keywords = sorted(keyword_frequency.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_keywords[:5] if freq > 1]
    
    def _discover_cross_domain_insights(self, 
                                      knowledge_items: List[KnowledgeItem], 
                                      thinking_seeds: List[ThinkingSeed]) -> List[Dict[str, Any]]:
        """å‘ç°è·¨åŸŸæ´å¯Ÿ"""
        insights = []
        
        # åŸºäºæ€ç»´ç§å­çš„è·¨åŸŸè¿æ¥
        for seed in thinking_seeds:
            if seed.cross_domain_connections:
                insight = {
                    "insight_id": f"cross_domain_{seed.seed_id}",
                    "insight_type": "cross_domain_connection",
                    "description": f"å‘ç°{seed.cross_domain_connections[0]}çš„åˆ›æ–°æœºä¼š",
                    "supporting_seeds": [seed.seed_id],
                    "potential_impact": "high",
                    "confidence": seed.confidence,
                    "discovered_at": time.time()
                }
                insights.append(insight)
        
        logger.info(f"ğŸ”— è·¨åŸŸæ´å¯Ÿå‘ç°å®Œæˆ: {len(insights)} ä¸ªæ´å¯Ÿ")
        return insights
    
    
    # ==================== ç­–ç•¥é€‰æ‹©ä¸ä¼˜åŒ– ====================
    
    def _select_optimal_strategy(self, targets: List[ExplorationTarget]) -> ExplorationStrategy:
        """åŸºäºç›®æ ‡å’Œå†å²è¡¨ç°é€‰æ‹©æœ€ä¼˜æ¢ç´¢ç­–ç•¥"""
        
        # å¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
        if not self.strategy_performance:
            return self.config["exploration_strategies"]["default_strategy"]
        
        # åŸºäºå†å²è¡¨ç°é€‰æ‹©ç­–ç•¥
        best_strategy = None
        best_score = 0.0
        
        for strategy_name, performance in self.strategy_performance.items():
            # ç»¼åˆè¯„åˆ†ï¼šæˆåŠŸç‡ * 0.6 + å¹³å‡è´¨é‡ * 0.4
            score = (performance["success_rate"] * 0.6 + 
                    performance["avg_quality"] * 0.4)
            
            if score > best_score:
                best_score = score
                try:
                    best_strategy = ExplorationStrategy(strategy_name)
                except ValueError:
                    continue
        
        return best_strategy or self.config["exploration_strategies"]["default_strategy"]
    
    def _select_user_directed_strategy(self, targets: List[ExplorationTarget], 
                                     user_context: Dict[str, Any]) -> ExplorationStrategy:
        """ğŸ¯ ä¸ºç”¨æˆ·æŒ‡ä»¤é©±åŠ¨æ¨¡å¼é€‰æ‹©æœ€ä¼˜ç­–ç•¥ - ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ†æ"""
        user_query = user_context.get("user_query", "")
        
        logger.debug(f"ğŸ¯ ä¸ºç”¨æˆ·æŸ¥è¯¢é€‰æ‹©æ¢ç´¢ç­–ç•¥: {user_query[:50]}...")
        
        # ğŸ§  ä¼˜å…ˆå°è¯•ä½¿ç”¨è¯­ä¹‰åˆ†æå™¨
        if self.semantic_analyzer and user_query:
            try:
                semantic_strategy = self._get_strategy_from_semantic_analysis(user_query, user_context)
                if semantic_strategy:
                    logger.info(f"âœ… è¯­ä¹‰åˆ†æé€‰æ‹©ç­–ç•¥: {semantic_strategy.value}")
                    return semantic_strategy
            except Exception as e:
                logger.warning(f"âš ï¸ è¯­ä¹‰åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {e}")
        
        # ğŸš¨ è¯­ä¹‰åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
        logger.warning("âš ï¸ è¯­ä¹‰åˆ†æä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¸“å®¶çŸ¥è¯†ç­–ç•¥")
        return ExplorationStrategy.EXPERT_KNOWLEDGE
    
    def _get_strategy_from_semantic_analysis(self, user_query: str, user_context: Dict[str, Any]) -> Optional[ExplorationStrategy]:
        """ğŸ§  åŸºäºè¯­ä¹‰åˆ†æé€‰æ‹©æ¢ç´¢ç­–ç•¥"""
        if not self.semantic_analyzer:
            return None
            
        try:
            # æ‰§è¡Œå¤šç»´åº¦è¯­ä¹‰åˆ†æ
            analysis_tasks = ['intent_detection', 'domain_classification']
            response = self.semantic_analyzer.analyze(user_query, analysis_tasks)
            
            if not response.overall_success:
                logger.warning("âš ï¸ è¯­ä¹‰åˆ†ææœªæˆåŠŸå®Œæˆ")
                return None
            
            # æå–åˆ†æç»“æœ
            intent_result = response.analysis_results.get('intent_detection')
            domain_result = response.analysis_results.get('domain_classification')
            
            intent = None
            domain = None
            confidence_threshold = 0.7
            
            if intent_result and intent_result.success and intent_result.confidence > confidence_threshold:
                intent = intent_result.result.get('primary_intent')
                
            if domain_result and domain_result.success and domain_result.confidence > confidence_threshold:
                domain = domain_result.result.get('domain')
            
            logger.debug(f"ğŸ§  è¯­ä¹‰åˆ†æç»“æœ: intent={intent}, domain={domain}")
            
            # åŸºäºè¯­ä¹‰åˆ†æç»“æœæ˜ å°„ç­–ç•¥
            strategy = self._map_semantic_analysis_to_strategy(intent, domain, user_query, user_context)
            
            return strategy
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰åˆ†æè¿‡ç¨‹å¼‚å¸¸: {e}")
            return None
    
    def _map_semantic_analysis_to_strategy(self, intent: Optional[str], domain: Optional[str], 
                                         user_query: str, user_context: Dict[str, Any]) -> Optional[ExplorationStrategy]:
        """ğŸ¯ å°†è¯­ä¹‰åˆ†æç»“æœæ˜ å°„åˆ°æ¢ç´¢ç­–ç•¥"""
        
        # åŸºäºæ„å›¾çš„ç­–ç•¥æ˜ å°„
        intent_strategy_mapping = {
            # è§£å†³æ–¹æ¡ˆå¯»æ±‚ - ä¸“å®¶çŸ¥è¯†ä¼˜å…ˆ
            'solution_seeking': ExplorationStrategy.EXPERT_KNOWLEDGE,
            'problem_solving': ExplorationStrategy.EXPERT_KNOWLEDGE,
            'how_to_query': ExplorationStrategy.EXPERT_KNOWLEDGE,
            
            # æ¯”è¾ƒåˆ†æ - ç«äº‰æƒ…æŠ¥
            'comparison_analysis': ExplorationStrategy.COMPETITIVE_INTELLIGENCE,
            'evaluation_request': ExplorationStrategy.COMPETITIVE_INTELLIGENCE,
            'selection_help': ExplorationStrategy.COMPETITIVE_INTELLIGENCE,
            
            # è¶‹åŠ¿å’Œæœ€æ–°ä¿¡æ¯
            'trend_monitoring': ExplorationStrategy.TREND_MONITORING,
            'latest_information': ExplorationStrategy.TREND_MONITORING,
            'news_seeking': ExplorationStrategy.TREND_MONITORING,
            
            # å­¦ä¹ å’Œäº†è§£
            'learning_request': ExplorationStrategy.DOMAIN_EXPANSION,
            'knowledge_acquisition': ExplorationStrategy.DOMAIN_EXPANSION,
            'understanding_seeking': ExplorationStrategy.DOMAIN_EXPANSION,
            
            # è·¨é¢†åŸŸæ•´åˆ
            'integration_query': ExplorationStrategy.CROSS_DOMAIN_LEARNING,
            'synthesis_request': ExplorationStrategy.CROSS_DOMAIN_LEARNING,
            
            # é—®é¢˜è¯Šæ–­
            'problem_diagnosis': ExplorationStrategy.GAP_ANALYSIS,
            'issue_identification': ExplorationStrategy.GAP_ANALYSIS,
        }
        
        # ä¼˜å…ˆåŸºäºæ„å›¾é€‰æ‹©ç­–ç•¥
        if intent and intent in intent_strategy_mapping:
            strategy = intent_strategy_mapping[intent]
            logger.debug(f"ğŸ¯ åŸºäºæ„å›¾ '{intent}' é€‰æ‹©ç­–ç•¥: {strategy.value}")
            return strategy
        
        # åŸºäºé¢†åŸŸçš„ç­–ç•¥æ˜ å°„ï¼ˆä½œä¸ºè¡¥å……ï¼‰
        domain_strategy_mapping = {
            # æŠ€æœ¯é¢†åŸŸ - ä¸“å®¶çŸ¥è¯†å’Œè¶‹åŠ¿ç›‘æ§
            'æŠ€æœ¯': ExplorationStrategy.EXPERT_KNOWLEDGE,
            'technology': ExplorationStrategy.EXPERT_KNOWLEDGE,
            'artificial_intelligence': ExplorationStrategy.TREND_MONITORING,
            
            # å•†ä¸šé¢†åŸŸ - ç«äº‰æƒ…æŠ¥å’Œè¶‹åŠ¿ç›‘æ§  
            'å•†ä¸š': ExplorationStrategy.COMPETITIVE_INTELLIGENCE,
            'business': ExplorationStrategy.COMPETITIVE_INTELLIGENCE,
            'marketing': ExplorationStrategy.COMPETITIVE_INTELLIGENCE,
            
            # å­¦æœ¯ç ”ç©¶ - é¢†åŸŸæ‰©å±•å’Œè·¨åŸŸå­¦ä¹ 
            'å­¦æœ¯': ExplorationStrategy.DOMAIN_EXPANSION,
            'academic': ExplorationStrategy.DOMAIN_EXPANSION,
            'research': ExplorationStrategy.CROSS_DOMAIN_LEARNING,
            
            # åˆ›æ–°å’Œåˆ›æ„ - è·¨åŸŸå­¦ä¹ å’Œå¶ç„¶å‘ç°
            'åˆ›æ–°': ExplorationStrategy.CROSS_DOMAIN_LEARNING,
            'innovation': ExplorationStrategy.SERENDIPITY_DISCOVERY,
        }
        
        if domain and domain in domain_strategy_mapping:
            strategy = domain_strategy_mapping[domain]
            logger.debug(f"ğŸ¯ åŸºäºé¢†åŸŸ '{domain}' é€‰æ‹©ç­–ç•¥: {strategy.value}")
            return strategy
        
        # åŸºäºæŸ¥è¯¢é•¿åº¦å’Œå¤æ‚åº¦çš„å¯å‘å¼ç­–ç•¥
        query_length = len(user_query.split())
        if query_length > 10:  # å¤æ‚æŸ¥è¯¢ï¼Œå¯èƒ½éœ€è¦ä¸“å®¶çŸ¥è¯†
            logger.debug("ğŸ¯ å¤æ‚æŸ¥è¯¢ï¼Œé€‰æ‹©ä¸“å®¶çŸ¥è¯†ç­–ç•¥")
            return ExplorationStrategy.EXPERT_KNOWLEDGE
        elif query_length < 5:  # ç®€å•æŸ¥è¯¢ï¼Œé¢†åŸŸæ‰©å±•
            logger.debug("ğŸ¯ ç®€å•æŸ¥è¯¢ï¼Œé€‰æ‹©é¢†åŸŸæ‰©å±•ç­–ç•¥")
            return ExplorationStrategy.DOMAIN_EXPANSION
        
        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œè¿”å›é»˜è®¤ç­–ç•¥
        logger.debug("ğŸ¯ è¯­ä¹‰åˆ†ææœªåŒ¹é…åˆ°æ˜ç¡®ç­–ç•¥ï¼Œä½¿ç”¨é»˜è®¤ä¸“å®¶çŸ¥è¯†ç­–ç•¥")
        return ExplorationStrategy.EXPERT_KNOWLEDGE
    
    
    def create_exploration_targets_from_context(self, context: Dict[str, Any]) -> List[ExplorationTarget]:
        """ğŸ¯ ä»è®¤çŸ¥è°ƒåº¦å™¨ä¸Šä¸‹æ–‡åˆ›å»ºæ¢ç´¢ç›®æ ‡ - åŒè½¨æ¢ç´¢å¢å¼ºç‰ˆ"""
        targets = []
        
        # æ£€æµ‹æ˜¯å¦ä¸ºç”¨æˆ·æŒ‡ä»¤é©±åŠ¨æ¨¡å¼
        is_user_directed = context.get("exploration_mode") == "user_directed"
        
        if is_user_directed:
            # ğŸ¯ ç”¨æˆ·æŒ‡ä»¤é©±åŠ¨æ¨¡å¼ - ç²¾ç¡®èšç„¦ï¼Œé«˜è´¨é‡æ¢ç´¢
            targets.extend(self._create_user_directed_targets(context))
        else:
            # ğŸ”„ ç³»ç»Ÿè‡ªä¸»æ¢ç´¢æ¨¡å¼ - å®½æ³›æ¢ç´¢ï¼Œå¼¥è¡¥çŸ¥è¯†ç¼ºå£
            targets.extend(self._create_autonomous_targets(context))
        
        logger.info(f"ğŸ¯ å·²åˆ›å»º {len(targets)} ä¸ªæ¢ç´¢ç›®æ ‡ ({'ç”¨æˆ·æŒ‡ä»¤é©±åŠ¨' if is_user_directed else 'ç³»ç»Ÿè‡ªä¸»'})")
        
        # æ˜¾ç¤ºç›®æ ‡è¯¦æƒ…
        for target in targets:
            priority_str = "ğŸ”´é«˜ä¼˜å…ˆçº§" if target.priority == ExplorationPriority.HIGH else "ğŸŸ¡ä¸­ä¼˜å…ˆçº§"
            logger.debug(f"   {priority_str}: {target.description[:60]}...")
        
        return targets
    
    def _create_user_directed_targets(self, context: Dict[str, Any]) -> List[ExplorationTarget]:
        """ğŸ¯ åˆ›å»ºç”¨æˆ·æŒ‡ä»¤é©±åŠ¨çš„é«˜ä¼˜å…ˆçº§ã€ç²¾ç¡®èšç„¦çš„æ¢ç´¢ç›®æ ‡"""
        targets = []
        user_query = context.get("user_query", "")
        user_context = context.get("user_context", {})
        
        if not user_query:
            logger.warning("âš ï¸ ç”¨æˆ·æŒ‡ä»¤é©±åŠ¨æ¨¡å¼ä½†ç¼ºå°‘user_query")
            return targets
        
        # ğŸ¯ ä¸»è¦ç›®æ ‡ï¼šç²¾ç¡®èšç„¦ç”¨æˆ·æŸ¥è¯¢æ ¸å¿ƒ
        core_keywords = self._extract_core_keywords_advanced(user_query)
        query_intent = self._analyze_user_query_intent(user_query)
        
        primary_target = ExplorationTarget(
            target_id=f"user_primary_{int(time.time())}",
            description=f"ç”¨æˆ·æ ¸å¿ƒæŸ¥è¯¢æ·±åº¦æ¢ç´¢: {user_query}",
            keywords=core_keywords,
            priority=ExplorationPriority.HIGH,
            domain=self._identify_query_domain(user_query),
            expected_quality=KnowledgeQuality.HIGH,
            metadata={
                "user_query": user_query,
                "exploration_mode": "user_directed",
                "target_type": "primary_focus",
                "query_intent": query_intent,
                "search_depth": "comprehensive",
                "search_diversity": "high",
                "immediate_priority": True,
                "urgency_level": user_context.get("urgency_level", "high"),
                "expected_depth": user_context.get("expected_depth", "detailed")
            }
        )
        targets.append(primary_target)
        
        # ğŸ¯ æ‰©å±•ç›®æ ‡ï¼šç›¸å…³ä¸Šä¸‹æ–‡æ¢ç´¢
        if query_intent.get("requires_context", False):
            context_target = ExplorationTarget(
                target_id=f"user_context_{int(time.time())}",
                description=f"ç”¨æˆ·æŸ¥è¯¢ç›¸å…³ä¸Šä¸‹æ–‡æ¢ç´¢: {query_intent.get('context_topics', [])}",
                keywords=self._generate_contextual_keywords(user_query, core_keywords),
                priority=ExplorationPriority.MEDIUM,
                domain=self._identify_query_domain(user_query),
                expected_quality=KnowledgeQuality.HIGH,
                metadata={
                    "user_query": user_query,
                    "exploration_mode": "user_directed",
                    "target_type": "contextual_expansion",
                    "search_depth": "moderate",
                    "search_diversity": "medium",
                    "parent_target": primary_target.target_id
                }
            )
            targets.append(context_target)
        
        # ğŸ¯ éªŒè¯ç›®æ ‡ï¼šå¦‚æœæŸ¥è¯¢æ¶‰åŠå¯è¡Œæ€§æˆ–å†³ç­–
        if query_intent.get("requires_verification", False):
            verification_target = ExplorationTarget(
                target_id=f"user_verify_{int(time.time())}",
                description=f"ç”¨æˆ·æŸ¥è¯¢å¯è¡Œæ€§éªŒè¯æ¢ç´¢: {user_query}",
                keywords=self._generate_verification_keywords(user_query, core_keywords),
                priority=ExplorationPriority.MEDIUM,
                domain=self._identify_query_domain(user_query),
                expected_quality=KnowledgeQuality.HIGH,
                metadata={
                    "user_query": user_query,
                    "exploration_mode": "user_directed",
                    "target_type": "verification_focused",
                    "search_depth": "targeted",
                    "search_diversity": "focused",
                    "verification_aspects": ["feasibility", "risks", "alternatives"]
                }
            )
            targets.append(verification_target)
        
        logger.debug(f"ğŸ¯ ç”¨æˆ·æŒ‡ä»¤ç›®æ ‡åˆ›å»ºå®Œæˆ: {len(targets)} ä¸ªç²¾ç¡®èšç„¦ç›®æ ‡")
        return targets
    
    def _create_autonomous_targets(self, context: Dict[str, Any]) -> List[ExplorationTarget]:
        """ğŸ”„ åˆ›å»ºç³»ç»Ÿè‡ªä¸»çš„ä½ä¼˜å…ˆçº§ã€å®½æ³›æ¢ç´¢ç›®æ ‡"""
        targets = []
        exploration_opportunities = context.get("exploration_opportunities", [])
        knowledge_gaps = context.get("current_knowledge_gaps", [])
        
        # ğŸ”„ çŸ¥è¯†ç¼ºå£ç›®æ ‡ï¼šå¼¥è¡¥ç³»ç»Ÿè®¤çŸ¥ç›²åŒº
        for i, gap in enumerate(knowledge_gaps[:2]):  # é™åˆ¶æ•°é‡ï¼Œé¿å…èµ„æºè¿‡è½½
            gap_target = ExplorationTarget(
                target_id=f"autonomous_gap_{int(time.time())}_{i}",
                description=f"çŸ¥è¯†ç¼ºå£å¼¥è¡¥æ¢ç´¢: {gap.get('gap_description', 'æœªçŸ¥é¢†åŸŸ')}",
                keywords=gap.get('related_keywords', [])[:8],
                priority=ExplorationPriority.LOW,  # è‡ªä¸»æ¢ç´¢ä½¿ç”¨ä½ä¼˜å…ˆçº§
                domain=gap.get('domain', 'é€šç”¨'),
                expected_quality=KnowledgeQuality.MEDIUM,
                metadata={
                    "exploration_mode": "autonomous",
                    "target_type": "knowledge_gap_filling",
                    "search_depth": "broad",
                    "search_diversity": "high",
                    "gap_type": gap.get('gap_type', 'unknown'),
                    "discovery_focus": True
                }
            )
            targets.append(gap_target)
        
        # ğŸ”„ æœºä¼šæ¢ç´¢ç›®æ ‡ï¼šå‘ç°æ„æƒ³ä¸åˆ°çš„çŸ¥è¯†
        for i, opportunity in enumerate(exploration_opportunities[:2]):
            serendipity_target = ExplorationTarget(
                target_id=f"autonomous_serendipity_{int(time.time())}_{i}",
                description=f"å¶ç„¶å‘ç°æ¢ç´¢: {opportunity.get('description', 'æ¢ç´¢æœªçŸ¥')}",
                keywords=opportunity.get('keywords', [])[:6],
                priority=ExplorationPriority.LOW,
                domain=opportunity.get('domain', 'é€šç”¨'),
                expected_quality=KnowledgeQuality.MEDIUM,
                metadata={
                    "exploration_mode": "autonomous",
                    "target_type": "serendipitous_discovery",
                    "search_depth": "exploratory",
                    "search_diversity": "maximum",
                    "opportunity_type": opportunity.get('type', 'general'),
                    "creativity_boost": True
                }
            )
            targets.append(serendipity_target)
        
        # ğŸ”„ è¶‹åŠ¿ç›‘æ§ç›®æ ‡ï¼šè·Ÿè¸ªé¢†åŸŸå‘å±•
        if len(targets) < 3:  # ç¡®ä¿è‡³å°‘æœ‰è¶‹åŠ¿ç›‘æ§ç›®æ ‡
            trend_target = ExplorationTarget(
                target_id=f"autonomous_trend_{int(time.time())}",
                description="ç³»ç»Ÿè‡ªä¸»è¶‹åŠ¿ç›‘æ§æ¢ç´¢",
                keywords=["æœ€æ–°å‘å±•", "æŠ€æœ¯è¶‹åŠ¿", "åˆ›æ–°åº”ç”¨", "æœªæ¥æ–¹å‘"],
                priority=ExplorationPriority.LOW,
                domain="æŠ€æœ¯",
                expected_quality=KnowledgeQuality.MEDIUM,
                metadata={
                    "exploration_mode": "autonomous",
                    "target_type": "trend_monitoring",
                    "search_depth": "surface",
                    "search_diversity": "wide",
                    "temporal_focus": "recent",
                    "future_oriented": True
                }
            )
            targets.append(trend_target)
        
        logger.debug(f"ğŸ”„ è‡ªä¸»æ¢ç´¢ç›®æ ‡åˆ›å»ºå®Œæˆ: {len(targets)} ä¸ªå®½æ³›æ¢ç´¢ç›®æ ‡")
        return targets
    
    
    def _identify_query_domain(self, user_query: str) -> str:
        """ğŸ§  æ™ºèƒ½è¯†åˆ«ç”¨æˆ·æŸ¥è¯¢çš„é¢†åŸŸ - ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ†æï¼Œå›é€€åˆ°å…³é”®è¯åŒ¹é…"""
        
        # ğŸ§  ä¼˜å…ˆå°è¯•è¯­ä¹‰åˆ†æ
        if self.semantic_analyzer and user_query:
            try:
                response = self.semantic_analyzer.analyze(user_query, ['domain_classification'])
                
                if response.overall_success:
                    domain_result = response.analysis_results.get('domain_classification')
                    
                    if domain_result and domain_result.success and domain_result.confidence > 0.7:
                        semantic_domain = domain_result.result.get('domain', '').lower()
                        
                        # æ˜ å°„è¯­ä¹‰åˆ†æç»“æœåˆ°æˆ‘ä»¬çš„é¢†åŸŸåˆ†ç±»
                        domain_mapping = {
                            'technology': 'æŠ€æœ¯',
                            'technical': 'æŠ€æœ¯',
                            'programming': 'æŠ€æœ¯',
                            'business': 'å•†ä¸š',
                            'commercial': 'å•†ä¸š',
                            'marketing': 'å•†ä¸š',
                            'academic': 'å­¦æœ¯',
                            'research': 'å­¦æœ¯',
                            'scientific': 'å­¦æœ¯',
                            'health': 'å¥åº·',
                            'medical': 'å¥åº·',
                            'healthcare': 'å¥åº·',
                            'education': 'æ•™è‚²',
                            'educational': 'æ•™è‚²',
                            'learning': 'æ•™è‚²',
                        }
                        
                        if semantic_domain in domain_mapping:
                            mapped_domain = domain_mapping[semantic_domain]
                            logger.debug(f"ğŸ§  è¯­ä¹‰åˆ†æè¯†åˆ«é¢†åŸŸ: {semantic_domain} â†’ {mapped_domain}")
                            return mapped_domain
                
            except Exception as e:
                logger.warning(f"âš ï¸ é¢†åŸŸè¯†åˆ«è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
        
        # ğŸš¨ è¯­ä¹‰åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é¢†åŸŸ
        logger.warning("âš ï¸ è¯­ä¹‰åˆ†æä¸å¯ç”¨æˆ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é€šç”¨é¢†åŸŸ")
        return "é€šç”¨"
    
    
    def _extract_core_keywords_advanced(self, user_query: str) -> List[str]:
        """ğŸ¯ é«˜çº§å…³é”®è¯æå– - é’ˆå¯¹ç”¨æˆ·æŸ¥è¯¢ä¼˜åŒ–"""
        import re
        
        # ä¸“ä¸šæœ¯è¯­æƒé‡æå‡
        technical_terms = ['API', 'api', 'ç®—æ³•', 'æ¶æ„', 'ç³»ç»Ÿ', 'æ¡†æ¶', 'æ¨¡å‹', 'ä¼˜åŒ–', 'æœºå™¨å­¦ä¹ ', 'AI', 
                          'äººå·¥æ™ºèƒ½', 'æ·±åº¦å­¦ä¹ ', 'æ•°æ®åˆ†æ', 'äº‘è®¡ç®—', 'å¾®æœåŠ¡', 'å®¹å™¨', 'åˆ†å¸ƒå¼']
        
        # æå–æ‰€æœ‰è¯æ±‡
        words = re.findall(r'\b\w+\b', user_query)
        
        # æŒ‰é‡è¦æ€§åˆ†ç±»
        high_priority_keywords = []
        medium_priority_keywords = []
        
        for word in words:
            if len(word) > 2:
                # ä¸“ä¸šæœ¯è¯­ä¼˜å…ˆçº§æœ€é«˜
                if any(term.lower() == word.lower() for term in technical_terms):
                    high_priority_keywords.append(word)
                # é•¿è¯æ±‡é€šå¸¸æ›´å…·ä½“
                elif len(word) > 5:
                    high_priority_keywords.append(word)
                # å…¶ä»–æœ‰æ•ˆè¯æ±‡
                elif word.lower() not in {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'å“ªé‡Œ', 'ä¸ºä»€ä¹ˆ', 
                                        'the', 'is', 'in', 'and', 'or', 'but', 'how', 'what', 'where', 'why', 'when'}:
                    medium_priority_keywords.append(word)
        
        # ç»„åˆå¹¶å»é‡ï¼Œä¼˜å…ˆçº§é«˜çš„åœ¨å‰
        core_keywords = list(dict.fromkeys(high_priority_keywords + medium_priority_keywords))
        return core_keywords[:12]  # è¿”å›å‰12ä¸ªæ ¸å¿ƒå…³é”®è¯
    
    def _analyze_user_query_intent(self, user_query: str) -> Dict[str, Any]:
        """ğŸ¯ åˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾ - æŒ‡å¯¼æ¢ç´¢ç­–ç•¥"""
        query_lower = user_query.lower()
        intent = {
            "primary_intent": "information_seeking",
            "requires_context": False,
            "requires_verification": False,
            "complexity_level": "medium",
            "temporal_focus": "current",
            "context_topics": [],
            "verification_aspects": []
        }
        
        # æ„å›¾åˆ†ç±»
        if any(word in query_lower for word in ['å¦‚ä½•', 'æ€ä¹ˆ', 'æ–¹æ³•', 'how', 'method', 'å®ç°']):
            intent["primary_intent"] = "solution_seeking"
            intent["requires_context"] = True
            intent["context_topics"] = ["æœ€ä½³å®è·µ", "å®ç°æ–¹æ³•", "æ¡ˆä¾‹ç ”ç©¶"]
            
        elif any(word in query_lower for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'é€‰æ‹©', 'å“ªä¸ªå¥½', 'compare', 'vs', 'versus']):
            intent["primary_intent"] = "comparison_analysis"
            intent["requires_verification"] = True
            intent["verification_aspects"] = ["ä¼˜åŠ£å¯¹æ¯”", "é€‚ç”¨åœºæ™¯", "æ€§èƒ½å·®å¼‚"]
            
        elif any(word in query_lower for word in ['å¯è¡Œ', 'å¯è¡Œæ€§', 'èƒ½å¦', 'æ˜¯å¦å¯èƒ½', 'feasible', 'possible']):
            intent["primary_intent"] = "feasibility_assessment"
            intent["requires_verification"] = True
            intent["verification_aspects"] = ["æŠ€æœ¯å¯è¡Œæ€§", "å®æ–½éš¾åº¦", "é£é™©è¯„ä¼°"]
            
        elif any(word in query_lower for word in ['æœ€æ–°', 'è¶‹åŠ¿', 'å‘å±•', 'åŠ¨æ€', 'latest', 'trend', 'recent']):
            intent["primary_intent"] = "trend_monitoring"
            intent["temporal_focus"] = "recent"
            intent["requires_context"] = True
            intent["context_topics"] = ["å‘å±•è¶‹åŠ¿", "æœªæ¥æ–¹å‘", "æŠ€æœ¯æ¼”è¿›"]
        
        # å¤æ‚åº¦è¯„ä¼°
        if len(user_query) > 50 or len(user_query.split()) > 10:
            intent["complexity_level"] = "high"
        elif len(user_query) < 20 or len(user_query.split()) < 5:
            intent["complexity_level"] = "low"
            
        return intent
    
    def _generate_contextual_keywords(self, user_query: str, core_keywords: List[str]) -> List[str]:
        """ğŸ¯ ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³å…³é”®è¯"""
        contextual_keywords = core_keywords.copy()
        
        # åŸºäºæŸ¥è¯¢å†…å®¹æ·»åŠ ä¸Šä¸‹æ–‡è¯æ±‡
        query_lower = user_query.lower()
        
        if any(tech in query_lower for tech in ['api', 'ç³»ç»Ÿ', 'æ¶æ„', 'æŠ€æœ¯']):
            contextual_keywords.extend(["æŠ€æœ¯èƒŒæ™¯", "å®ç°åŸç†", "åº”ç”¨åœºæ™¯"])
            
        elif any(biz in query_lower for biz in ['å•†ä¸š', 'å¸‚åœº', 'ä¸šåŠ¡', 'è¥é”€']):
            contextual_keywords.extend(["å•†ä¸šæ¨¡å¼", "å¸‚åœºåˆ†æ", "ç«äº‰æ ¼å±€"])
            
        elif any(academic in query_lower for academic in ['ç ”ç©¶', 'å­¦æœ¯', 'ç†è®º', 'æ–¹æ³•è®º']):
            contextual_keywords.extend(["ç ”ç©¶ç°çŠ¶", "ç†è®ºåŸºç¡€", "æ–¹æ³•è®º"])
        
        return list(dict.fromkeys(contextual_keywords))[:10]
    
    def _generate_verification_keywords(self, user_query: str, core_keywords: List[str]) -> List[str]:
        """ğŸ¯ ç”ŸæˆéªŒè¯ç›¸å…³å…³é”®è¯"""
        verification_keywords = core_keywords.copy()
        
        # æ·»åŠ éªŒè¯å¯¼å‘çš„å…³é”®è¯
        verification_keywords.extend([
            "å¯è¡Œæ€§åˆ†æ", "æŠ€æœ¯è¯„ä¼°", "é£é™©è¯„ä»·", "å®æ–½éš¾åº¦", 
            "æˆåŠŸæ¡ˆä¾‹", "å¤±è´¥æ•™è®­", "æŒ‘æˆ˜ä¸éšœç¢", "è§£å†³æ–¹æ¡ˆ"
        ])
        
        # åŸºäºæŸ¥è¯¢ç±»å‹æ·»åŠ ç‰¹å®šéªŒè¯è¯æ±‡
        query_lower = user_query.lower()
        
        if any(word in query_lower for word in ['æŠ€æœ¯', 'ç³»ç»Ÿ', 'api', 'æ¶æ„']):
            verification_keywords.extend(["æŠ€æœ¯é£é™©", "æ€§èƒ½ç“¶é¢ˆ", "æ‰©å±•æ€§", "ç»´æŠ¤æˆæœ¬"])
            
        elif any(word in query_lower for word in ['å•†ä¸š', 'é¡¹ç›®', 'æŠ•èµ„']):
            verification_keywords.extend(["æŠ•èµ„å›æŠ¥", "å¸‚åœºé£é™©", "ç«äº‰å¨èƒ", "ç›ˆåˆ©èƒ½åŠ›"])
        
        return list(dict.fromkeys(verification_keywords))[:12]
    
    def _calculate_success_rate(self, result: ExplorationResult) -> float:
        """è®¡ç®—æ¢ç´¢æˆåŠŸç‡"""
        if not result.targets:
            return 0.0
        
        successful_targets = 0
        for target in result.targets:
            # å¦‚æœç›®æ ‡äº§ç”Ÿäº†çŸ¥è¯†æˆ–ç§å­ï¼Œè§†ä¸ºæˆåŠŸ
            target_knowledge = [k for k in result.discovered_knowledge 
                              if target.target_id in k.related_concepts]
            target_seeds = [s for s in result.generated_seeds 
                          if any(target.target_id in s.generation_context.get("related_targets", []))]
            
            if target_knowledge or target_seeds:
                successful_targets += 1
        
        return successful_targets / len(result.targets)
    
    def _calculate_quality_score(self, result: ExplorationResult) -> float:
        """è®¡ç®—æ•´ä½“è´¨é‡è¯„åˆ†"""
        if not result.discovered_knowledge:
            return 0.0
        
        total_score = 0.0
        for knowledge in result.discovered_knowledge:
            knowledge_score = (knowledge.confidence_score + 
                             knowledge.relevance_score + 
                             knowledge.novelty_score) / 3
            total_score += knowledge_score
        
        return total_score / len(result.discovered_knowledge)
    
    def _update_caches_and_stats(self, result: ExplorationResult):
        """æ›´æ–°ç¼“å­˜å’Œç»Ÿè®¡ä¿¡æ¯"""
        # æ›´æ–°æ¢ç´¢å†å²
        self.exploration_history.append(result)
        
        # æ›´æ–°çŸ¥è¯†ç¼“å­˜
        for knowledge in result.discovered_knowledge:
            self.knowledge_cache[knowledge.knowledge_id] = knowledge
        
        # æ›´æ–°ç§å­ç¼“å­˜
        for seed in result.generated_seeds:
            self.seed_cache[seed.seed_id] = seed
        
        # æ›´æ–°ç­–ç•¥æ€§èƒ½
        strategy_name = result.strategy.value
        self.strategy_performance[strategy_name]["success_rate"] = (
            (self.strategy_performance[strategy_name]["success_rate"] + result.success_rate) / 2
        )
        self.strategy_performance[strategy_name]["avg_quality"] = (
            (self.strategy_performance[strategy_name]["avg_quality"] + result.quality_score) / 2
        )
        self.strategy_performance[strategy_name]["total_seeds"] += len(result.generated_seeds)
        
        # æ›´æ–°å…¨å±€ç»Ÿè®¡
        self.stats["total_explorations"] += 1
        if result.success_rate > 0.5:
            self.stats["successful_explorations"] += 1
        self.stats["total_knowledge_discovered"] += len(result.discovered_knowledge)
        self.stats["total_seeds_generated"] += len(result.generated_seeds)
        
        # æ›´æ–°å¹³å‡å€¼
        total_explorations = self.stats["total_explorations"]
        self.stats["average_quality_score"] = (
            (self.stats["average_quality_score"] * (total_explorations - 1) + result.quality_score) / 
            total_explorations
        )
        self.stats["average_execution_time"] = (
            (self.stats["average_execution_time"] * (total_explorations - 1) + result.execution_time) / 
            total_explorations
        )
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_caches()
    
    def _cleanup_caches(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        max_history = 100
        max_knowledge_cache = 500
        max_seed_cache = 300
        
        # æ¸…ç†æ¢ç´¢å†å²
        if len(self.exploration_history) > max_history:
            self.exploration_history = self.exploration_history[-max_history//2:]
        
        # æ¸…ç†çŸ¥è¯†ç¼“å­˜ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
        if len(self.knowledge_cache) > max_knowledge_cache:
            sorted_knowledge = sorted(
                self.knowledge_cache.items(),
                key=lambda x: x[1].discovered_at,
                reverse=True
            )
            self.knowledge_cache = dict(sorted_knowledge[:max_knowledge_cache//2])
        
        # æ¸…ç†ç§å­ç¼“å­˜ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
        if len(self.seed_cache) > max_seed_cache:
            sorted_seeds = sorted(
                self.seed_cache.items(),
                key=lambda x: x[1].generated_at,
                reverse=True
            )
            self.seed_cache = dict(sorted_seeds[:max_seed_cache//2])
    
    # ==================== å…¬å…±æ¥å£æ–¹æ³• ====================
    
    def create_exploration_targets_from_context(self, 
                                              context: Dict[str, Any]) -> List[ExplorationTarget]:
        """åŸºäºä¸Šä¸‹æ–‡åˆ›å»ºæ¢ç´¢ç›®æ ‡"""
        targets = []
        
        # ä»ä¸Šä¸‹æ–‡æå–å…³é”®ä¿¡æ¯
        session_insights = context.get("session_insights", {})
        knowledge_gaps = context.get("current_knowledge_gaps", [])
        
        # åŸºäºçŸ¥è¯†ç¼ºå£åˆ›å»ºç›®æ ‡
        for gap in knowledge_gaps[:3]:  # é™åˆ¶ç›®æ ‡æ•°é‡
            target = ExplorationTarget(
                target_id=f"gap_target_{gap.get('gap_id', 'unknown')}",
                target_type="knowledge_gap",
                description=gap.get("description", "çŸ¥è¯†ç¼ºå£æ¢ç´¢"),
                keywords=gap.get("keywords", [gap.get("area", "")]),
                priority=gap.get("exploration_priority", 0.5)
            )
            targets.append(target)
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ç¼ºå£ï¼Œåˆ›å»ºé€šç”¨æ¢ç´¢ç›®æ ‡
        if not targets:
            general_target = ExplorationTarget(
                target_id=f"general_target_{int(time.time())}",
                target_type="general_exploration",
                description="é€šç”¨çŸ¥è¯†æ¢ç´¢",
                keywords=["æŠ€æœ¯è¶‹åŠ¿", "åˆ›æ–°æ–¹æ³•", "æœ€ä½³å®è·µ"],
                priority=0.6
            )
            targets.append(general_target)
        
        return targets
    
    def get_exploration_stats(self) -> Dict[str, Any]:
        """è·å–æ¢ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "strategy_performance": dict(self.strategy_performance),
            "cache_status": {
                "knowledge_cache_size": len(self.knowledge_cache),
                "seed_cache_size": len(self.seed_cache),
                "exploration_history_size": len(self.exploration_history)
            },
            "recent_explorations": len([r for r in self.exploration_history 
                                      if time.time() - r.timestamp < 3600])  # æœ€è¿‘1å°æ—¶
        }
    
    def _merge_config(self, base_config: Dict, user_config: Dict):
        """é€’å½’åˆå¹¶é…ç½®"""
        for key, value in user_config.items():
            if key in base_config and isinstance(base_config[key], dict) and isinstance(value, dict):
                self._merge_config(base_config[key], value)
            else:
                base_config[key] = value
