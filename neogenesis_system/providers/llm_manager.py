#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LLMç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†å¤šä¸ªLLMæä¾›å•†
LLM Manager - Unified management for multiple LLM providers
"""

import os
import time
import logging
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
from collections import defaultdict

from .llm_base import BaseLLMClient, LLMConfig, LLMProvider, LLMResponse, LLMMessage
from .impl.deepseek_client import create_llm_client
try:
    from neogenesis_system.config import (
        LLM_PROVIDERS_CONFIG, DEFAULT_LLM_CONFIG, LLM_MANAGER_CONFIG, 
        COST_CONTROL_CONFIG, FEATURE_FLAGS
    )
except ImportError:
    try:
        from ..config import (
            LLM_PROVIDERS_CONFIG, DEFAULT_LLM_CONFIG, LLM_MANAGER_CONFIG, 
            COST_CONTROL_CONFIG, FEATURE_FLAGS
        )
    except ImportError:
        # æä¾›é»˜è®¤é…ç½®
        LLM_PROVIDERS_CONFIG = {}
        DEFAULT_LLM_CONFIG = {}
        LLM_MANAGER_CONFIG = {}
        COST_CONTROL_CONFIG = {}
        FEATURE_FLAGS = {}

logger = logging.getLogger(__name__)


@dataclass
class ProviderStatus:
    """æä¾›å•†çŠ¶æ€"""
    name: str
    enabled: bool
    healthy: bool
    last_check: float
    error_count: int
    success_count: int
    avg_response_time: float
    last_error: Optional[str] = None


class LLMManager:
    """
    LLMç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†å¤šä¸ªLLMæä¾›å•†
    
    åŠŸèƒ½ï¼š
    - è‡ªåŠ¨å‘ç°å’Œåˆå§‹åŒ–å¯ç”¨çš„LLMæä¾›å•†
    - æ™ºèƒ½è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
    - è‡ªåŠ¨å›é€€æœºåˆ¶
    - æˆæœ¬è·Ÿè¸ªå’Œæ§åˆ¶
    - å¥åº·æ£€æŸ¥å’Œç›‘æ§
    """
    
    def __init__(self):
        """åˆå§‹åŒ–LLMç®¡ç†å™¨"""
        self.providers: Dict[str, BaseLLMClient] = {}
        self.provider_status: Dict[str, ProviderStatus] = {}
        self.config = DEFAULT_LLM_CONFIG.copy()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'fallback_count': 0,
            'provider_usage': defaultdict(int),
            'cost_tracking': defaultdict(float),
            'request_history': []
        }
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.initialized = False
        self.last_health_check = 0
        
        logger.info("ğŸš€ LLMç®¡ç†å™¨åˆå§‹åŒ–å¼€å§‹")
        self._initialize_providers()
        
    def _initialize_providers(self):
        """åˆå§‹åŒ–å¯ç”¨çš„æä¾›å•†"""
        if not FEATURE_FLAGS.get("enable_multi_llm_support", False):
            logger.info("ğŸ“‹ å¤šLLMæ”¯æŒå·²ç¦ç”¨ï¼Œä»…ä½¿ç”¨DeepSeek")
            self._initialize_single_provider()
            return
        
        initialized_count = 0
        
        for provider_name, provider_config in LLM_PROVIDERS_CONFIG.items():
            try:
                if not provider_config.get("enabled", False):
                    logger.debug(f"â­ï¸ è·³è¿‡ç¦ç”¨çš„æä¾›å•†: {provider_name}")
                    continue
                
                # æ£€æŸ¥APIå¯†é’¥
                api_key_env = provider_config.get("api_key_env")
                api_key = ""
                
                if api_key_env:
                    api_key = os.getenv(api_key_env, "")
                    if not api_key:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{provider_name}çš„APIå¯†é’¥: {api_key_env}")
                        continue
                
                # åˆ›å»ºLLMé…ç½®
                llm_config = self._create_llm_config(provider_name, provider_config, api_key)
                
                # åˆ›å»ºå®¢æˆ·ç«¯
                client = create_llm_client(llm_config.api_key)
                
                # å¿«é€Ÿå¥åº·æ£€æŸ¥
                if self._quick_health_check(client, provider_name):
                    self.providers[provider_name] = client
                    self.provider_status[provider_name] = ProviderStatus(
                        name=provider_name,
                        enabled=True,
                        healthy=True,
                        last_check=time.time(),
                        error_count=0,
                        success_count=1,
                        avg_response_time=0.0
                    )
                    initialized_count += 1
                    logger.info(f"âœ… {provider_name}æä¾›å•†åˆå§‹åŒ–æˆåŠŸ")
                else:
                    logger.warning(f"âŒ {provider_name}æä¾›å•†å¥åº·æ£€æŸ¥å¤±è´¥")
                    
            except Exception as e:
                logger.error(f"âŒ åˆå§‹åŒ–{provider_name}æä¾›å•†å¤±è´¥: {e}")
                continue
        
        if initialized_count == 0:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„LLMæä¾›å•†ï¼Œå›é€€åˆ°å•ä¸€æä¾›å•†æ¨¡å¼")
            self._initialize_single_provider()
        else:
            logger.info(f"ğŸ‰ LLMç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æä¾›å•†: {initialized_count}ä¸ª")
            self.initialized = True
    
    def _initialize_single_provider(self):
        """åˆå§‹åŒ–å•ä¸€æä¾›å•†ï¼ˆDeepSeekï¼‰"""
        try:
            from .client_adapter import get_or_create_unified_client
            
            api_key = os.getenv("DEEPSEEK_API_KEY", "")
            if not api_key:
                logger.error("âŒ æœªæ‰¾åˆ°DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
                return
            
            client = get_or_create_unified_client(api_key)
            self.providers["deepseek"] = client
            self.provider_status["deepseek"] = ProviderStatus(
                name="deepseek",
                enabled=True,
                healthy=True,
                last_check=time.time(),
                error_count=0,
                success_count=1,
                avg_response_time=0.0
            )
            
            logger.info("âœ… å•ä¸€æä¾›å•†æ¨¡å¼åˆå§‹åŒ–æˆåŠŸ (DeepSeek)")
            self.initialized = True
            
        except Exception as e:
            logger.error(f"âŒ å•ä¸€æä¾›å•†åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _create_llm_config(self, provider_name: str, provider_config: Dict, api_key: str) -> LLMConfig:
        """åˆ›å»ºLLMé…ç½®"""
        provider_type = provider_config["provider_type"]
        provider_enum = LLMProvider(provider_type)
        
        # ç‰¹æ®Šå¤„ç†Azure OpenAI
        extra_params = {}
        base_url = provider_config.get("base_url")
        
        if provider_type == "azure_openai":
            azure_endpoint = os.getenv(provider_config.get("azure_endpoint_env", ""), "")
            if azure_endpoint:
                base_url = azure_endpoint
                extra_params["api_version"] = provider_config.get("api_version", "2024-02-15-preview")
        
        return LLMConfig(
            provider=provider_enum,
            api_key=api_key,
            model_name=provider_config["default_model"],
            temperature=provider_config.get("temperature", 0.7),
            max_tokens=provider_config.get("max_tokens", 2000),
            base_url=base_url,
            timeout=tuple(provider_config.get("timeout", (30, 120))),
            max_retries=provider_config.get("max_retries", 3),
            retry_delay_base=provider_config.get("retry_delay_base", 2.0),
            request_interval=provider_config.get("request_interval", 1.0),
            extra_params=extra_params
        )
    
    def _quick_health_check(self, client: BaseLLMClient, provider_name: str) -> bool:
        """å¿«é€Ÿå¥åº·æ£€æŸ¥"""
        try:
            # ç®€å•çš„é…ç½®éªŒè¯
            return client.validate_config()
        except Exception as e:
            logger.debug(f"ğŸ” {provider_name}å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def call_api(self, prompt: str, 
                 system_message: Optional[str] = None,
                 temperature: Optional[float] = None,
                 **kwargs) -> str:
        """
        ç®€åŒ–çš„APIè°ƒç”¨æ¥å£ - å…¼å®¹ç°æœ‰ä»£ç 
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            temperature: æ¸©åº¦å‚æ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            str: LLMå“åº”å†…å®¹
            
        Raises:
            Exception: è°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = []
            if system_message:
                messages.append(LLMMessage(role="system", content=system_message))
            messages.append(LLMMessage(role="user", content=prompt))
            
            # è°ƒç”¨chat_completion
            response = self.chat_completion(
                messages=messages,
                temperature=temperature,
                **kwargs
            )
            
            if response.success:
                return response.content
            else:
                raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {response.error_message}")
                
        except Exception as e:
            logger.error(f"âŒ LLM APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def chat_completion(self, 
                       messages: Union[str, List[LLMMessage]], 
                       provider_name: Optional[str] = None,
                       temperature: Optional[float] = None,
                       **kwargs) -> LLMResponse:
        """
        èŠå¤©å®Œæˆ - æ™ºèƒ½è·¯ç”±åˆ°æœ€ä½³æä¾›å•†
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹
            provider_name: æŒ‡å®šæä¾›å•†ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: ç»Ÿä¸€å“åº”
        """
        self.stats['total_requests'] += 1
        
        if not self.initialized or not self.providers:
            return self._create_error_response("æ²¡æœ‰å¯ç”¨çš„LLMæä¾›å•†")
        
        # å¤„ç†ç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²çš„æƒ…å†µ
        if isinstance(messages, str):
            messages = [LLMMessage(role="user", content=messages)]
        
        # é€‰æ‹©æä¾›å•†
        selected_provider = self._select_provider(provider_name)
        if not selected_provider:
            return self._create_error_response("æ— æ³•é€‰æ‹©åˆé€‚çš„æä¾›å•†")
        
        # æ·»åŠ temperatureåˆ°kwargsä¸­
        if temperature is not None:
            kwargs['temperature'] = temperature
        
        # æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦å›é€€æœºåˆ¶ï¼‰
        return self._execute_with_fallback(selected_provider, messages, **kwargs)
    
    def _select_provider(self, preferred_provider: Optional[str] = None) -> Optional[str]:
        """é€‰æ‹©æä¾›å•†"""
        # å¦‚æœæŒ‡å®šäº†æä¾›å•†ä¸”å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨
        if preferred_provider and preferred_provider in self.providers:
            if self.provider_status[preferred_provider].healthy:
                return preferred_provider
        
        # æ£€æŸ¥ä¸»è¦æä¾›å•†è®¾ç½®
        primary = self.config.get("primary_provider", "auto")
        
        if primary == "auto":
            # è‡ªåŠ¨é€‰æ‹©ï¼šæŒ‰é¦–é€‰é¡ºåºé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„æä¾›å•†
            preferred_order = self.config.get("preferred_providers", ["deepseek", "openai", "anthropic"])
            for provider_name in preferred_order:
                if provider_name in self.providers and self.provider_status[provider_name].healthy:
                    return provider_name
        else:
            # ä½¿ç”¨æŒ‡å®šçš„ä¸»è¦æä¾›å•†
            if primary in self.providers and self.provider_status[primary].healthy:
                return primary
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¥åº·çš„æä¾›å•†
        for name, status in self.provider_status.items():
            if status.healthy and name in self.providers:
                return name
        
        return None
    
    def _execute_with_fallback(self, provider_name: str, messages: Union[str, List[LLMMessage]], **kwargs) -> LLMResponse:
        """æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦å›é€€æœºåˆ¶ï¼‰"""
        providers_to_try = [provider_name]
        
        # æ·»åŠ å›é€€æä¾›å•†
        if self.config.get("auto_fallback", True):
            fallback_providers = self.config.get("fallback_providers", [])
            for fallback in fallback_providers:
                if fallback != provider_name and fallback in self.providers:
                    providers_to_try.append(fallback)
        
        last_error = None
        
        for current_provider in providers_to_try:
            try:
                if not self.provider_status[current_provider].healthy:
                    continue
                
                start_time = time.time()
                client = self.providers[current_provider]
                
                logger.info(f"ğŸ¤– ä½¿ç”¨æä¾›å•†: {current_provider}")
                response = client.chat_completion(messages, **kwargs)
                
                response_time = time.time() - start_time
                
                if response.success:
                    # æ›´æ–°ç»Ÿè®¡
                    self._update_provider_stats(current_provider, True, response_time)
                    self.stats['successful_requests'] += 1
                    self.stats['provider_usage'][current_provider] += 1
                    
                    # æˆæœ¬è·Ÿè¸ª
                    if response.usage and COST_CONTROL_CONFIG.get("token_usage_tracking", True):
                        self._track_cost(current_provider, response.usage)
                    
                    return response
                else:
                    # è®°å½•å¤±è´¥ä½†ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæä¾›å•†
                    self._update_provider_stats(current_provider, False, response_time)
                    last_error = response.error_message
                    logger.warning(f"âš ï¸ {current_provider}è¯·æ±‚å¤±è´¥: {last_error}")
                    
                    if len(providers_to_try) > 1:
                        self.stats['fallback_count'] += 1
                        continue
                    else:
                        return response
                        
            except Exception as e:
                self._update_provider_stats(current_provider, False, 0)
                last_error = str(e)
                logger.error(f"âŒ {current_provider}æ‰§è¡Œå¼‚å¸¸: {e}")
                continue
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥
        self.stats['failed_requests'] += 1
        return self._create_error_response(f"æ‰€æœ‰æä¾›å•†éƒ½ä¸å¯ç”¨: {last_error}")
    
    def _update_provider_stats(self, provider_name: str, success: bool, response_time: float):
        """æ›´æ–°æä¾›å•†ç»Ÿè®¡"""
        if provider_name not in self.provider_status:
            return
        
        status = self.provider_status[provider_name]
        status.last_check = time.time()
        
        if success:
            status.success_count += 1
            status.error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
            status.healthy = True
            
            # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
            if status.avg_response_time == 0:
                status.avg_response_time = response_time
            else:
                status.avg_response_time = (status.avg_response_time + response_time) / 2
        else:
            status.error_count += 1
            
            # è¿ç»­é”™è¯¯è¿‡å¤šæ—¶æ ‡è®°ä¸ºä¸å¥åº·
            if status.error_count >= 3:
                status.healthy = False
                logger.warning(f"âš ï¸ {provider_name}è¢«æ ‡è®°ä¸ºä¸å¥åº·")
    
    def _track_cost(self, provider_name: str, usage):
        """è·Ÿè¸ªæˆæœ¬"""
        try:
            provider_config = LLM_PROVIDERS_CONFIG.get(provider_name, {})
            cost_per_1k = provider_config.get("cost_per_1k_tokens", {})
            
            input_cost = (usage.prompt_tokens / 1000) * cost_per_1k.get("input", 0)
            output_cost = (usage.completion_tokens / 1000) * cost_per_1k.get("output", 0)
            total_cost = input_cost + output_cost
            
            self.stats['cost_tracking'][provider_name] += total_cost
            
            logger.debug(f"ğŸ’° {provider_name}æˆæœ¬: ${total_cost:.6f}")
            
        except Exception as e:
            logger.debug(f"âŒ æˆæœ¬è·Ÿè¸ªå¤±è´¥: {e}")
    
    def _create_error_response(self, error_message: str) -> LLMResponse:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        from .llm_base import LLMErrorType, create_error_response
        return create_error_response(
            provider="llm_manager",
            error_type=LLMErrorType.UNKNOWN_ERROR,
            error_message=error_message
        )
    
    def get_provider_status(self) -> Dict[str, Any]:
        """è·å–æä¾›å•†çŠ¶æ€"""
        return {
            'initialized': self.initialized,
            'total_providers': len(self.providers),
            'healthy_providers': sum(1 for s in self.provider_status.values() if s.healthy),
            'providers': {name: {
                'enabled': status.enabled,
                'healthy': status.healthy,
                'success_count': status.success_count,
                'error_count': status.error_count,
                'avg_response_time': status.avg_response_time,
                'last_error': status.last_error
            } for name, status in self.provider_status.items()},
            'stats': self.stats.copy()
        }
    
    def get_available_models(self, provider_name: Optional[str] = None) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨æ¨¡å‹"""
        if provider_name:
            if provider_name in self.providers:
                return {provider_name: self.providers[provider_name].get_available_models()}
            else:
                return {}
        
        return {
            name: client.get_available_models() 
            for name, client in self.providers.items()
        }
    
    def switch_primary_provider(self, provider_name: str) -> bool:
        """åˆ‡æ¢ä¸»è¦æä¾›å•†"""
        if provider_name in self.providers and self.provider_status[provider_name].healthy:
            self.config["primary_provider"] = provider_name
            logger.info(f"ğŸ”„ ä¸»è¦æä¾›å•†å·²åˆ‡æ¢åˆ°: {provider_name}")
            return True
        return False
    
    def health_check(self, force: bool = False) -> Dict[str, bool]:
        """å¥åº·æ£€æŸ¥"""
        current_time = time.time()
        check_interval = LLM_MANAGER_CONFIG.get("health_check_interval", 300)
        
        if not force and (current_time - self.last_health_check) < check_interval:
            return {name: status.healthy for name, status in self.provider_status.items()}
        
        logger.info("ğŸ” æ‰§è¡ŒLLMæä¾›å•†å¥åº·æ£€æŸ¥")
        results = {}
        
        for name, client in self.providers.items():
            try:
                healthy = self._quick_health_check(client, name)
                self.provider_status[name].healthy = healthy
                results[name] = healthy
                logger.debug(f"ğŸ” {name}: {'âœ…' if healthy else 'âŒ'}")
            except Exception as e:
                self.provider_status[name].healthy = False
                results[name] = False
                logger.error(f"ğŸ” {name}å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
        
        self.last_health_check = current_time
        return results

    def generate_response(self, query: str, provider: str = 'deepseek', **kwargs) -> str:
        """
        ç”Ÿæˆå“åº” - ç®€åŒ–çš„APIè°ƒç”¨æ¥å£
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            provider: æŒ‡å®šæä¾›å•†
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            str: ç”Ÿæˆçš„å“åº”å†…å®¹
        """
        try:
            from .llm_base import LLMMessage
            messages = [LLMMessage(role='user', content=query)]
            response = self.chat_completion(
                messages=messages,
                provider_name=provider,
                **kwargs
            )
            return response.content
        except Exception as e:
            logger.error(f"ç”Ÿæˆå“åº”å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶é‡åˆ°é”™è¯¯: {str(e)}"
