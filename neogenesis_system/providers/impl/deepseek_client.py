#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¼ºåŒ–ç‰ˆ DeepSeek API å®¢æˆ·ç«¯ - æ”¯æŒç»Ÿä¸€LLMæ¥å£
Enhanced DeepSeek API Client with Unified LLM Interface

ç‰¹æ€§:
- å®ç°ç»Ÿä¸€LLMå®¢æˆ·ç«¯æ¥å£
- ä½¿ç”¨ requests.Session æé«˜æ€§èƒ½
- é…ç½®åŒ–çš„é‡è¯•é€»è¾‘å’Œè¶…æ—¶æ§åˆ¶
- ç²¾ç»†çš„é”™è¯¯å¤„ç†å’Œç»“æ„åŒ–æ—¥å¿—
- è‡ªåŠ¨ JSON è§£æå’Œå“åº”éªŒè¯
- æµå¼å“åº”æ”¯æŒ
- è¯·æ±‚ç¼“å­˜æœºåˆ¶
- æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡
"""

import json
import time
import hashlib
import logging
import asyncio
import requests
from typing import Optional, Dict, Any, List, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from contextlib import contextmanager

try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False
    httpx = None

try:
    from neogenesis_system.config import API_CONFIG, DEEPSEEK_CHAT_ENDPOINT, DEEPSEEK_MODEL
except ImportError:
    try:
        from ...config import API_CONFIG, DEEPSEEK_CHAT_ENDPOINT, DEEPSEEK_MODEL
    except ImportError:
        # æä¾›é»˜è®¤å€¼ä»¥é˜²é…ç½®æ–‡ä»¶ä¸å­˜åœ¨
        API_CONFIG = {}
        DEEPSEEK_CHAT_ENDPOINT = "https://api.deepseek.com/chat/completions"
        DEEPSEEK_MODEL = "deepseek-chat"
from ..llm_base import (
    BaseLLMClient, LLMConfig, LLMResponse, LLMMessage, LLMUsage, 
    LLMProvider, LLMErrorType, create_error_response
)

logger = logging.getLogger(__name__)


# APIErrorTypeå·²è¿ç§»åˆ°LLMErrorTypeï¼Œä¿æŒå‘åå…¼å®¹
APIErrorType = LLMErrorType

@dataclass
class APIResponse:
    """APIå“åº”æ•°æ®ç»“æ„ - å‘åå…¼å®¹"""
    success: bool
    content: str = ""
    raw_response: Optional[Dict[str, Any]] = None
    error_type: Optional[LLMErrorType] = None
    error_message: str = ""
    status_code: int = 0
    response_time: float = 0.0
    tokens_used: int = 0
    model_used: str = ""
    
    def to_llm_response(self, provider: str = "deepseek") -> LLMResponse:
        """è½¬æ¢ä¸ºç»Ÿä¸€çš„LLMResponseæ ¼å¼"""
        usage = None
        if self.tokens_used > 0:
            usage = LLMUsage(
                prompt_tokens=0,  # DeepSeekç›®å‰ä¸å•ç‹¬è¿”å›
                completion_tokens=self.tokens_used,
                total_tokens=self.tokens_used
            )
        
        return LLMResponse(
            success=self.success,
            content=self.content,
            provider=provider,
            model=self.model_used,
            response_time=self.response_time,
            usage=usage,
            error_type=self.error_type,
            error_message=self.error_message,
            raw_response=self.raw_response
        )


@dataclass  
class ClientConfig:
    """å®¢æˆ·ç«¯é…ç½® - å…¼å®¹æ—§ç‰ˆæœ¬çš„é…ç½®ç»“æ„"""
    api_key: str
    base_url: str = "https://api.deepseek.com"
    model: str = DEEPSEEK_MODEL
    timeout: tuple = (30, 180)
    max_retries: int = 3
    retry_delay_base: float = 2.0
    temperature: float = 0.7
    max_tokens: int = 2000
    enable_cache: bool = True
    cache_ttl: int = 300  # ç¼“å­˜æ—¶é—´(ç§’)
    enable_metrics: bool = True
    proxies: Optional[Dict[str, str]] = None
    request_interval: float = 1.0  # ğŸ”§ æ–°å¢ï¼šè¯·æ±‚é—´éš”æ—¶é—´(ç§’)
    
    def to_llm_config(self) -> LLMConfig:
        """è½¬æ¢ä¸ºç»Ÿä¸€çš„LLMConfigæ ¼å¼"""
        return LLMConfig(
            provider=LLMProvider.DEEPSEEK,
            api_key=self.api_key,
            model_name=self.model,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            base_url=self.base_url,
            timeout=self.timeout,
            max_retries=self.max_retries,
            retry_delay_base=self.retry_delay_base,
            enable_cache=self.enable_cache,
            cache_ttl=self.cache_ttl,
            proxies=self.proxies,
            request_interval=self.request_interval
        )


@dataclass
class ClientMetrics:
    """å®¢æˆ·ç«¯æ€§èƒ½æŒ‡æ ‡"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_response_time: float = 0.0
    total_tokens_used: int = 0
    cache_hits: int = 0
    error_counts: Dict[APIErrorType, int] = field(default_factory=dict)
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        if self.total_requests == 0:
            return 0.0
        return self.successful_requests / self.total_requests
    
    @property
    def average_response_time(self) -> float:
        """å¹³å‡å“åº”æ—¶é—´"""
        if self.successful_requests == 0:
            return 0.0
        return self.total_response_time / self.successful_requests


class DeepSeekClient(BaseLLMClient):
    """
    å¼ºåŒ–ç‰ˆ DeepSeek API å®¢æˆ·ç«¯ - å®ç°ç»Ÿä¸€LLMæ¥å£
    
    ç‰¹æ€§:
    - ç»§æ‰¿BaseLLMClientç»Ÿä¸€æ¥å£
    - é«˜æ€§èƒ½ä¼šè¯å¤ç”¨
    - æ™ºèƒ½é‡è¯•æœºåˆ¶
    - è¯·æ±‚ç¼“å­˜
    - æ€§èƒ½ç›‘æ§
    - ç»“æ„åŒ–é”™è¯¯å¤„ç†
    """
    
    def __init__(self, config: Union[ClientConfig, LLMConfig]):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯ - æ”¯æŒæ–°æ—§ä¸¤ç§é…ç½®æ ¼å¼
        
        Args:
            config: å®¢æˆ·ç«¯é…ç½®ï¼ˆClientConfigæˆ–LLMConfigï¼‰
        """
        # é…ç½®è½¬æ¢å’ŒéªŒè¯
        if isinstance(config, LLMConfig):
            # æ–°çš„ç»Ÿä¸€é…ç½®æ ¼å¼
            llm_config = config
            self.config = self._convert_llm_config_to_client_config(config)
        else:
            # æ—§çš„ClientConfigæ ¼å¼
            self.config = config
            llm_config = config.to_llm_config()
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(llm_config)
        
        # DeepSeekç‰¹æœ‰çš„æŒ‡æ ‡ç³»ç»Ÿ
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æŒ‡æ ‡
        enable_metrics = getattr(self.config, 'enable_metrics', True)
        if enable_metrics:
            self.metrics = ClientMetrics()
        else:
            self.metrics = None
        
        # åˆå§‹åŒ– requests.Session
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {self.config.api_key}',
            'Content-Type': 'application/json',
            'User-Agent': 'Neogenesis-System/1.0'
        })
        
        # é…ç½®ä»£ç†
        if self.config.proxies:
            self.session.proxies.update(self.config.proxies)
        
        # ğŸš€ åˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯
        self.async_client = None
        if HTTPX_AVAILABLE:
            self._init_async_client()
        else:
            logger.warning("âš ï¸ httpxæœªå®‰è£…ï¼Œå¼‚æ­¥åŠŸèƒ½ä¸å¯ç”¨ã€‚è¯·å®‰è£…: pip install httpx")
        
        # è¯·æ±‚ç¼“å­˜
        self._cache: Dict[str, tuple] = {}  # key -> (response, timestamp)
        
        # ğŸ”§ æ–°å¢ï¼šè¯·æ±‚é¢‘ç‡æ§åˆ¶
        self._last_request_time = 0
        self._request_interval = getattr(self.config, 'request_interval', 1.0)  # é»˜è®¤1ç§’é—´éš”
        
        logger.info(f"ğŸš€ DeepSeekClient åˆå§‹åŒ–å®Œæˆ")
        # å…¼å®¹æ—§çš„ClientConfigå’Œæ–°çš„LLMConfig
        model_name = getattr(self.config, 'model', None) or getattr(self.config, 'model_name', 'deepseek-chat')
        logger.info(f"   æ¨¡å‹: {model_name}")
        logger.info(f"   ç¼“å­˜: {'å¯ç”¨' if self.config.enable_cache else 'ç¦ç”¨'}")
        # å…¼å®¹æ–°æ—§é…ç½®æ ¼å¼
        enable_metrics = getattr(self.config, 'enable_metrics', True)
        logger.info(f"   æŒ‡æ ‡: {'å¯ç”¨' if enable_metrics else 'ç¦ç”¨'}")
        logger.info(f"   è¯·æ±‚é—´éš”: {self._request_interval}s")
    
    def _convert_llm_config_to_client_config(self, llm_config: LLMConfig) -> ClientConfig:
        """å°†ç»Ÿä¸€LLMConfigè½¬æ¢ä¸ºDeepSeekçš„ClientConfig"""
        return ClientConfig(
            api_key=llm_config.api_key,
            base_url=llm_config.base_url or "https://api.deepseek.com",
            model=llm_config.model_name,
            timeout=llm_config.timeout,
            max_retries=llm_config.max_retries,
            retry_delay_base=llm_config.retry_delay_base,
            temperature=llm_config.temperature,
            max_tokens=llm_config.max_tokens,
            enable_cache=llm_config.enable_cache,
            cache_ttl=llm_config.cache_ttl,
            enable_metrics=True,  # ä¸ºLLMConfigè®¾ç½®é»˜è®¤å€¼
            proxies=llm_config.proxies,
            request_interval=llm_config.request_interval
        )
    
    def _init_async_client(self):
        """ğŸš€ åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯"""
        try:
            headers = {
                'Authorization': f'Bearer {self.config.api_key}',
                'Content-Type': 'application/json',
                'User-Agent': 'Neogenesis-System/1.0'
            }
            
            # é…ç½®å¼‚æ­¥å®¢æˆ·ç«¯
            client_kwargs = {
                'headers': headers,
                'timeout': httpx.Timeout(self.config.timeout),
                'limits': httpx.Limits(max_keepalive_connections=10, max_connections=100)
            }
            
            # å¤„ç†ä»£ç†é…ç½® - ä½¿ç”¨æ›´å…¼å®¹çš„æ–¹å¼
            if self.config.proxies:
                try:
                    # å°è¯•æ–°çš„httpxæ–¹å¼
                    client_kwargs['proxies'] = self.config.proxies
                    self.async_client = httpx.AsyncClient(**client_kwargs)
                    logger.debug("ğŸš€ å¼‚æ­¥HTTPå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ˆå«ä»£ç†é…ç½®ï¼‰")
                except TypeError as te:
                    # å¦‚æœå¤±è´¥ï¼Œåˆ™ä¸ä½¿ç”¨ä»£ç†åˆ›å»ºå®¢æˆ·ç«¯
                    logger.warning(f"âš ï¸ httpxç‰ˆæœ¬ä¸æ”¯æŒproxieså‚æ•°ï¼Œè·³è¿‡ä»£ç†é…ç½®: {te}")
                    client_kwargs.pop('proxies', None)
                    self.async_client = httpx.AsyncClient(**client_kwargs)
                    logger.debug("ğŸš€ å¼‚æ­¥HTTPå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ˆæ— ä»£ç†ï¼‰")
            else:
                self.async_client = httpx.AsyncClient(**client_kwargs)
                logger.debug("ğŸš€ å¼‚æ­¥HTTPå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.async_client = None
    
    # ==================== èŠå¤©å®ŒæˆAPIæ¥å£ ====================
    
    def chat_completion(self, 
                       messages: Union[str, List[LLMMessage]], 
                       temperature: Optional[float] = None,
                       max_tokens: Optional[int] = None,
                       **kwargs) -> LLMResponse:
        """
        èŠå¤©å®ŒæˆAPIè°ƒç”¨ - ç»Ÿä¸€æ¥å£å®ç°
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: ç»Ÿä¸€çš„å“åº”å¯¹è±¡
        """
        start_time = time.time()
        
        # å‡†å¤‡æ¶ˆæ¯æ ¼å¼
        prepared_messages = self._prepare_messages(messages)
        
        # å‚æ•°å¤„ç†
        temperature = temperature or self.config.temperature
        max_tokens = max_tokens or self.config.max_tokens
        # å…¼å®¹æ–°æ—§é…ç½®æ ¼å¼
        model = kwargs.get('model') or getattr(self.config, 'model', None) or getattr(self.config, 'model_name', 'deepseek-chat')
        enable_cache = kwargs.get('enable_cache', self.config.enable_cache)
        
        # è½¬æ¢ä¸ºDeepSeek APIæ ¼å¼
        api_messages = []
        for msg in prepared_messages:
            api_messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        request_data = {
            'model': model,
            'messages': api_messages,
            'temperature': temperature,
            'max_tokens': max_tokens
        }
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(request_data)
        if enable_cache and self._is_cache_valid(cache_key):
            cached_api_response, _ = self._cache[cache_key]
            self.metrics.cache_hits += 1
            logger.debug(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜å“åº”: {cache_key[:16]}...")
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            llm_response = cached_api_response.to_llm_response("deepseek")
            self._update_stats(llm_response)
            return llm_response
        
        # æ‰§è¡ŒAPIè°ƒç”¨
        api_response = self._execute_request(request_data, start_time)
        
        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        llm_response = api_response.to_llm_response("deepseek")
        
        # æ›´æ–°ç¼“å­˜
        if enable_cache and api_response.success:
            self._cache[cache_key] = (api_response, time.time())
            self._cleanup_cache()
        
        # æ›´æ–°æŒ‡æ ‡
        enable_metrics = getattr(self.config, 'enable_metrics', True)
        if enable_metrics and self.metrics:
            self._update_metrics(api_response)
        
        # æ›´æ–°çˆ¶ç±»ç»Ÿè®¡
        self._update_stats(llm_response)
        
        return llm_response
    
    async def achat_completion(self, 
                              messages: Union[str, List[LLMMessage]], 
                              temperature: Optional[float] = None,
                              max_tokens: Optional[int] = None,
                              **kwargs) -> LLMResponse:
        """
        ğŸš€ å¼‚æ­¥èŠå¤©å®ŒæˆAPIè°ƒç”¨ - ç»Ÿä¸€æ¥å£å®ç°
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: ç»Ÿä¸€çš„å“åº”å¯¹è±¡
        """
        if not HTTPX_AVAILABLE:
            raise RuntimeError("httpxæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨å¼‚æ­¥åŠŸèƒ½ã€‚è¯·å®‰è£…: pip install httpx")
        
        if not self.async_client:
            self._init_async_client()
            if not self.async_client:
                raise RuntimeError("å¼‚æ­¥å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
        
        start_time = time.time()
        
        # å‡†å¤‡æ¶ˆæ¯æ ¼å¼
        prepared_messages = self._prepare_messages(messages)
        
        # å‚æ•°å¤„ç†
        temperature = temperature or self.config.temperature
        max_tokens = max_tokens or self.config.max_tokens
        model = kwargs.get('model') or getattr(self.config, 'model', None) or getattr(self.config, 'model_name', 'deepseek-chat')
        enable_cache = kwargs.get('enable_cache', self.config.enable_cache)
        
        # è½¬æ¢ä¸ºDeepSeek APIæ ¼å¼
        api_messages = []
        for msg in prepared_messages:
            api_messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        request_data = {
            'model': model,
            'messages': api_messages,
            'temperature': temperature,
            'max_tokens': max_tokens
        }
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(request_data)
        if enable_cache and self._is_cache_valid(cache_key):
            cached_api_response, _ = self._cache[cache_key]
            self.metrics.cache_hits += 1
            logger.debug(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜å“åº”: {cache_key[:16]}...")
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            llm_response = cached_api_response.to_llm_response("deepseek")
            self._update_stats(llm_response)
            return llm_response
        
        # ğŸš€ æ‰§è¡Œå¼‚æ­¥APIè°ƒç”¨
        api_response = await self._aexecute_request(request_data, start_time)
        
        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        llm_response = api_response.to_llm_response("deepseek")
        
        # æ›´æ–°ç¼“å­˜
        if enable_cache and api_response.success:
            self._cache[cache_key] = (api_response, time.time())
            self._cleanup_cache()
        
        # æ›´æ–°æŒ‡æ ‡
        enable_metrics = getattr(self.config, 'enable_metrics', True)
        if enable_metrics and self.metrics:
            self._update_metrics(api_response)
        
        # æ›´æ–°çˆ¶ç±»ç»Ÿè®¡
        self._update_stats(llm_response)
        
        return llm_response
    
    def simple_chat(
        self,
        prompt: str,
        system_message: Optional[str] = None,
        **kwargs
    ) -> APIResponse:
        """
        ç®€åŒ–çš„èŠå¤©æ¥å£
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_message: ç³»ç»Ÿæ¶ˆæ¯
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            APIå“åº”å¯¹è±¡
        """
        messages = [{"role": "user", "content": prompt}]
        return self.chat_completion(
            messages=messages,
            system_message=system_message,
            **kwargs
        )
    
    def _execute_request(self, request_data: Dict[str, Any], start_time: float) -> APIResponse:
        """
        æ‰§è¡ŒAPIè¯·æ±‚ï¼ˆåŒ…å«é‡è¯•é€»è¾‘ï¼‰
        
        Args:
            request_data: è¯·æ±‚æ•°æ®
            start_time: å¼€å§‹æ—¶é—´
            
        Returns:
            APIå“åº”å¯¹è±¡
        """
        # ğŸ”§ è¯·æ±‚é¢‘ç‡æ§åˆ¶ - ç¡®ä¿ä¸¤æ¬¡è¯·æ±‚ä¹‹é—´æœ‰è¶³å¤Ÿé—´éš”
        current_time = time.time()
        time_since_last = current_time - self._last_request_time
        
        if time_since_last < self._request_interval:
            wait_time = self._request_interval - time_since_last
            logger.debug(f"â±ï¸ è¯·æ±‚é—´éš”æ§åˆ¶ï¼Œç­‰å¾… {wait_time:.1f}s...")
            time.sleep(wait_time)
        
        self._last_request_time = time.time()
        
        last_error = None
        
        for attempt in range(self.config.max_retries):
            try:
                logger.debug(f"ğŸ¤– APIè°ƒç”¨å°è¯• {attempt + 1}/{self.config.max_retries}")
                
                response = self.session.post(
                    f"{self.config.base_url}/chat/completions",
                    json=request_data,
                    timeout=self.config.timeout
                )
                
                response_time = time.time() - start_time
                
                # å¤„ç†æˆåŠŸå“åº”
                if response.status_code == 200:
                    return self._process_success_response(response, response_time)
                
                # å¤„ç†é”™è¯¯å“åº”
                error_response = self._process_error_response(response, response_time)
                
                # å†³å®šæ˜¯å¦é‡è¯•
                if not self._should_retry(error_response.error_type, attempt):
                    return error_response
                
                # è®¡ç®—ç­‰å¾…æ—¶é—´å¹¶é‡è¯•
                wait_time = self._calculate_retry_delay(error_response.error_type, attempt)
                logger.warning(f"ğŸ”„ ç­‰å¾… {wait_time:.1f}s åé‡è¯•...")
                time.sleep(wait_time)
                last_error = error_response
                
            except requests.exceptions.Timeout as e:
                response_time = time.time() - start_time
                last_error = APIResponse(
                    success=False,
                    error_type=LLMErrorType.TIMEOUT_ERROR,
                    error_message=f"è¯·æ±‚è¶…æ—¶: {str(e)}",
                    response_time=response_time
                )
                
                if attempt < self.config.max_retries - 1:
                    wait_time = 5 * (attempt + 1)
                    logger.warning(f"â±ï¸ è¶…æ—¶é‡è¯•ï¼Œç­‰å¾… {wait_time}s...")
                    time.sleep(wait_time)
                
            except requests.exceptions.ConnectionError as e:
                response_time = time.time() - start_time
                last_error = APIResponse(
                    success=False,
                    error_type=APIErrorType.NETWORK_ERROR,
                    error_message=f"ç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}",
                    response_time=response_time
                )
                
                if attempt < self.config.max_retries - 1:
                    wait_time = 10 * (attempt + 1)
                    logger.warning(f"ğŸŒ ç½‘ç»œé”™è¯¯é‡è¯•ï¼Œç­‰å¾… {wait_time}s...")
                    time.sleep(wait_time)
                    
            except Exception as e:
                response_time = time.time() - start_time
                last_error = APIResponse(
                    success=False,
                    error_type=LLMErrorType.UNKNOWN_ERROR,
                    error_message=f"æœªçŸ¥é”™è¯¯: {str(e)}",
                    response_time=response_time
                )
                
                if attempt < self.config.max_retries - 1:
                    logger.warning(f"âŒ æœªçŸ¥é”™è¯¯é‡è¯•ï¼Œç­‰å¾… 3s...")
                    time.sleep(3)
        
        # æ‰€æœ‰é‡è¯•å¤±è´¥
        logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥: æ‰€æœ‰ {self.config.max_retries} æ¬¡é‡è¯•å‡å¤±è´¥")
        return last_error or APIResponse(
            success=False,
            error_type=APIErrorType.UNKNOWN_ERROR,
            error_message="æ‰€æœ‰é‡è¯•å°è¯•å‡å¤±è´¥"
        )
    
    def _process_success_response(self, response: requests.Response, response_time: float) -> APIResponse:
        """å¤„ç†æˆåŠŸå“åº”"""
        try:
            data = response.json()
            content = data['choices'][0]['message']['content']
            
            # æå–tokenä½¿ç”¨ä¿¡æ¯
            tokens_used = 0
            if 'usage' in data:
                tokens_used = data['usage'].get('total_tokens', 0)
            
            logger.info(f"âœ… APIè°ƒç”¨æˆåŠŸ ({response_time:.2f}s, {tokens_used} tokens)")
            
            return APIResponse(
                success=True,
                content=content,
                raw_response=data,
                status_code=response.status_code,
                response_time=response_time,
                tokens_used=tokens_used,
                model_used=data.get('model', getattr(self.config, 'model', None) or getattr(self.config, 'model_name', 'deepseek-chat'))
            )
            
        except (KeyError, json.JSONDecodeError) as e:
            logger.error(f"âŒ å“åº”è§£æå¤±è´¥: {str(e)}")
            return APIResponse(
                success=False,
                error_type=LLMErrorType.PARSE_ERROR,
                error_message=f"å“åº”è§£æå¤±è´¥: {str(e)}",
                status_code=response.status_code,
                response_time=response_time
            )
    
    def _process_error_response(self, response: requests.Response, response_time: float) -> APIResponse:
        """å¤„ç†é”™è¯¯å“åº”"""
        error_type = LLMErrorType.UNKNOWN_ERROR
        error_message = f"HTTP {response.status_code}"
        
        # æ ¹æ®çŠ¶æ€ç åˆ†ç±»é”™è¯¯
        if response.status_code == 401:
            error_type = LLMErrorType.AUTHENTICATION
            error_message = "APIå¯†é’¥è®¤è¯å¤±è´¥"
        elif response.status_code == 429:
            error_type = LLMErrorType.RATE_LIMIT
            error_message = "APIè°ƒç”¨é¢‘ç‡é™åˆ¶"
        elif response.status_code in [500, 502, 503, 504]:
            error_type = LLMErrorType.SERVER_ERROR
            error_message = f"æœåŠ¡å™¨é”™è¯¯ {response.status_code}"
        elif response.status_code == 400:
            error_type = LLMErrorType.INVALID_REQUEST
            error_message = "è¯·æ±‚å‚æ•°æ— æ•ˆ"
        
        # å°è¯•æå–è¯¦ç»†é”™è¯¯ä¿¡æ¯
        try:
            error_data = response.json()
            if 'error' in error_data:
                error_message = error_data['error'].get('message', error_message)
        except:
            pass
        
        logger.error(f"âŒ APIé”™è¯¯: {error_message} ({response.status_code})")
        
        return APIResponse(
            success=False,
            error_type=error_type,
            error_message=error_message,
            status_code=response.status_code,
            response_time=response_time
        )
    
    def _should_retry(self, error_type: LLMErrorType, attempt: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        if attempt >= self.config.max_retries - 1:
            return False
        
        # ä¸é‡è¯•çš„é”™è¯¯ç±»å‹
        non_retryable = {
            LLMErrorType.AUTHENTICATION,
            LLMErrorType.PARSE_ERROR,
            LLMErrorType.INVALID_REQUEST
        }
        
        return error_type not in non_retryable
    
    def _calculate_retry_delay(self, error_type: LLMErrorType, attempt: int) -> float:
        """è®¡ç®—é‡è¯•å»¶è¿Ÿæ—¶é—´"""
        base_delay = self.config.retry_delay_base
        
        if error_type == LLMErrorType.RATE_LIMIT:
            # é™æµé”™è¯¯ä½¿ç”¨æŒ‡æ•°é€€é¿
            return base_delay ** (attempt + 1) * 2
        elif error_type == LLMErrorType.SERVER_ERROR:
            # æœåŠ¡å™¨é”™è¯¯ä½¿ç”¨çº¿æ€§å¢é•¿
            return 5 * (attempt + 1)
        else:
            # å…¶ä»–é”™è¯¯ä½¿ç”¨åŸºç¡€å»¶è¿Ÿ
            return base_delay * (attempt + 1)
    
    def _generate_cache_key(self, request_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # å°†è¯·æ±‚æ•°æ®åºåˆ—åŒ–å¹¶ç”Ÿæˆå“ˆå¸Œ
        cache_string = json.dumps(request_data, sort_keys=True)
        return hashlib.md5(cache_string.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key not in self._cache:
            return False
        
        _, timestamp = self._cache[cache_key]
        return time.time() - timestamp < self.config.cache_ttl
    
    def _cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self._cache.items()
            if current_time - timestamp > self.config.cache_ttl
        ]
        
        for key in expired_keys:
            del self._cache[key]
        
        if expired_keys:
            logger.debug(f"ğŸ§¹ æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜é¡¹")
    
    def _update_metrics(self, response: APIResponse):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        if not self.metrics:
            return
            
        self.metrics.total_requests += 1
        
        if response.success:
            self.metrics.successful_requests += 1
            self.metrics.total_response_time += response.response_time
            self.metrics.total_tokens_used += response.tokens_used
        else:
            self.metrics.failed_requests += 1
            if response.error_type:
                self.metrics.error_counts[response.error_type] = \
                    self.metrics.error_counts.get(response.error_type, 0) + 1
    
    def get_metrics(self) -> Optional[ClientMetrics]:
        """è·å–å®¢æˆ·ç«¯æ€§èƒ½æŒ‡æ ‡"""
        return self.metrics
    
    def reset_metrics(self):
        """é‡ç½®æ€§èƒ½æŒ‡æ ‡"""
        if self.metrics:
            self.metrics = ClientMetrics()
            logger.info("ğŸ“Š æ€§èƒ½æŒ‡æ ‡å·²é‡ç½®")
        else:
            logger.debug("ğŸ“Š æ€§èƒ½æŒ‡æ ‡åŠŸèƒ½æœªå¯ç”¨")
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        logger.info("ğŸ§¹ ç¼“å­˜å·²æ¸…ç©º")
    
    @contextmanager
    def batch_mode(self):
        """æ‰¹é‡æ¨¡å¼ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆå¯ä»¥æ·»åŠ æ‰¹é‡ä¼˜åŒ–é€»è¾‘ï¼‰"""
        logger.debug("ğŸ”„ è¿›å…¥æ‰¹é‡æ¨¡å¼")
        try:
            yield self
        finally:
            logger.debug("âœ… é€€å‡ºæ‰¹é‡æ¨¡å¼")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """æ¸…ç†èµ„æº"""
        self.session.close()
        logger.debug("ğŸ”„ DeepSeekClient èµ„æºå·²æ¸…ç†")
    
    # ==================== å®ç°BaseLLMClientæŠ½è±¡æ–¹æ³• ====================
    
    def validate_config(self) -> bool:
        """
        éªŒè¯é…ç½®æ˜¯å¦æœ‰æ•ˆ
        
        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            if not self.config.api_key:
                return False
            
            # ç®€å•æµ‹è¯•APIè¿é€šæ€§
            test_response = self.simple_chat("test", system_message="Reply with 'ok'")
            return test_response.success
            
        except Exception as e:
            logger.error(f"âŒ DeepSeeké…ç½®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Returns:
            List[str]: å¯ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
        """
        # DeepSeekç›®å‰æ”¯æŒçš„æ¨¡å‹
        return [
            "deepseek-chat",
            "deepseek-coder"
        ]
    
    def get_supported_features(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨
        
        Returns:
            List[str]: æ”¯æŒçš„åŠŸèƒ½
        """
        return [
            "chat_completion", 
            "achat_completion",  # ğŸš€ æ–°å¢å¼‚æ­¥æ”¯æŒ
            "text_generation", 
            "chinese_language",
            "coding_assistance",
            "caching",
            "retry_mechanism"
        ]
    
    # ==================== ğŸš€ å¼‚æ­¥è¯·æ±‚æ‰§è¡Œæ–¹æ³• ====================
    
    async def _aexecute_request(self, request_data: Dict[str, Any], start_time: float) -> APIResponse:
        """
        ğŸš€ å¼‚æ­¥æ‰§è¡ŒAPIè¯·æ±‚
        
        Args:
            request_data: è¯·æ±‚æ•°æ®
            start_time: å¼€å§‹æ—¶é—´
            
        Returns:
            APIResponse: APIå“åº”å¯¹è±¡
        """
        if not self.async_client:
            raise RuntimeError("å¼‚æ­¥å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        # ğŸ”§ é¢‘ç‡æ§åˆ¶ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        current_time = time.time()
        time_since_last = current_time - self._last_request_time
        if time_since_last < self._request_interval:
            wait_time = self._request_interval - time_since_last
            logger.debug(f"â±ï¸ å¼‚æ­¥é¢‘ç‡æ§åˆ¶ï¼Œç­‰å¾… {wait_time:.1f}s")
            await asyncio.sleep(wait_time)
        
        self._last_request_time = time.time()
        
        last_error = None
        
        for attempt in range(self.config.max_retries):
            try:
                logger.debug(f"ğŸš€ å¼‚æ­¥APIè°ƒç”¨å°è¯• {attempt + 1}/{self.config.max_retries}")
                
                response = await self.async_client.post(
                    f"{self.config.base_url}/chat/completions",
                    json=request_data,
                    timeout=self.config.timeout
                )
                
                response_time = time.time() - start_time
                
                # å¤„ç†æˆåŠŸå“åº”
                if response.status_code == 200:
                    return self._aprocess_success_response(response, response_time)
                
                # å¤„ç†é”™è¯¯å“åº”
                error_response = self._aprocess_error_response(response, response_time)
                
                # å†³å®šæ˜¯å¦é‡è¯•
                if not self._should_retry(error_response.error_type, attempt):
                    return error_response
                
                # è®¡ç®—ç­‰å¾…æ—¶é—´å¹¶é‡è¯•
                wait_time = self._calculate_retry_delay(error_response.error_type, attempt)
                logger.warning(f"ğŸ”„ å¼‚æ­¥ç­‰å¾… {wait_time:.1f}s åé‡è¯•...")
                await asyncio.sleep(wait_time)
                last_error = error_response
                
            except Exception as e:
                # ç®€åŒ–å¼‚å¸¸å¤„ç†ï¼Œå…¼å®¹httpxä¸å¯ç”¨çš„æƒ…å†µ
                response_time = time.time() - start_time
                error_type = LLMErrorType.TIMEOUT_ERROR if 'timeout' in str(e).lower() else LLMErrorType.NETWORK_ERROR
                last_error = APIResponse(
                    success=False,
                    error_type=error_type,
                    error_message=f"å¼‚æ­¥è¯·æ±‚é”™è¯¯: {str(e)}",
                    response_time=response_time
                )
                
                if attempt < self.config.max_retries - 1:
                    wait_time = 3 * (attempt + 1)
                    logger.warning(f"ğŸ”„ å¼‚æ­¥é”™è¯¯é‡è¯•ï¼Œç­‰å¾… {wait_time}s...")
                    await asyncio.sleep(wait_time)
                else:
                    break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        if self.metrics:
            self.metrics.total_failures += 1
        return last_error or APIResponse(
            success=False,
            error_type=LLMErrorType.UNKNOWN_ERROR,
            error_message="å¼‚æ­¥APIè°ƒç”¨å¤±è´¥ï¼šæ‰€æœ‰é‡è¯•éƒ½å·²ç”¨å°½",
            response_time=time.time() - start_time
        )
    
    def _aprocess_success_response(self, response, response_time: float) -> APIResponse:
        """ğŸš€ å¤„ç†å¼‚æ­¥æˆåŠŸå“åº”"""
        try:
            response_data = response.json()
            
            # æå–æ¶ˆæ¯å†…å®¹
            content = ""
            if 'choices' in response_data and response_data['choices']:
                choice = response_data['choices'][0]
                if 'message' in choice:
                    content = choice['message'].get('content', '')
                elif 'text' in choice:
                    content = choice.get('text', '')
            
            # æ›´æ–°æŒ‡æ ‡
            if self.metrics:
                self.metrics.total_requests += 1
                self.metrics.successful_requests += 1
                self.metrics.total_response_time += response_time
                
                # ç»Ÿè®¡tokenä½¿ç”¨
                if 'usage' in response_data:
                    usage = response_data['usage']
                    self.metrics.total_tokens += usage.get('total_tokens', 0)
                    self.metrics.prompt_tokens += usage.get('prompt_tokens', 0)
                    self.metrics.completion_tokens += usage.get('completion_tokens', 0)
            
            logger.debug(f"ğŸš€ å¼‚æ­¥APIè°ƒç”¨æˆåŠŸï¼Œå“åº”æ—¶é—´: {response_time:.2f}s")
            
            return APIResponse(
                success=True,
                content=content,
                raw_response=response_data,
                status_code=response.status_code,
                response_time=response_time
            )
            
        except Exception as e:
            logger.error(f"âŒ å¼‚æ­¥å“åº”è§£æå¤±è´¥: {e}")
            return APIResponse(
                success=False,
                error_type=LLMErrorType.PARSE_ERROR,
                error_message=f"å¼‚æ­¥å“åº”è§£æå¤±è´¥: {str(e)}",
                status_code=getattr(response, 'status_code', 0),
                response_time=response_time
            )
    
    def _aprocess_error_response(self, response, response_time: float) -> APIResponse:
        """ğŸš€ å¤„ç†å¼‚æ­¥é”™è¯¯å“åº”"""
        try:
            error_data = response.json()
            error_message = error_data.get('error', {}).get('message', f'HTTP {response.status_code}')
        except:
            error_message = f"HTTP {response.status_code}: å“åº”è§£æå¤±è´¥"
        
        # é”™è¯¯ç±»å‹æ˜ å°„
        error_type = LLMErrorType.SERVER_ERROR
        if response.status_code == 401:
            error_type = LLMErrorType.AUTHENTICATION
        elif response.status_code == 429:
            error_type = LLMErrorType.RATE_LIMIT
        elif response.status_code == 403:
            error_type = LLMErrorType.QUOTA_EXCEEDED
        elif response.status_code == 400:
            error_type = LLMErrorType.INVALID_REQUEST
        
        # æ›´æ–°æŒ‡æ ‡
        if self.metrics:
            self.metrics.total_requests += 1
            self.metrics.failed_requests += 1
            self.metrics.error_counts[error_type.value] = self.metrics.error_counts.get(error_type.value, 0) + 1
        
        logger.warning(f"âš ï¸ å¼‚æ­¥APIé”™è¯¯ {response.status_code}: {error_message}")
        
        return APIResponse(
            success=False,
            error_type=error_type,
            error_message=error_message,
            status_code=response.status_code,
            response_time=response_time
        )
    
    async def aclose(self):
        """ğŸš€ å…³é—­å¼‚æ­¥å®¢æˆ·ç«¯"""
        if self.async_client:
            await self.async_client.aclose()
            self.async_client = None
            logger.debug("ğŸš€ å¼‚æ­¥HTTPå®¢æˆ·ç«¯å·²å…³é—­")


# å·¥å‚å‡½æ•°å’Œä¾¿æ·æ¥å£
def create_client(api_key: str, **kwargs) -> DeepSeekClient:
    """
    åˆ›å»º DeepSeek å®¢æˆ·ç«¯çš„å·¥å‚å‡½æ•° - å‘åå…¼å®¹
    
    Args:
        api_key: APIå¯†é’¥
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        DeepSeekClient å®ä¾‹
    """
    config = ClientConfig(api_key=api_key, **kwargs)
    return DeepSeekClient(config)


def create_llm_client(api_key: str, **kwargs) -> DeepSeekClient:
    """
    åˆ›å»ºç»Ÿä¸€LLMå®¢æˆ·ç«¯çš„å·¥å‚å‡½æ•°
    
    Args:
        api_key: APIå¯†é’¥
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        DeepSeekClient å®ä¾‹ï¼ˆå®ç°BaseLLMClientæ¥å£ï¼‰
    """
    llm_config = LLMConfig(
        provider=LLMProvider.DEEPSEEK,
        api_key=api_key,
        model_name=kwargs.pop('model_name', 'deepseek-chat'),
        **kwargs
    )
    return DeepSeekClient(llm_config)


def quick_chat(api_key: str, prompt: str, system_message: Optional[str] = None) -> str:
    """
    å¿«é€ŸèŠå¤©ä¾¿æ·å‡½æ•°
    
    Args:
        api_key: APIå¯†é’¥
        prompt: ç”¨æˆ·æç¤º
        system_message: ç³»ç»Ÿæ¶ˆæ¯
        
    Returns:
        AIå“åº”å†…å®¹
        
    Raises:
        Exception: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    with create_client(api_key) as client:
        response = client.simple_chat(prompt, system_message)
        if response.success:
            return response.content
        else:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.error_message}")

       
