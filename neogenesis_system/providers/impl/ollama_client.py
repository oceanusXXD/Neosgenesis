#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ollamaå®¢æˆ·ç«¯å®ç° - ç»Ÿä¸€LLMæ¥å£
Ollama Client Implementation - Unified LLM Interface for local models
"""

import time
import logging
import requests
import json
import asyncio
import aiohttp
from typing import List, Optional, Union, Dict, Any

from ..llm_base import (
    BaseLLMClient, LLMConfig, LLMResponse, LLMMessage, LLMUsage, 
    LLMProvider, LLMErrorType, create_error_response
)

logger = logging.getLogger(__name__)


class OllamaClient(BaseLLMClient):
    """
    Ollamaå®¢æˆ·ç«¯ - å®ç°ç»Ÿä¸€LLMæ¥å£
    
    æ”¯æŒ:
    - æœ¬åœ°è¿è¡Œçš„å¼€æºæ¨¡å‹
    - Llamaã€Mistralã€CodeLlamaç­‰
    - å¿«é€Ÿæœ¬åœ°æ¨ç†
    - ç¦»çº¿ä½¿ç”¨
    """
    
    def __init__(self, config: LLMConfig):
        """
        åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯
        
        Args:
            config: LLMé…ç½®å¯¹è±¡
        """
        super().__init__(config)
        
        # è®¾ç½®OllamaæœåŠ¡å™¨åœ°å€
        self.base_url = config.base_url or "http://localhost:11434"
        self.session = requests.Session()
        
        # è®¾ç½®è¶…æ—¶
        self.timeout = config.timeout[1] if isinstance(config.timeout, tuple) else config.timeout
        
        logger.info(f"ğŸ¤– Ollamaå®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {config.model_name} @ {self.base_url}")
    
    def chat_completion(self, 
                       messages: Union[str, List[LLMMessage]], 
                       temperature: Optional[float] = None,
                       max_tokens: Optional[int] = None,
                       **kwargs) -> LLMResponse:
        """
        OllamaèŠå¤©å®Œæˆæ¥å£å®ç°
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: ç»Ÿä¸€çš„å“åº”å¯¹è±¡
        """
        start_time = time.time()
        
        try:
            # å‡†å¤‡æ¶ˆæ¯æ ¼å¼
            prepared_messages = self._prepare_messages(messages)
            
            # è½¬æ¢ä¸ºOllama APIæ ¼å¼
            ollama_messages = []
            for msg in prepared_messages:
                ollama_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_data = {
                "model": kwargs.get('model') or self.config.model_name,
                "messages": ollama_messages,
                "stream": False,  # ä¸ä½¿ç”¨æµå¼å“åº”
                "options": {
                    "temperature": temperature or self.config.temperature,
                    "num_predict": max_tokens or self.config.max_tokens
                }
            }
            
            # è°ƒç”¨Ollama API
            logger.debug(f"ğŸ¤– è°ƒç”¨Ollama API: {request_data['model']}")
            response = self.session.post(
                f"{self.base_url}/api/chat",
                json=request_data,
                timeout=self.timeout
            )
            
            response_time = time.time() - start_time
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                return self._handle_error_response(response, response_time)
            
            # è§£æå“åº”
            response_data = response.json()
            content = response_data.get("message", {}).get("content", "")
            
            # æ„å»ºä½¿ç”¨ç»Ÿè®¡ï¼ˆOllamaé€šå¸¸ä¸è¿”å›è¯¦ç»†çš„tokenç»Ÿè®¡ï¼‰
            usage = None
            if "usage" in response_data:
                usage_data = response_data["usage"]
                usage = LLMUsage(
                    prompt_tokens=usage_data.get("prompt_tokens", 0),
                    completion_tokens=usage_data.get("completion_tokens", 0),
                    total_tokens=usage_data.get("total_tokens", 0)
                )
            
            # æ„å»ºå“åº”å¯¹è±¡
            llm_response = LLMResponse(
                success=True,
                content=content,
                provider=self.provider.value,
                model=request_data["model"],
                response_time=response_time,
                usage=usage,
                finish_reason=response_data.get("done_reason"),
                raw_response=response_data
            )
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(llm_response)
            
            logger.info(f"âœ… Ollama APIè°ƒç”¨æˆåŠŸ: {content[:50]}...")
            return llm_response
            
        except requests.exceptions.ConnectionError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.NETWORK_ERROR,
                error_message=f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡å™¨: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ Ollamaè¿æ¥å¤±è´¥: {e}")
            return error_response
            
        except requests.exceptions.Timeout as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.TIMEOUT_ERROR,
                error_message=f"Ollamaè¯·æ±‚è¶…æ—¶: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ Ollamaè¯·æ±‚è¶…æ—¶: {e}")
            return error_response
            
        except json.JSONDecodeError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.PARSE_ERROR,
                error_message=f"Ollamaå“åº”è§£æå¤±è´¥: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ Ollamaå“åº”è§£æå¤±è´¥: {e}")
            return error_response
            
        except Exception as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.UNKNOWN_ERROR,
                error_message=f"OllamaæœªçŸ¥é”™è¯¯: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ OllamaæœªçŸ¥é”™è¯¯: {e}")
            return error_response
    
    async def achat_completion(self, 
                              messages: Union[str, List[LLMMessage]], 
                              temperature: Optional[float] = None,
                              max_tokens: Optional[int] = None,
                              **kwargs) -> LLMResponse:
        """
        Ollamaå¼‚æ­¥èŠå¤©å®Œæˆæ¥å£å®ç°
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: ç»Ÿä¸€çš„å“åº”å¯¹è±¡
        """
        start_time = time.time()
        
        try:
            # å‡†å¤‡æ¶ˆæ¯æ ¼å¼
            prepared_messages = self._prepare_messages(messages)
            
            # è½¬æ¢ä¸ºOllama APIæ ¼å¼
            ollama_messages = []
            for msg in prepared_messages:
                ollama_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_data = {
                "model": kwargs.get('model') or self.config.model_name,
                "messages": ollama_messages,
                "stream": False,  # ä¸ä½¿ç”¨æµå¼å“åº”
                "options": {
                    "temperature": temperature or self.config.temperature,
                    "num_predict": max_tokens or self.config.max_tokens
                }
            }
            
            # å¼‚æ­¥è°ƒç”¨Ollama API
            logger.debug(f"ğŸ¤– å¼‚æ­¥è°ƒç”¨Ollama API: {request_data['model']}")
            
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(
                    f"{self.base_url}/api/chat",
                    json=request_data
                ) as response:
                    response_time = time.time() - start_time
                    
                    # æ£€æŸ¥å“åº”çŠ¶æ€
                    if response.status != 200:
                        return await self._handle_async_error_response(response, response_time)
                    
                    # è§£æå“åº”
                    response_data = await response.json()
                    content = response_data.get("message", {}).get("content", "")
                    
                    # æ„å»ºä½¿ç”¨ç»Ÿè®¡ï¼ˆOllamaé€šå¸¸ä¸è¿”å›è¯¦ç»†çš„tokenç»Ÿè®¡ï¼‰
                    usage = None
                    if "usage" in response_data:
                        usage_data = response_data["usage"]
                        usage = LLMUsage(
                            prompt_tokens=usage_data.get("prompt_tokens", 0),
                            completion_tokens=usage_data.get("completion_tokens", 0),
                            total_tokens=usage_data.get("total_tokens", 0)
                        )
                    
                    # æ„å»ºå“åº”å¯¹è±¡
                    llm_response = LLMResponse(
                        success=True,
                        content=content,
                        provider=self.provider.value,
                        model=request_data["model"],
                        response_time=response_time,
                        usage=usage,
                        finish_reason=response_data.get("done_reason"),
                        raw_response=response_data
                    )
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self._update_stats(llm_response)
                    
                    logger.info(f"âœ… Ollamaå¼‚æ­¥APIè°ƒç”¨æˆåŠŸ: {content[:50]}...")
                    return llm_response
            
        except asyncio.TimeoutError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.TIMEOUT_ERROR,
                error_message=f"Ollamaå¼‚æ­¥è¯·æ±‚è¶…æ—¶: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ Ollamaå¼‚æ­¥è¯·æ±‚è¶…æ—¶: {e}")
            return error_response
            
        except aiohttp.ClientError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.NETWORK_ERROR,
                error_message=f"Ollamaå¼‚æ­¥ç½‘ç»œé”™è¯¯: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ Ollamaå¼‚æ­¥ç½‘ç»œé”™è¯¯: {e}")
            return error_response
            
        except Exception as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.UNKNOWN_ERROR,
                error_message=f"Ollamaå¼‚æ­¥æœªçŸ¥é”™è¯¯: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ Ollamaå¼‚æ­¥æœªçŸ¥é”™è¯¯: {e}")
            return error_response
    
    async def _handle_async_error_response(self, response: aiohttp.ClientResponse, response_time: float) -> LLMResponse:
        """å¤„ç†å¼‚æ­¥é”™è¯¯å“åº”"""
        error_type = LLMErrorType.UNKNOWN_ERROR
        
        if response.status == 404:
            error_type = LLMErrorType.MODEL_ERROR
            error_message = f"æ¨¡å‹æœªæ‰¾åˆ°: {self.config.model_name}"
        elif response.status == 400:
            error_type = LLMErrorType.INVALID_REQUEST
            error_message = "è¯·æ±‚å‚æ•°æ— æ•ˆ"
        elif response.status >= 500:
            error_type = LLMErrorType.SERVER_ERROR
            error_message = f"OllamaæœåŠ¡å™¨é”™è¯¯: {response.status}"
        else:
            error_message = f"HTTPé”™è¯¯: {response.status}"
        
        try:
            error_data = await response.json()
            if "error" in error_data:
                error_message = error_data["error"]
        except:
            pass
        
        return create_error_response(
            provider=self.provider.value,
            error_type=error_type,
            error_message=error_message,
            response_time=response_time
        )
    
    def _handle_error_response(self, response: requests.Response, response_time: float) -> LLMResponse:
        """å¤„ç†é”™è¯¯å“åº”"""
        error_type = LLMErrorType.UNKNOWN_ERROR
        
        if response.status_code == 404:
            error_type = LLMErrorType.MODEL_ERROR
            error_message = f"æ¨¡å‹æœªæ‰¾åˆ°: {self.config.model_name}"
        elif response.status_code == 400:
            error_type = LLMErrorType.INVALID_REQUEST
            error_message = "è¯·æ±‚å‚æ•°æ— æ•ˆ"
        elif response.status_code >= 500:
            error_type = LLMErrorType.SERVER_ERROR
            error_message = f"OllamaæœåŠ¡å™¨é”™è¯¯: {response.status_code}"
        else:
            error_message = f"HTTPé”™è¯¯: {response.status_code}"
        
        try:
            error_data = response.json()
            if "error" in error_data:
                error_message = error_data["error"]
        except:
            pass
        
        return create_error_response(
            provider=self.provider.value,
            error_type=error_type,
            error_message=error_message,
            response_time=response_time
        )
    
    def validate_config(self) -> bool:
        """
        éªŒè¯Ollamaé…ç½®æ˜¯å¦æœ‰æ•ˆ
        
        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # åŸºç¡€é…ç½®éªŒè¯ - ä¸è¿›è¡Œå®é™…ç½‘ç»œè°ƒç”¨
            if not hasattr(self.config, 'model_name') or not self.config.model_name:
                logger.debug("âŒ Ollamaæ¨¡å‹åç§°æœªè®¾ç½®")
                return False
            
            # æ£€æŸ¥base_urlæ ¼å¼
            base_url = getattr(self, 'base_url', None) or getattr(self.config, 'base_url', None)
            if base_url:
                if not (base_url.startswith('http://') or base_url.startswith('https://')):
                    logger.debug("âŒ Ollama base_urlæ ¼å¼æ— æ•ˆ")
                    return False
            
            logger.debug("âœ… OllamaåŸºç¡€é…ç½®éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ollamaé…ç½®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Returns:
            List[str]: å¯ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
        """
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return [model["name"] for model in data.get("models", [])]
            else:
                logger.error(f"âŒ è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"âŒ è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            # è¿”å›å¸¸è§çš„æ¨¡å‹åç§°
            return [
                "deepseek-r1:7b", "deepseek-r1:latest",    # DeepSeekç³»åˆ—ï¼ˆä¼˜å…ˆæ¨èï¼‰
                "llama2", "llama2:7b", "llama2:13b",
                "mistral", "mistral:7b",
                "codellama", "codellama:7b",
                "phi", "phi:2.7b",
                "neural-chat", "neural-chat:7b"
            ]
    
    def get_supported_features(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨
        
        Returns:
            List[str]: æ”¯æŒçš„åŠŸèƒ½
        """
        return [
            "chat_completion", 
            "text_generation",
            "local_inference",  # æœ¬åœ°æ¨ç†
            "offline_usage",    # ç¦»çº¿ä½¿ç”¨
            "open_source",      # å¼€æºæ¨¡å‹
            "customizable",     # å¯å®šåˆ¶
            "privacy_focused"   # éšç§ä¿æŠ¤
        ]


def create_ollama_client(model_name: str = "deepseek-r1:7b", base_url: str = "http://localhost:11434", **kwargs) -> OllamaClient:
    """
    åˆ›å»ºOllamaå®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_name: æ¨¡å‹åç§°
        base_url: OllamaæœåŠ¡å™¨åœ°å€
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        OllamaClient: å®¢æˆ·ç«¯å®ä¾‹
    """
    config = LLMConfig(
        provider=LLMProvider.OLLAMA,
        api_key="",  # Ollamaé€šå¸¸ä¸éœ€è¦APIå¯†é’¥
        model_name=model_name,
        base_url=base_url,
        **kwargs
    )
    return OllamaClient(config)
