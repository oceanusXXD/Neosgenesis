#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Anthropic Claudeå®¢æˆ·ç«¯å®ç° - ç»Ÿä¸€LLMæ¥å£
Anthropic Claude Client Implementation - Unified LLM Interface
"""

import time
import logging
from typing import List, Optional, Union, Dict, Any

try:
    import anthropic
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    
from ..llm_base import (
    BaseLLMClient, LLMConfig, LLMResponse, LLMMessage, LLMUsage, 
    LLMProvider, LLMErrorType, create_error_response
)

logger = logging.getLogger(__name__)


class AnthropicClient(BaseLLMClient):
    """
    Anthropic Claudeå®¢æˆ·ç«¯ - å®ç°ç»Ÿä¸€LLMæ¥å£
    
    æ”¯æŒ:
    - Claude 3ç³»åˆ—æ¨¡å‹
    - é•¿ä¸Šä¸‹æ–‡å¤„ç†
    - é«˜è´¨é‡æ¨ç†èƒ½åŠ›
    - å®Œæ•´çš„é”™è¯¯å¤„ç†
    """
    
    def __init__(self, config: LLMConfig):
        """
        åˆå§‹åŒ–Anthropicå®¢æˆ·ç«¯
        
        Args:
            config: LLMé…ç½®å¯¹è±¡
        """
        if not ANTHROPIC_AVAILABLE:
            raise ImportError("Anthropicåº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install anthropic")
        
        super().__init__(config)
        
        # åˆ›å»ºAnthropicå®¢æˆ·ç«¯
        self.client = Anthropic(
            api_key=config.api_key,
            base_url=config.base_url,
            timeout=config.timeout[1],  # ä½¿ç”¨è¯»å–è¶…æ—¶
            max_retries=config.max_retries
        )
        
        logger.info(f"ğŸ¤– Anthropicå®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {config.model_name}")
    
    def chat_completion(self, 
                       messages: Union[str, List[LLMMessage]], 
                       temperature: Optional[float] = None,
                       max_tokens: Optional[int] = None,
                       **kwargs) -> LLMResponse:
        """
        AnthropicèŠå¤©å®Œæˆæ¥å£å®ç°
        
        Args:
            messages: æ¶ˆæ¯å†…å®¹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            LLMResponse: ç»Ÿä¸€çš„å“åº”å¯¹è±¡
        """
        start_time = time.time()
        
        try:
            # å‡†å¤‡æ¶ˆæ¯æ ¼å¼
            prepared_messages = self._prepare_messages(messages)
            
            # åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå¯¹è¯æ¶ˆæ¯
            system_message = ""
            dialogue_messages = []
            
            for msg in prepared_messages:
                if msg.role == "system":
                    system_message = msg.content
                else:
                    dialogue_messages.append({
                        "role": msg.role,
                        "content": msg.content
                    })
            
            # å¦‚æœæ²¡æœ‰å¯¹è¯æ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
            if not dialogue_messages:
                if isinstance(messages, str):
                    dialogue_messages = [{"role": "user", "content": messages}]
                else:
                    dialogue_messages = [{"role": "user", "content": "Hello"}]
            
            # å‚æ•°å¤„ç†
            params = {
                "model": kwargs.get('model') or self.config.model_name,
                "messages": dialogue_messages,
                "max_tokens": max_tokens or self.config.max_tokens,
                "temperature": temperature or self.config.temperature
            }
            
            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if system_message:
                params["system"] = system_message
            
            # è°ƒç”¨Anthropic API
            logger.debug(f"ğŸ¤– è°ƒç”¨Anthropic API: {params['model']}")
            response = self.client.messages.create(**params)
            
            response_time = time.time() - start_time
            
            # æå–å“åº”å†…å®¹
            content = ""
            if response.content and len(response.content) > 0:
                content = response.content[0].text
            
            finish_reason = response.stop_reason
            
            # æ„å»ºä½¿ç”¨ç»Ÿè®¡
            usage = None
            if response.usage:
                usage = LLMUsage(
                    prompt_tokens=response.usage.input_tokens,
                    completion_tokens=response.usage.output_tokens,
                    total_tokens=response.usage.input_tokens + response.usage.output_tokens
                )
            
            # æ„å»ºå“åº”å¯¹è±¡
            llm_response = LLMResponse(
                success=True,
                content=content,
                provider=self.provider.value,
                model=response.model,
                response_time=response_time,
                usage=usage,
                finish_reason=finish_reason,
                raw_response=response.model_dump() if hasattr(response, 'model_dump') else None
            )
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(llm_response)
            
            logger.info(f"âœ… Anthropic APIè°ƒç”¨æˆåŠŸ: {content[:50]}...")
            return llm_response
            
        except anthropic.AuthenticationError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.AUTHENTICATION,
                error_message=f"Anthropicè®¤è¯å¤±è´¥: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ Anthropicè®¤è¯å¤±è´¥: {e}")
            return error_response
            
        except anthropic.RateLimitError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.RATE_LIMIT,
                error_message=f"Anthropicé€Ÿç‡é™åˆ¶: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ Anthropicé€Ÿç‡é™åˆ¶: {e}")
            return error_response
            
        except anthropic.BadRequestError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.INVALID_REQUEST,
                error_message=f"Anthropicè¯·æ±‚æ— æ•ˆ: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ Anthropicè¯·æ±‚æ— æ•ˆ: {e}")
            return error_response
            
        except anthropic.InternalServerError as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.SERVER_ERROR,
                error_message=f"AnthropicæœåŠ¡å™¨é”™è¯¯: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ AnthropicæœåŠ¡å™¨é”™è¯¯: {e}")
            return error_response
            
        except Exception as e:
            response_time = time.time() - start_time
            error_response = create_error_response(
                provider=self.provider.value,
                error_type=LLMErrorType.UNKNOWN_ERROR,
                error_message=f"AnthropicæœªçŸ¥é”™è¯¯: {str(e)}",
                response_time=response_time
            )
            self._update_stats(error_response)
            logger.error(f"âŒ AnthropicæœªçŸ¥é”™è¯¯: {e}")
            return error_response
    
    def validate_config(self) -> bool:
        """
        éªŒè¯Anthropicé…ç½®æ˜¯å¦æœ‰æ•ˆ
        
        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # ç®€å•æµ‹è¯•APIè¿é€šæ€§
            response = self.chat_completion(
                messages="Hello",
                max_tokens=5
            )
            return response.success
            
        except Exception as e:
            logger.error(f"âŒ Anthropicé…ç½®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Returns:
            List[str]: å¯ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
        """
        # Anthropicç›®å‰æ”¯æŒçš„æ¨¡å‹
        return [
            "claude-3-opus-20240229",
            "claude-3-sonnet-20240229", 
            "claude-3-haiku-20240307",
            "claude-3-5-sonnet-20241022",
            "claude-3-5-haiku-20241022"
        ]
    
    def get_supported_features(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„åŠŸèƒ½åˆ—è¡¨
        
        Returns:
            List[str]: æ”¯æŒçš„åŠŸèƒ½
        """
        return [
            "chat_completion", 
            "text_generation",
            "long_context",  # æ”¯æŒé•¿ä¸Šä¸‹æ–‡
            "reasoning",     # å¼ºæ¨ç†èƒ½åŠ›
            "analysis",      # åˆ†æèƒ½åŠ›
            "creative_writing",  # åˆ›æ„å†™ä½œ
            "code_analysis", # ä»£ç åˆ†æ
            "system_messages"
        ]


def create_anthropic_client(api_key: str, model_name: str = "claude-3-sonnet-20240229", **kwargs) -> AnthropicClient:
    """
    åˆ›å»ºAnthropicå®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        api_key: APIå¯†é’¥
        model_name: æ¨¡å‹åç§°
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        AnthropicClient: å®¢æˆ·ç«¯å®ä¾‹
    """
    config = LLMConfig(
        provider=LLMProvider.ANTHROPIC,
        api_key=api_key,
        model_name=model_name,
        **kwargs
    )
    return AnthropicClient(config)