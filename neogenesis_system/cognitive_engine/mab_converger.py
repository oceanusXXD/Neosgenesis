#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
MABæ”¶æ•›å™¨ - é˜¶æ®µä¸‰ï¼šæ€ç»´è·¯å¾„é€‰æ‹©å™¨
è´Ÿè´£ä»å¤šä¸ªæ€ç»´è·¯å¾„ä¸­é€‰æ‹©æœ€ä¼˜è·¯å¾„çš„å¤šè‡‚è€è™æœºç®—æ³•
MAB Converger - Stage 3: Reasoning Path Selector
Responsible for selecting optimal reasoning path from multiple paths using MAB algorithms
"""

import time
import logging
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
from dataclasses import dataclass

from .data_structures import EnhancedDecisionArm, ReasoningPath
try:
    from neogenesis_system.config import MAB_CONFIG
except ImportError:
    try:
        from ..config import MAB_CONFIG
    except ImportError:
        MAB_CONFIG = {
            "convergence_threshold": 0.95,
            "min_samples": 10
        }

logger = logging.getLogger(__name__)


@dataclass
class MABConverger:
    """MABæ”¶æ•›å™¨ - é˜¶æ®µä¸‰ï¼šæ€ç»´è·¯å¾„é€‰æ‹©å™¨"""
    
    def __init__(self):
        # æ”¹ä¸ºå­˜å‚¨è·¯å¾„çº§åˆ«çš„å†³ç­–è‡‚ï¼špath_id -> EnhancedDecisionArm
        self.path_arms: Dict[str, EnhancedDecisionArm] = {}
        self.convergence_threshold = MAB_CONFIG["convergence_threshold"]  # æ”¶æ•›é˜ˆå€¼
        self.min_samples = MAB_CONFIG["min_samples"]  # æœ€å°æ ·æœ¬æ•°
        
        # ğŸ”§ æ–°å¢ï¼šå·¥å…·çº§åˆ«çš„å†³ç­–è‡‚å­˜å‚¨ï¼štool_id -> EnhancedDecisionArm
        self.tool_arms: Dict[str, EnhancedDecisionArm] = {}
        self.tool_selection_history = []  # å·¥å…·é€‰æ‹©å†å²
        self.total_tool_selections = 0  # æ€»å·¥å…·é€‰æ‹©æ¬¡æ•°
        
        # ç®—æ³•é€‰æ‹©ç­–ç•¥
        self.algorithm_preferences = {
            'thompson_sampling': 0.4,
            'ucb_variant': 0.35,
            'epsilon_greedy': 0.25
        }
        
        # è·¯å¾„çº§åˆ«çš„æ€§èƒ½ç»Ÿè®¡
        self.algorithm_performance = defaultdict(lambda: {'successes': 0, 'total': 0})
        self.path_selection_history = []  # è·¯å¾„é€‰æ‹©å†å²
        self.total_path_selections = 0  # æ€»è·¯å¾„é€‰æ‹©æ¬¡æ•°
        
        # ğŸ”§ æ–°å¢ï¼šå·¥å…·çº§åˆ«çš„æ€§èƒ½ç»Ÿè®¡
        self.tool_algorithm_performance = defaultdict(lambda: {'successes': 0, 'total': 0})
        
        # ğŸ† é»„é‡‘å†³ç­–æ¨¡æ¿ç³»ç»Ÿ
        self.golden_templates: Dict[str, Dict[str, any]] = {}  # å­˜å‚¨é»„é‡‘æ¨¡æ¿
        self.golden_template_config = {
            'success_rate_threshold': 0.90,  # æˆåŠŸç‡é˜ˆå€¼90%
            'min_samples_required': 20,      # æœ€å°æ ·æœ¬æ•°20æ¬¡
            'confidence_threshold': 0.95,    # ç½®ä¿¡åº¦é˜ˆå€¼
            'stability_check_window': 10,    # ç¨³å®šæ€§æ£€æŸ¥çª—å£
            'max_golden_templates': 50       # æœ€å¤§é»„é‡‘æ¨¡æ¿æ•°é‡
        }
        self.template_usage_stats = defaultdict(int)  # é»„é‡‘æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡
        self.template_match_history = []  # æ¨¡æ¿åŒ¹é…å†å²
        
        # ğŸ”§ æ”¹è¿›æ–¹æ¡ˆï¼šé‡‡ç”¨åŠ¨æ€åˆ›å»ºç­–ç•¥ï¼Œåœ¨éœ€è¦æ—¶è‡ªåŠ¨åˆ›å»ºå†³ç­–è‡‚
        
        # ğŸ” æ–°å¢ï¼šçŸ¥è¯†æ¥æºè¿½è¸ªç³»ç»Ÿ
        self.feedback_source_tracking = {
            "retrospection": {"count": 0, "success_rate": 0.0, "avg_reward": 0.0},
            "user_feedback": {"count": 0, "success_rate": 0.0, "avg_reward": 0.0},
            "auto_evaluation": {"count": 0, "success_rate": 0.0, "avg_reward": 0.0},
            "tool_verification": {"count": 0, "success_rate": 0.0, "avg_reward": 0.0}
        }
        self.source_weight_config = {
            "retrospection": 0.8,      # å›æº¯åˆ†ææƒé‡ï¼ˆåˆå§‹æ¢ç´¢å¥–åŠ±ï¼‰
            "user_feedback": 1.0,      # ç”¨æˆ·åé¦ˆæƒé‡ï¼ˆæ ‡å‡†æƒé‡ï¼‰
            "auto_evaluation": 0.6,    # è‡ªåŠ¨è¯„ä¼°æƒé‡
            "tool_verification": 0.9   # å·¥å…·éªŒè¯æƒé‡
        }
        
        # ğŸ­ æ–°å¢ï¼šè¯•ç‚¼åœºç³»ç»Ÿ - æ–°æ€æƒ³çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†
        self.trial_ground = {
            "learned_paths": {},       # å­¦ä¹ è·¯å¾„æ³¨å†Œè¡¨: strategy_id -> metadata
            "trial_history": [],       # è¯•ç‚¼å†å²è®°å½•
            "promotion_candidates": set(),  # é»„é‡‘æ¨¡æ¿å€™é€‰è·¯å¾„
            "culling_candidates": set(),    # æ·˜æ±°å€™é€‰è·¯å¾„
            "exploration_boost_active": {},  # æ­£åœ¨äº«å—æ¢ç´¢å¢å¼ºçš„è·¯å¾„: strategy_id -> remaining_boosts
            "performance_watch_list": {},   # æ€§èƒ½ç›‘æ§åˆ—è¡¨: strategy_id -> watch_data
            "culled_paths": []             # æ·˜æ±°å†å²è®°å½•
        }
        
        # è¯•ç‚¼åœºé…ç½®
        self.trial_config = {
            "exploration_boost_rounds": 10,        # æ–°è·¯å¾„äº«å—æ¢ç´¢å¢å¼ºçš„è½®æ•°
            "promotion_evaluation_window": 15,     # æå‡è¯„ä¼°çª—å£ï¼ˆæœ€è¿‘Næ¬¡è¡¨ç°ï¼‰
            "culling_threshold": 0.25,             # æ·˜æ±°é˜ˆå€¼ï¼ˆæˆåŠŸç‡ä½äºæ­¤å€¼è€ƒè™‘æ·˜æ±°ï¼‰
            "culling_min_samples": 20,             # æ·˜æ±°å‰çš„æœ€å°‘è¯•éªŒæ¬¡æ•°
            "learned_path_bonus": 0.15,            # å­¦ä¹ è·¯å¾„é¢å¤–æ¢ç´¢å¥–åŠ±
            "golden_promotion_threshold": 0.85,    # é»„é‡‘æ¨¡æ¿æå‡é˜ˆå€¼
            "learned_path_protection_time": 3600,  # å­¦ä¹ è·¯å¾„ä¿æŠ¤æ—¶é—´ï¼ˆç§’ï¼‰
            "max_culled_history": 100,             # æœ€å¤§æ·˜æ±°å†å²è®°å½•æ•°
            "consecutive_failures_limit": 10       # è¿ç»­å¤±è´¥æ·˜æ±°é™åˆ¶
        }
        
        logger.info("ğŸ° MABConverger å·²åˆå§‹åŒ– - åŒå±‚å­¦ä¹ æ¨¡å¼ï¼šæ€ç»´è·¯å¾„ + å·¥å…·é€‰æ‹©")
        logger.info("ğŸ† é»„é‡‘å†³ç­–æ¨¡æ¿ç³»ç»Ÿå·²å¯ç”¨")
        logger.info("ğŸ”§ å·¥å…·é€‰æ‹©MABç³»ç»Ÿå·²å°±ç»ª")
        logger.info("ğŸ” çŸ¥è¯†æ¥æºè¿½è¸ªç³»ç»Ÿå·²æ¿€æ´»")
        logger.info("ğŸ­ è¯•ç‚¼åœºç³»ç»Ÿå·²å°±ç»ª - æ–°æ€æƒ³çš„æˆé•¿æ‘‡ç¯®")
    
    def _create_strategy_arm_if_missing(self, strategy_id: str, path_type: str = None, 
                                       path_source: str = "unknown", reasoning_path: 'ReasoningPath' = None) -> EnhancedDecisionArm:
        """
        ğŸŒŸ å¢å¼ºç‰ˆåŠ¨æ€ç­–ç•¥å†³ç­–è‡‚åˆ›å»ºå™¨ - æ–°æ€æƒ³çš„è¯•ç‚¼åœºå…¥å£
        
        è¿™æ˜¯æ–°æ€æƒ³è¿›å…¥MABç³»ç»Ÿçš„å…³é”®å…¥å£ï¼Œè´Ÿè´£ï¼š
        1. è¯†åˆ«è·¯å¾„æ¥æºï¼ˆé™æ€æ¨¡æ¿ vs åŠ¨æ€å­¦ä¹ ï¼‰
        2. ä¸ºä¸åŒæ¥æºçš„è·¯å¾„è®¾ç½®é€‚å½“çš„åˆå§‹å‚æ•°
        3. æ¿€æ´»æ–°æ€æƒ³çš„æ¢ç´¢å¢å¼ºæœºåˆ¶
        
        Args:
            strategy_id: ç­–ç•¥ID
            path_type: è·¯å¾„ç±»å‹ï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨æ¨æ–­ï¼‰
            path_source: è·¯å¾„æ¥æº ("static_template", "learned_exploration", "manual_addition", "unknown")
            reasoning_path: å®Œæ•´çš„æ¨ç†è·¯å¾„å¯¹è±¡ï¼ˆåŒ…å«æ›´å¤šå…ƒæ•°æ®ï¼‰
            
        Returns:
            å¯¹åº”çš„å†³ç­–è‡‚ï¼Œå·²é’ˆå¯¹è·¯å¾„æ¥æºè¿›è¡Œä¼˜åŒ–åˆå§‹åŒ–
        """
        if strategy_id not in self.path_arms:
            if path_type is None:
                # è‡ªåŠ¨æ¨æ–­è·¯å¾„ç±»å‹
                path_type = self._infer_path_type_from_strategy_id(strategy_id)
            
            # ğŸ¯ æ ¹æ®è·¯å¾„æ¥æºæ¨æ–­ç±»å‹å’Œåˆå§‹åŒ–å‚æ•°
            detected_source = self._detect_path_source(strategy_id, path_type, reasoning_path)
            effective_source = path_source if path_source != "unknown" else detected_source
            
            # åˆ›å»ºå†³ç­–è‡‚
            new_arm = EnhancedDecisionArm(
                path_id=strategy_id,
                option=path_type
            )
            
            # ğŸŒ± æ–°æ€æƒ³ç‰¹æ®Šåˆå§‹åŒ–ï¼šä¸ºå­¦ä¹ è·¯å¾„æä¾›æ¢ç´¢ä¼˜åŠ¿
            if effective_source == "learned_exploration":
                # å­¦ä¹ è·¯å¾„è·å¾—åˆå§‹æ¢ç´¢å¥–åŠ±
                new_arm.success_count = 1  # ç»™äºˆåˆå§‹æ­£å‘ä¿¡å·
                new_arm.total_reward = 0.3  # ç»™äºˆé€‚ä¸­çš„åˆå§‹å¥–åŠ±
                new_arm.rl_reward_history = [0.3]  # è®°å½•åˆå§‹å¥–åŠ±
                
                # æ ‡è®°ä¸ºæ–°å­¦ä¹ è·¯å¾„
                self._mark_as_learned_path(strategy_id, reasoning_path)
                
                logger.info(f"ğŸŒ± æ–°å­¦ä¹ è·¯å¾„è¿›å…¥è¯•ç‚¼åœº: {strategy_id} ({path_type})")
                logger.info(f"   è·å¾—æ¢ç´¢å¢å¼º: åˆå§‹æˆåŠŸä¿¡å· + 0.3å¥–åŠ±")
                
            elif effective_source == "manual_addition":
                # æ‰‹åŠ¨æ·»åŠ çš„è·¯å¾„è·å¾—ä¸­ç­‰æ¢ç´¢ä¼˜åŠ¿
                new_arm.success_count = 1
                new_arm.total_reward = 0.2
                new_arm.rl_reward_history = [0.2]
                
                logger.info(f"â• æ‰‹åŠ¨è·¯å¾„è¿›å…¥è¯•ç‚¼åœº: {strategy_id} ({path_type})")
                
            else:
                # é™æ€æ¨¡æ¿æˆ–æœªçŸ¥æ¥æºä¿æŒé»˜è®¤åˆå§‹åŒ–
                logger.debug(f"ğŸ†• åŠ¨æ€åˆ›å»ºç­–ç•¥å†³ç­–è‡‚: {strategy_id} ({path_type}) [æ¥æº: {effective_source}]")
            
            self.path_arms[strategy_id] = new_arm
            
            # è®°å½•åˆ°è¯•ç‚¼åœºå†å²
            self._record_trial_entry(strategy_id, path_type, effective_source)
        
        return self.path_arms[strategy_id]
    
    def _create_tool_arm_if_missing(self, tool_id: str, tool_name: str = None) -> EnhancedDecisionArm:
        """
        åŠ¨æ€åˆ›å»ºå·¥å…·å†³ç­–è‡‚ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        
        Args:
            tool_id: å·¥å…·ID
            tool_name: å·¥å…·åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨tool_idï¼‰
            
        Returns:
            å¯¹åº”çš„å·¥å…·å†³ç­–è‡‚
        """
        if tool_id not in self.tool_arms:
            if tool_name is None:
                tool_name = tool_id  # é»˜è®¤ä½¿ç”¨tool_idä½œä¸ºå·¥å…·åç§°
            
            self.tool_arms[tool_id] = EnhancedDecisionArm(
                path_id=tool_id,
                option=tool_name
            )
            logger.debug(f"ğŸ”§ åŠ¨æ€åˆ›å»ºå·¥å…·å†³ç­–è‡‚: {tool_id} ({tool_name})")
        
        return self.tool_arms[tool_id]
    
    # ==================== ğŸ­ è¯•ç‚¼åœºç³»ç»Ÿæ ¸å¿ƒæ–¹æ³• ====================
    
    def _detect_path_source(self, strategy_id: str, path_type: str, reasoning_path: 'ReasoningPath' = None) -> str:
        """
        æ™ºèƒ½æ£€æµ‹è·¯å¾„æ¥æº
        
        Args:
            strategy_id: ç­–ç•¥ID
            path_type: è·¯å¾„ç±»å‹
            reasoning_path: æ¨ç†è·¯å¾„å¯¹è±¡
            
        Returns:
            è·¯å¾„æ¥æºç±»å‹
        """
        # 1. åŸºäºç­–ç•¥IDå‘½åæ¨¡å¼æ£€æµ‹
        if "learned_" in strategy_id or "explored_" in strategy_id or "generated_" in strategy_id:
            return "learned_exploration"
        
        if "custom_" in strategy_id or "manual_" in strategy_id or "user_" in strategy_id:
            return "manual_addition"
        
        # 2. åŸºäºè·¯å¾„ç±»å‹æ£€æµ‹
        if path_type and ("å­¦ä¹ " in path_type or "æ¢ç´¢" in path_type or "å‘ç°" in path_type):
            return "learned_exploration"
        
        # 3. åŸºäºæ¨ç†è·¯å¾„å¯¹è±¡å…ƒæ•°æ®æ£€æµ‹
        if reasoning_path:
            # æ£€æŸ¥è·¯å¾„æè¿°ä¸­çš„å…³é”®è¯
            description = getattr(reasoning_path, 'description', '')
            if any(keyword in description for keyword in ["ä»æ¢ç´¢ä¸­å­¦ä¹ ", "çŸ¥è¯†å‘ç°", "å¤–éƒ¨æ™ºæ…§", "åŠ¨æ€ç”Ÿæˆ"]):
                return "learned_exploration"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å­¦ä¹ æ¥æºæ ‡è®°
            if hasattr(reasoning_path, 'is_learned') and getattr(reasoning_path, 'is_learned', False):
                return "learned_exploration"
        
        # 4. é»˜è®¤ä¸ºé™æ€æ¨¡æ¿
        return "static_template"
    
    def _mark_as_learned_path(self, strategy_id: str, reasoning_path: 'ReasoningPath' = None):
        """
        æ ‡è®°ä¸ºå­¦ä¹ è·¯å¾„ï¼Œè®°å½•ç›¸å…³å…ƒæ•°æ®
        
        Args:
            strategy_id: ç­–ç•¥ID
            reasoning_path: æ¨ç†è·¯å¾„å¯¹è±¡
        """
        learned_metadata = {
            "strategy_id": strategy_id,
            "marked_at": time.time(),
            "source": "knowledge_exploration",
            "initial_boost_given": True,
            "promotion_eligible": True,
            "trial_start_time": time.time()
        }
        
        # ä»æ¨ç†è·¯å¾„å¯¹è±¡ä¸­æå–æ›´å¤šå…ƒæ•°æ®
        if reasoning_path:
            learned_metadata.update({
                "path_type": getattr(reasoning_path, 'path_type', ''),
                "description": getattr(reasoning_path, 'description', ''),
                "learning_source": getattr(reasoning_path, 'learning_source', ''),
                "is_learned": getattr(reasoning_path, 'is_learned', True),
                "effectiveness_score": getattr(reasoning_path, 'effectiveness_score', 0.5)
            })
        
        # æ³¨å†Œåˆ°è¯•ç‚¼åœº
        self.trial_ground["learned_paths"][strategy_id] = learned_metadata
        
        # æ¿€æ´»æ¢ç´¢å¢å¼º
        self.trial_ground["exploration_boost_active"][strategy_id] = self.trial_config["exploration_boost_rounds"]
        
        logger.debug(f"ğŸŒ± è·¯å¾„å·²æ ‡è®°ä¸ºå­¦ä¹ è·¯å¾„: {strategy_id}")
        logger.debug(f"   æ¢ç´¢å¢å¼ºè½®æ•°: {self.trial_config['exploration_boost_rounds']}")
    
    def _record_trial_entry(self, strategy_id: str, path_type: str, source: str):
        """
        è®°å½•è¯•ç‚¼åœºè¿›å…¥å†å²
        
        Args:
            strategy_id: ç­–ç•¥ID
            path_type: è·¯å¾„ç±»å‹
            source: è·¯å¾„æ¥æº
        """
        trial_record = {
            "strategy_id": strategy_id,
            "path_type": path_type,
            "source": source,
            "entry_time": time.time(),
            "entry_round": self.total_path_selections,
            "status": "active_trial"
        }
        
        self.trial_ground["trial_history"].append(trial_record)
        
        # ä¿æŒå†å²è®°å½•å¤§å°
        if len(self.trial_ground["trial_history"]) > 1000:
            self.trial_ground["trial_history"] = self.trial_ground["trial_history"][-800:]
        
        logger.debug(f"ğŸ­ è¯•ç‚¼åœºè®°å½•: {strategy_id} å¼€å§‹è¯•ç‚¼ ({source})")
    
    def is_learned_path(self, strategy_id: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºå­¦ä¹ è·¯å¾„
        
        Args:
            strategy_id: ç­–ç•¥ID
            
        Returns:
            æ˜¯å¦ä¸ºå­¦ä¹ è·¯å¾„
        """
        return strategy_id in self.trial_ground["learned_paths"]
    
    def get_exploration_boost(self, strategy_id: str) -> float:
        """
        è·å–è·¯å¾„çš„æ¢ç´¢å¢å¼ºç³»æ•°
        
        Args:
            strategy_id: ç­–ç•¥ID
            
        Returns:
            æ¢ç´¢å¢å¼ºç³»æ•° (1.0 = æ— å¢å¼º, > 1.0 = æœ‰å¢å¼º)
        """
        boost_factor = 1.0
        
        # åŸºç¡€å¢å¼ºï¼šæ–°å­¦ä¹ è·¯å¾„
        if strategy_id in self.trial_ground["exploration_boost_active"]:
            remaining_rounds = self.trial_ground["exploration_boost_active"][strategy_id]
            if remaining_rounds > 0:
                # é€’å‡çš„æ¢ç´¢å¢å¼º
                base_bonus = self.trial_config["learned_path_bonus"]
                decay_factor = remaining_rounds / self.trial_config["exploration_boost_rounds"]
                boost_factor += base_bonus * decay_factor
        
        # ç‰¹æ®Šå¢å¼ºï¼šå­¦ä¹ è·¯å¾„æ°¸ä¹…å°å¹…å¢å¼º
        if self.is_learned_path(strategy_id):
            boost_factor += 0.05  # 5%çš„æ°¸ä¹…å°å¹…å¢å¼º
        
        return boost_factor
    
    def _update_exploration_boost(self, strategy_id: str):
        """
        æ›´æ–°æ¢ç´¢å¢å¼ºçŠ¶æ€ï¼ˆæ¯æ¬¡é€‰æ‹©åè°ƒç”¨ï¼‰
        
        Args:
            strategy_id: è¢«é€‰æ‹©çš„ç­–ç•¥ID
        """
        if strategy_id in self.trial_ground["exploration_boost_active"]:
            remaining = self.trial_ground["exploration_boost_active"][strategy_id]
            if remaining > 0:
                self.trial_ground["exploration_boost_active"][strategy_id] = remaining - 1
                
                if remaining == 1:  # å³å°†ç”¨å®Œ
                    logger.info(f"ğŸ¯ è·¯å¾„ {strategy_id} çš„æ¢ç´¢å¢å¼ºå³å°†ç»“æŸ")
                elif remaining <= 0:
                    del self.trial_ground["exploration_boost_active"][strategy_id]
                    logger.info(f"âœ… è·¯å¾„ {strategy_id} å®Œæˆæ¢ç´¢å¢å¼ºæœŸï¼Œè¿›å…¥æ­£å¸¸ç«äº‰")
    
    def _check_culling_candidates(self, strategy_id: str, arm: EnhancedDecisionArm, success: bool):
        """
        ğŸ—¡ï¸ æ£€æŸ¥å¹¶ç®¡ç†æ·˜æ±°å€™é€‰è·¯å¾„
        
        Args:
            strategy_id: ç­–ç•¥ID
            arm: å†³ç­–è‡‚å¯¹è±¡
            success: æœ€æ–°æ‰§è¡Œç»“æœ
        """
        # åªæ£€æŸ¥æœ‰è¶³å¤Ÿæ ·æœ¬çš„è·¯å¾„
        if arm.activation_count < self.trial_config["culling_min_samples"]:
            return
        
        # è·å–æˆåŠŸç‡é˜ˆå€¼
        culling_threshold = self.trial_config["culling_threshold"]
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¢«åŠ å…¥æ·˜æ±°å€™é€‰
        if arm.success_rate < culling_threshold:
            if strategy_id not in self.trial_ground["culling_candidates"]:
                self.trial_ground["culling_candidates"].add(strategy_id)
                
                # è®°å½•è¿›å…¥æ·˜æ±°å€™é€‰çš„åŸå› 
                self.trial_ground["performance_watch_list"][strategy_id] = {
                    "reason": "low_success_rate",
                    "success_rate": arm.success_rate,
                    "threshold": culling_threshold,
                    "added_at": time.time(),
                    "sample_count": arm.activation_count,
                    "consecutive_failures": self._calculate_consecutive_failures(arm)
                }
                
                logger.warning(f"âš ï¸ è·¯å¾„ {strategy_id} è¿›å…¥æ·˜æ±°å€™é€‰åå•")
                logger.warning(f"   æˆåŠŸç‡: {arm.success_rate:.3f} < é˜ˆå€¼: {culling_threshold}")
                logger.warning(f"   æ ·æœ¬æ•°: {arm.activation_count}")
                
                # å¦‚æœæ˜¯å­¦ä¹ è·¯å¾„ï¼Œç»™äºˆè­¦å‘Šä½†ä¸ç«‹å³æ·˜æ±°
                if self.is_learned_path(strategy_id):
                    logger.warning(f"ğŸŒ± å­¦ä¹ è·¯å¾„ {strategy_id} è¡¨ç°ä¸ä½³ï¼Œå°†ç»™äºˆé¢å¤–è§‚å¯ŸæœŸ")
        
        # å¦‚æœæˆåŠŸç‡å›å‡ï¼Œç§»å‡ºæ·˜æ±°å€™é€‰
        elif strategy_id in self.trial_ground["culling_candidates"]:
            if arm.success_rate >= culling_threshold * 1.2:  # éœ€è¦è¶…è¿‡é˜ˆå€¼20%æ‰èƒ½ç§»å‡º
                self.trial_ground["culling_candidates"].remove(strategy_id)
                if strategy_id in self.trial_ground["performance_watch_list"]:
                    del self.trial_ground["performance_watch_list"][strategy_id]
                
                logger.info(f"âœ… è·¯å¾„ {strategy_id} æ€§èƒ½å›å‡ï¼Œç§»å‡ºæ·˜æ±°å€™é€‰åå•")
                logger.info(f"   å½“å‰æˆåŠŸç‡: {arm.success_rate:.3f} >= å›å½’é˜ˆå€¼: {culling_threshold * 1.2:.3f}")
    
    def _calculate_consecutive_failures(self, arm: EnhancedDecisionArm) -> int:
        """
        è®¡ç®—è¿ç»­å¤±è´¥æ¬¡æ•°
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            è¿ç»­å¤±è´¥æ¬¡æ•°
        """
        if not arm.recent_results:
            return 0
        
        consecutive_count = 0
        # ä»æœ€è¿‘çš„ç»“æœå¼€å§‹å¾€å‰æ•°
        for result in reversed(arm.recent_results):
            if not result:  # å¦‚æœæ˜¯å¤±è´¥
                consecutive_count += 1
            else:  # å¦‚æœæˆåŠŸäº†ï¼Œå°±åœæ­¢è®¡æ•°
                break
        
        return consecutive_count
    
    def execute_automatic_culling(self) -> Dict[str, Any]:
        """
        ğŸ—¡ï¸ æ‰§è¡Œè‡ªåŠ¨æ·˜æ±°æœºåˆ¶
        
        Returns:
            æ·˜æ±°æ‰§è¡Œç»“æœ
        """
        culling_results = {
            "executed_at": time.time(),
            "candidates_reviewed": len(self.trial_ground["culling_candidates"]),
            "paths_culled": [],
            "paths_spared": [],
            "action_summary": {}
        }
        
        if not self.trial_ground["culling_candidates"]:
            culling_results["action_summary"] = {"message": "æ— éœ€è¦æ·˜æ±°çš„å€™é€‰è·¯å¾„"}
            return culling_results
        
        logger.info(f"ğŸ—¡ï¸ å¼€å§‹è‡ªåŠ¨æ·˜æ±°æ£€æŸ¥ï¼Œå€™é€‰è·¯å¾„: {len(self.trial_ground['culling_candidates'])} ä¸ª")
        
        candidates_to_remove = set()
        
        for strategy_id in list(self.trial_ground["culling_candidates"]):
            if strategy_id not in self.path_arms:
                candidates_to_remove.add(strategy_id)
                continue
            
            arm = self.path_arms[strategy_id]
            watch_data = self.trial_ground["performance_watch_list"].get(strategy_id, {})
            
            # å†³å®šæ˜¯å¦çœŸæ­£æ·˜æ±°
            should_cull, reason = self._should_cull_path(strategy_id, arm, watch_data)
            
            if should_cull:
                # æ‰§è¡Œæ·˜æ±°
                self._cull_path(strategy_id, reason)
                candidates_to_remove.add(strategy_id)
                culling_results["paths_culled"].append({
                    "strategy_id": strategy_id,
                    "reason": reason,
                    "final_success_rate": arm.success_rate,
                    "total_activations": arm.activation_count
                })
                logger.info(f"ğŸ—¡ï¸ æ·˜æ±°è·¯å¾„: {strategy_id} - {reason}")
            else:
                # æš‚ç¼“æ·˜æ±°
                culling_results["paths_spared"].append({
                    "strategy_id": strategy_id,
                    "reason": f"æš‚ç¼“æ·˜æ±° - {reason}",
                    "current_success_rate": arm.success_rate
                })
                logger.info(f"â³ æš‚ç¼“æ·˜æ±°è·¯å¾„: {strategy_id} - {reason}")
        
        # æ¸…ç†å€™é€‰åå•
        for strategy_id in candidates_to_remove:
            self.trial_ground["culling_candidates"].discard(strategy_id)
            self.trial_ground["performance_watch_list"].pop(strategy_id, None)
        
        culling_results["action_summary"] = {
            "total_culled": len(culling_results["paths_culled"]),
            "total_spared": len(culling_results["paths_spared"]),
            "remaining_candidates": len(self.trial_ground["culling_candidates"])
        }
        
        logger.info(f"ğŸ—¡ï¸ æ·˜æ±°æ£€æŸ¥å®Œæˆ: æ·˜æ±° {len(culling_results['paths_culled'])} ä¸ª, "
                   f"æš‚ç¼“ {len(culling_results['paths_spared'])} ä¸ª")
        
        return culling_results
    
    def _should_cull_path(self, strategy_id: str, arm: EnhancedDecisionArm, watch_data: Dict[str, Any]) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥æ·˜æ±°æŒ‡å®šè·¯å¾„
        
        Args:
            strategy_id: ç­–ç•¥ID
            arm: å†³ç­–è‡‚å¯¹è±¡
            watch_data: ç›‘æ§æ•°æ®
            
        Returns:
            (æ˜¯å¦æ·˜æ±°, åŸå› )
        """
        # 1. å­¦ä¹ è·¯å¾„è·å¾—é¢å¤–ä¿æŠ¤
        if self.is_learned_path(strategy_id):
            # å­¦ä¹ è·¯å¾„éœ€è¦æ›´å·®çš„è¡¨ç°æ‰ä¼šè¢«æ·˜æ±°
            harsh_threshold = self.trial_config["culling_threshold"] * 0.5  # æ›´ä¸¥æ ¼çš„é˜ˆå€¼
            if arm.success_rate > harsh_threshold:
                return False, "å­¦ä¹ è·¯å¾„äº«å—ä¿æŠ¤æœŸ"
            
            # æ£€æŸ¥å­¦ä¹ è·¯å¾„çš„è¯•éªŒæ—¶é—´
            learned_meta = self.trial_ground["learned_paths"].get(strategy_id, {})
            trial_time = time.time() - learned_meta.get("trial_start_time", time.time())
            if trial_time < 3600:  # 1å°æ—¶ä¿æŠ¤æœŸ
                return False, "å­¦ä¹ è·¯å¾„ä»åœ¨ä¿æŠ¤æœŸå†…"
        
        # 2. é»„é‡‘æ¨¡æ¿ç»ä¸æ·˜æ±°
        if strategy_id in self.golden_templates:
            return False, "é»„é‡‘æ¨¡æ¿ä¸å¯æ·˜æ±°"
        
        # 3. æ£€æŸ¥è¿ç»­å¤±è´¥æƒ…å†µ
        consecutive_failures = watch_data.get("consecutive_failures", 0)
        if consecutive_failures >= 10:  # è¿ç»­10æ¬¡å¤±è´¥
            return True, f"è¿ç»­å¤±è´¥{consecutive_failures}æ¬¡"
        
        # 4. æ£€æŸ¥é•¿æœŸè¡¨ç°
        if arm.success_rate < self.trial_config["culling_threshold"] * 0.8:  # ä½äºé˜ˆå€¼çš„80%
            watch_duration = time.time() - watch_data.get("added_at", time.time())
            if watch_duration > 1800:  # åœ¨è§‚å¯Ÿåå•è¶…è¿‡30åˆ†é’Ÿ
                return True, f"é•¿æœŸè¡¨ç°ä¸ä½³ (æˆåŠŸç‡: {arm.success_rate:.3f})"
        
        # 5. æ£€æŸ¥ä½¿ç”¨é¢‘ç‡
        if arm.activation_count > 50 and arm.success_rate < self.trial_config["culling_threshold"]:
            return True, f"å¤§é‡è¯•éªŒåè¡¨ç°ä»ä¸ä½³ ({arm.activation_count} æ¬¡è¯•éªŒ)"
        
        return False, "æš‚ä¸ç¬¦åˆæ·˜æ±°æ¡ä»¶"
    
    def _cull_path(self, strategy_id: str, reason: str):
        """
        æ‰§è¡Œè·¯å¾„æ·˜æ±°
        
        Args:
            strategy_id: ç­–ç•¥ID
            reason: æ·˜æ±°åŸå› 
        """
        if strategy_id in self.path_arms:
            # è®°å½•æ·˜æ±°å†å²
            culled_arm = self.path_arms[strategy_id]
            cull_record = {
                "strategy_id": strategy_id,
                "culled_at": time.time(),
                "reason": reason,
                "final_stats": {
                    "success_rate": culled_arm.success_rate,
                    "total_activations": culled_arm.activation_count,
                    "total_reward": culled_arm.total_reward
                },
                "was_learned_path": self.is_learned_path(strategy_id)
            }
            
            # ä¿å­˜åˆ°å†å²è®°å½•
            if "culled_paths" not in self.trial_ground:
                self.trial_ground["culled_paths"] = []
            self.trial_ground["culled_paths"].append(cull_record)
            
            # æ‰§è¡Œç§»é™¤
            del self.path_arms[strategy_id]
            
            # æ¸…ç†ç›¸å…³æ•°æ®
            self.trial_ground["learned_paths"].pop(strategy_id, None)
            self.trial_ground["exploration_boost_active"].pop(strategy_id, None)
            self.trial_ground["promotion_candidates"].discard(strategy_id)
            
            logger.info(f"ğŸ—¡ï¸ è·¯å¾„ {strategy_id} å·²è¢«æ·˜æ±°: {reason}")
            logger.info(f"   æœ€ç»ˆç»Ÿè®¡: æˆåŠŸç‡ {culled_arm.success_rate:.3f}, æ¿€æ´» {culled_arm.activation_count} æ¬¡")
    
    def get_trial_ground_analytics(self) -> Dict[str, Any]:
        """
        ğŸ“Š è·å–è¯•ç‚¼åœºå…¨é¢åˆ†ææ•°æ®
        
        Returns:
            è¯•ç‚¼åœºç»Ÿè®¡åˆ†æ
        """
        analytics = {
            "timestamp": time.time(),
            "overview": self._get_trial_overview(),
            "learned_paths": self._analyze_learned_paths(),
            "exploration_status": self._analyze_exploration_status(),
            "culling_analysis": self._analyze_culling_situation(),
            "performance_trends": self._analyze_performance_trends(),
            "golden_template_candidates": self._analyze_golden_candidates()
        }
        
        return analytics
    
    def _get_trial_overview(self) -> Dict[str, Any]:
        """è·å–è¯•ç‚¼åœºæ€»ä½“æ¦‚å†µ"""
        total_paths = len(self.path_arms)
        learned_paths = len(self.trial_ground["learned_paths"])
        
        return {
            "total_active_paths": total_paths,
            "learned_paths_count": learned_paths,
            "static_paths_count": total_paths - learned_paths,
            "exploration_boost_active": len(self.trial_ground["exploration_boost_active"]),
            "culling_candidates": len(self.trial_ground["culling_candidates"]),
            "promotion_candidates": len(self.trial_ground["promotion_candidates"]),
            "golden_templates": len(self.golden_templates),
            "culled_paths_history": len(self.trial_ground.get("culled_paths", []))
        }
    
    def _analyze_learned_paths(self) -> Dict[str, Any]:
        """åˆ†æå­¦ä¹ è·¯å¾„çš„è¯¦ç»†æƒ…å†µ"""
        learned_analysis = {
            "active_learned_paths": [],
            "performance_summary": {
                "excellent": 0,  # > 0.8
                "good": 0,       # 0.6-0.8
                "average": 0,    # 0.4-0.6
                "poor": 0        # < 0.4
            },
            "avg_success_rate": 0.0,
            "total_activations": 0
        }
        
        if not self.trial_ground["learned_paths"]:
            return learned_analysis
        
        success_rates = []
        total_activations = 0
        
        for strategy_id, meta in self.trial_ground["learned_paths"].items():
            if strategy_id not in self.path_arms:
                continue
                
            arm = self.path_arms[strategy_id]
            success_rate = arm.success_rate
            success_rates.append(success_rate)
            total_activations += arm.activation_count
            
            # æ€§èƒ½åˆ†ç±»
            if success_rate >= 0.8:
                learned_analysis["performance_summary"]["excellent"] += 1
            elif success_rate >= 0.6:
                learned_analysis["performance_summary"]["good"] += 1
            elif success_rate >= 0.4:
                learned_analysis["performance_summary"]["average"] += 1
            else:
                learned_analysis["performance_summary"]["poor"] += 1
            
            # è¯¦ç»†ä¿¡æ¯
            trial_duration = time.time() - meta.get("trial_start_time", time.time())
            learned_analysis["active_learned_paths"].append({
                "strategy_id": strategy_id,
                "source": meta.get("source", "unknown"),
                "success_rate": success_rate,
                "activations": arm.activation_count,
                "trial_duration_hours": trial_duration / 3600,
                "has_exploration_boost": strategy_id in self.trial_ground["exploration_boost_active"],
                "is_promotion_candidate": strategy_id in self.trial_ground["promotion_candidates"],
                "is_culling_candidate": strategy_id in self.trial_ground["culling_candidates"]
            })
        
        learned_analysis["avg_success_rate"] = sum(success_rates) / len(success_rates) if success_rates else 0.0
        learned_analysis["total_activations"] = total_activations
        
        return learned_analysis
    
    def _analyze_exploration_status(self) -> Dict[str, Any]:
        """åˆ†ææ¢ç´¢å¢å¼ºçŠ¶æ€"""
        exploration_analysis = {
            "active_boosts": [],
            "total_boost_paths": len(self.trial_ground["exploration_boost_active"]),
            "boost_distribution": {}
        }
        
        for strategy_id, remaining_rounds in self.trial_ground["exploration_boost_active"].items():
            if strategy_id in self.path_arms:
                arm = self.path_arms[strategy_id]
                exploration_analysis["active_boosts"].append({
                    "strategy_id": strategy_id,
                    "remaining_rounds": remaining_rounds,
                    "current_success_rate": arm.success_rate,
                    "activations_during_boost": arm.activation_count,
                    "is_learned_path": self.is_learned_path(strategy_id)
                })
                
                # åˆ†å¸ƒç»Ÿè®¡
                if remaining_rounds not in exploration_analysis["boost_distribution"]:
                    exploration_analysis["boost_distribution"][remaining_rounds] = 0
                exploration_analysis["boost_distribution"][remaining_rounds] += 1
        
        return exploration_analysis
    
    def _analyze_culling_situation(self) -> Dict[str, Any]:
        """åˆ†ææ·˜æ±°æƒ…å†µ"""
        culling_analysis = {
            "current_candidates": [],
            "recent_culled": [],
            "culling_threshold": self.trial_config["culling_threshold"],
            "protection_summary": {
                "golden_templates": 0,
                "learned_paths_protected": 0,
                "recent_paths": 0
            }
        }
        
        # å½“å‰æ·˜æ±°å€™é€‰
        for strategy_id in self.trial_ground["culling_candidates"]:
            if strategy_id in self.path_arms:
                arm = self.path_arms[strategy_id]
                watch_data = self.trial_ground["performance_watch_list"].get(strategy_id, {})
                
                culling_analysis["current_candidates"].append({
                    "strategy_id": strategy_id,
                    "success_rate": arm.success_rate,
                    "activations": arm.activation_count,
                    "watch_reason": watch_data.get("reason", "unknown"),
                    "watch_duration_minutes": (time.time() - watch_data.get("added_at", time.time())) / 60,
                    "is_learned_path": self.is_learned_path(strategy_id),
                    "consecutive_failures": watch_data.get("consecutive_failures", 0)
                })
        
        # æœ€è¿‘æ·˜æ±°çš„è·¯å¾„ï¼ˆæœ€å10ä¸ªï¼‰
        recent_culled = self.trial_ground.get("culled_paths", [])[-10:]
        for cull_record in recent_culled:
            culling_analysis["recent_culled"].append({
                "strategy_id": cull_record["strategy_id"],
                "culled_hours_ago": (time.time() - cull_record["culled_at"]) / 3600,
                "reason": cull_record["reason"],
                "final_success_rate": cull_record["final_stats"]["success_rate"],
                "was_learned_path": cull_record["was_learned_path"]
            })
        
        # ä¿æŠ¤æƒ…å†µç»Ÿè®¡
        for strategy_id in self.path_arms:
            if strategy_id in self.golden_templates:
                culling_analysis["protection_summary"]["golden_templates"] += 1
            elif self.is_learned_path(strategy_id):
                learned_meta = self.trial_ground["learned_paths"].get(strategy_id, {})
                trial_time = time.time() - learned_meta.get("trial_start_time", time.time())
                if trial_time < 3600:  # 1å°æ—¶ä¿æŠ¤æœŸ
                    culling_analysis["protection_summary"]["learned_paths_protected"] += 1
        
        return culling_analysis
    
    def _analyze_performance_trends(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        trends = {
            "overall_system_health": "healthy",
            "avg_success_rate": 0.0,
            "performance_distribution": {
                "excellent": 0,    # > 0.8
                "good": 0,         # 0.6-0.8
                "average": 0,      # 0.4-0.6
                "poor": 0,         # < 0.4
                "critical": 0      # < 0.2
            },
            "activation_distribution": {
                "highly_used": 0,     # > 100 activations
                "moderately_used": 0, # 20-100 activations
                "lightly_used": 0,    # 5-20 activations
                "rarely_used": 0      # < 5 activations
            },
            "trend_indicators": {
                "paths_improving": 0,
                "paths_declining": 0,
                "stable_paths": 0
            }
        }
        
        if not self.path_arms:
            return trends
        
        success_rates = []
        
        for strategy_id, arm in self.path_arms.items():
            success_rate = arm.success_rate
            success_rates.append(success_rate)
            
            # æ€§èƒ½åˆ†å¸ƒ
            if success_rate >= 0.8:
                trends["performance_distribution"]["excellent"] += 1
            elif success_rate >= 0.6:
                trends["performance_distribution"]["good"] += 1
            elif success_rate >= 0.4:
                trends["performance_distribution"]["average"] += 1
            elif success_rate >= 0.2:
                trends["performance_distribution"]["poor"] += 1
            else:
                trends["performance_distribution"]["critical"] += 1
            
            # ä½¿ç”¨é¢‘ç‡åˆ†å¸ƒ
            if arm.activation_count > 100:
                trends["activation_distribution"]["highly_used"] += 1
            elif arm.activation_count >= 20:
                trends["activation_distribution"]["moderately_used"] += 1
            elif arm.activation_count >= 5:
                trends["activation_distribution"]["lightly_used"] += 1
            else:
                trends["activation_distribution"]["rarely_used"] += 1
        
        trends["avg_success_rate"] = sum(success_rates) / len(success_rates)
        
        # ç³»ç»Ÿå¥åº·è¯„ä¼°
        poor_performance_ratio = (trends["performance_distribution"]["poor"] + 
                                trends["performance_distribution"]["critical"]) / len(self.path_arms)
        
        if poor_performance_ratio > 0.4:
            trends["overall_system_health"] = "critical"
        elif poor_performance_ratio > 0.2:
            trends["overall_system_health"] = "degraded"
        elif trends["avg_success_rate"] > 0.7:
            trends["overall_system_health"] = "excellent"
        else:
            trends["overall_system_health"] = "healthy"
        
        return trends
    
    def _analyze_golden_candidates(self) -> Dict[str, Any]:
        """åˆ†æé»„é‡‘æ¨¡æ¿å€™é€‰æƒ…å†µ"""
        golden_analysis = {
            "current_golden_count": len(self.golden_templates),
            "promotion_candidates": [],
            "golden_performance": {
                "avg_success_rate": 0.0,
                "total_activations": 0,
                "stability_score": 0.0
            }
        }
        
        # åˆ†ææå‡å€™é€‰
        for strategy_id in self.trial_ground["promotion_candidates"]:
            if strategy_id in self.path_arms:
                arm = self.path_arms[strategy_id]
                golden_analysis["promotion_candidates"].append({
                    "strategy_id": strategy_id,
                    "success_rate": arm.success_rate,
                    "activations": arm.activation_count,
                    "stability": arm.get_stability_score() if hasattr(arm, 'get_stability_score') else 0.0,
                    "is_learned_path": self.is_learned_path(strategy_id),
                    "qualification_score": self._calculate_golden_qualification_score(strategy_id, arm)
                })
        
        # åˆ†æç°æœ‰é»„é‡‘æ¨¡æ¿æ€§èƒ½
        if self.golden_templates:
            golden_success_rates = []
            golden_activations = 0
            
            for strategy_id in self.golden_templates:
                if strategy_id in self.path_arms:
                    arm = self.path_arms[strategy_id]
                    golden_success_rates.append(arm.success_rate)
                    golden_activations += arm.activation_count
            
            if golden_success_rates:
                golden_analysis["golden_performance"]["avg_success_rate"] = sum(golden_success_rates) / len(golden_success_rates)
                golden_analysis["golden_performance"]["total_activations"] = golden_activations
                golden_analysis["golden_performance"]["stability_score"] = min(golden_success_rates)  # æœ€ä½æˆåŠŸç‡ä½œä¸ºç¨³å®šæ€§æŒ‡æ ‡
        
        return golden_analysis
    
    def _calculate_golden_qualification_score(self, strategy_id: str, arm: EnhancedDecisionArm) -> float:
        """
        è®¡ç®—é»„é‡‘æ¨¡æ¿èµ„æ ¼è¯„åˆ†
        
        Args:
            strategy_id: ç­–ç•¥ID
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            èµ„æ ¼è¯„åˆ† (0-1ä¹‹é—´)
        """
        # åŸºç¡€æˆåŠŸç‡æƒé‡ (40%)
        success_score = arm.success_rate * 0.4
        
        # ä½¿ç”¨é¢‘ç‡æƒé‡ (20%)
        frequency_score = min(arm.activation_count / 100, 1.0) * 0.2
        
        # ç¨³å®šæ€§æƒé‡ (20%) - åŸºäºæœ€è¿‘è¡¨ç°çš„æ–¹å·®
        stability_score = 0.0
        if arm.recent_results and len(arm.recent_results) >= 5:
            recent_success_rate = sum(arm.recent_results[-10:]) / min(len(arm.recent_results), 10)
            # ç¨³å®šæ€§ = 1 - |æ€»ä½“æˆåŠŸç‡ - æœ€è¿‘æˆåŠŸç‡|
            stability_score = max(0, 1 - abs(arm.success_rate - recent_success_rate)) * 0.2
        
        # å­¦ä¹ è·¯å¾„åŠ åˆ† (10%) - é¼“åŠ±ä»æ¢ç´¢ä¸­å­¦åˆ°çš„ä¼˜ç§€è·¯å¾„
        learning_bonus = 0.1 if self.is_learned_path(strategy_id) and arm.success_rate > 0.7 else 0.0
        
        # æ—¶é—´è¡°å‡ (10%) - æ–°è·¯å¾„éœ€è¦æ›´å¤šéªŒè¯æ—¶é—´
        if strategy_id in self.trial_ground["learned_paths"]:
            learned_meta = self.trial_ground["learned_paths"][strategy_id]
            trial_duration = time.time() - learned_meta.get("trial_start_time", time.time())
            time_score = min(trial_duration / (24 * 3600), 1.0) * 0.1  # 24å°æ—¶è¾¾åˆ°æ»¡åˆ†
        else:
            time_score = 0.1  # é™æ€è·¯å¾„ç»™æ»¡åˆ†
        
        return success_score + frequency_score + stability_score + learning_bonus + time_score
    
    # ğŸ­ è¯•ç‚¼åœºç®¡ç†å’Œç»´æŠ¤æ–¹æ³•
    
    def trigger_trial_ground_maintenance(self) -> Dict[str, Any]:
        """
        ğŸ”§ è§¦å‘è¯•ç‚¼åœºç»´æŠ¤ä»»åŠ¡
        
        Returns:
            ç»´æŠ¤ç»“æœæ‘˜è¦
        """
        maintenance_result = {
            "timestamp": time.time(),
            "tasks_executed": [],
            "cleanup_results": {},
            "analytics_snapshot": {}
        }
        
        logger.info("ğŸ”§ å¼€å§‹è¯•ç‚¼åœºç»´æŠ¤ä»»åŠ¡...")
        
        # 1. æ‰§è¡Œè‡ªåŠ¨æ·˜æ±°æ£€æŸ¥
        try:
            culling_results = self.execute_automatic_culling()
            maintenance_result["tasks_executed"].append("automatic_culling")
            maintenance_result["cleanup_results"]["culling"] = culling_results
            logger.info(f"âœ… è‡ªåŠ¨æ·˜æ±°æ£€æŸ¥å®Œæˆ: æ·˜æ±° {len(culling_results.get('paths_culled', []))} ä¸ªè·¯å¾„")
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨æ·˜æ±°æ£€æŸ¥å¤±è´¥: {e}")
            maintenance_result["cleanup_results"]["culling"] = {"error": str(e)}
        
        # 2. æ¸…ç†è¿‡æœŸçš„æ¢ç´¢å¢å¼º
        try:
            expired_boosts = self._cleanup_expired_exploration_boosts()
            maintenance_result["tasks_executed"].append("boost_cleanup")
            maintenance_result["cleanup_results"]["expired_boosts"] = expired_boosts
            logger.info(f"âœ… æ¢ç´¢å¢å¼ºæ¸…ç†å®Œæˆ: æ¸…ç† {expired_boosts['cleaned_count']} ä¸ªè¿‡æœŸå¢å¼º")
        except Exception as e:
            logger.error(f"âŒ æ¢ç´¢å¢å¼ºæ¸…ç†å¤±è´¥: {e}")
            maintenance_result["cleanup_results"]["expired_boosts"] = {"error": str(e)}
        
        # 3. ç®¡ç†æ·˜æ±°å†å²è®°å½•å¤§å°
        try:
            history_cleanup = self._manage_culled_history()
            maintenance_result["tasks_executed"].append("history_management")
            maintenance_result["cleanup_results"]["history"] = history_cleanup
            if history_cleanup.get("trimmed", 0) > 0:
                logger.info(f"âœ… æ·˜æ±°å†å²ç®¡ç†å®Œæˆ: ä¿®å‰ª {history_cleanup['trimmed']} æ¡è®°å½•")
        except Exception as e:
            logger.error(f"âŒ æ·˜æ±°å†å²ç®¡ç†å¤±è´¥: {e}")
            maintenance_result["cleanup_results"]["history"] = {"error": str(e)}
        
        # 4. ç”Ÿæˆåˆ†æå¿«ç…§
        try:
            analytics_snapshot = self.get_trial_ground_analytics()
            maintenance_result["tasks_executed"].append("analytics_snapshot")
            maintenance_result["analytics_snapshot"] = analytics_snapshot
            logger.info("âœ… åˆ†æå¿«ç…§ç”Ÿæˆå®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¿«ç…§ç”Ÿæˆå¤±è´¥: {e}")
            maintenance_result["analytics_snapshot"] = {"error": str(e)}
        
        logger.info(f"ğŸ”§ è¯•ç‚¼åœºç»´æŠ¤å®Œæˆï¼Œæ‰§è¡Œäº† {len(maintenance_result['tasks_executed'])} ä¸ªä»»åŠ¡")
        
        return maintenance_result
    
    def _cleanup_expired_exploration_boosts(self) -> Dict[str, Any]:
        """æ¸…ç†è¿‡æœŸçš„æ¢ç´¢å¢å¼º"""
        cleanup_result = {
            "cleaned_count": 0,
            "expired_paths": []
        }
        
        expired_paths = []
        for strategy_id, remaining_rounds in list(self.trial_ground["exploration_boost_active"].items()):
            if remaining_rounds <= 0:
                expired_paths.append(strategy_id)
        
        for strategy_id in expired_paths:
            del self.trial_ground["exploration_boost_active"][strategy_id]
            cleanup_result["expired_paths"].append(strategy_id)
        
        cleanup_result["cleaned_count"] = len(expired_paths)
        return cleanup_result
    
    def _manage_culled_history(self) -> Dict[str, Any]:
        """ç®¡ç†æ·˜æ±°å†å²è®°å½•å¤§å°"""
        history_result = {
            "current_count": len(self.trial_ground["culled_paths"]),
            "max_allowed": self.trial_config["max_culled_history"],
            "trimmed": 0
        }
        
        if history_result["current_count"] > history_result["max_allowed"]:
            # åªä¿ç•™æœ€æ–°çš„è®°å½•
            excess = history_result["current_count"] - history_result["max_allowed"]
            self.trial_ground["culled_paths"] = self.trial_ground["culled_paths"][-history_result["max_allowed"]:]
            history_result["trimmed"] = excess
            
            logger.info(f"ğŸ“š æ·˜æ±°å†å²è®°å½•ä¿®å‰ª: ä¿ç•™æœ€æ–° {history_result['max_allowed']} æ¡, åˆ é™¤ {excess} æ¡æ—§è®°å½•")
        
        return history_result
    
    def reset_path_trial_status(self, strategy_id: str) -> Dict[str, Any]:
        """
        ğŸ”„ é‡ç½®æŒ‡å®šè·¯å¾„çš„è¯•ç‚¼çŠ¶æ€
        
        Args:
            strategy_id: ç­–ç•¥ID
            
        Returns:
            é‡ç½®ç»“æœ
        """
        reset_result = {
            "strategy_id": strategy_id,
            "actions_taken": [],
            "success": False
        }
        
        try:
            # ä»å„ç§å€™é€‰åå•ä¸­ç§»é™¤
            if strategy_id in self.trial_ground["culling_candidates"]:
                self.trial_ground["culling_candidates"].remove(strategy_id)
                reset_result["actions_taken"].append("removed_from_culling_candidates")
            
            if strategy_id in self.trial_ground["promotion_candidates"]:
                self.trial_ground["promotion_candidates"].remove(strategy_id)
                reset_result["actions_taken"].append("removed_from_promotion_candidates")
            
            # é‡ç½®ç›‘æ§æ•°æ®
            if strategy_id in self.trial_ground["performance_watch_list"]:
                del self.trial_ground["performance_watch_list"][strategy_id]
                reset_result["actions_taken"].append("cleared_watch_list_entry")
            
            # é‡ç½®æ¢ç´¢å¢å¼º
            if strategy_id in self.trial_ground["exploration_boost_active"]:
                del self.trial_ground["exploration_boost_active"][strategy_id]
                reset_result["actions_taken"].append("cleared_exploration_boost")
            
            # é‡ç½®å†³ç­–è‡‚ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if strategy_id in self.path_arms:
                arm = self.path_arms[strategy_id]
                # ä¿ç•™åŸºæœ¬ç»“æ„ï¼Œä½†é‡ç½®ç»Ÿè®¡
                arm.successes = 0
                arm.failures = 0
                arm.total_reward = 0.0
                arm.recent_results = []
                arm.activation_count = 0
                reset_result["actions_taken"].append("reset_decision_arm_stats")
            
            reset_result["success"] = True
            logger.info(f"ğŸ”„ è·¯å¾„ {strategy_id} è¯•ç‚¼çŠ¶æ€å·²é‡ç½®: {', '.join(reset_result['actions_taken'])}")
            
        except Exception as e:
            reset_result["error"] = str(e)
            logger.error(f"âŒ é‡ç½®è·¯å¾„ {strategy_id} è¯•ç‚¼çŠ¶æ€å¤±è´¥: {e}")
        
        return reset_result
    
    def force_promote_to_golden(self, strategy_id: str, reason: str = "manual_promotion") -> Dict[str, Any]:
        """
        ğŸ† å¼ºåˆ¶æå‡è·¯å¾„ä¸ºé»„é‡‘æ¨¡æ¿
        
        Args:
            strategy_id: ç­–ç•¥ID
            reason: æå‡åŸå› 
            
        Returns:
            æå‡ç»“æœ
        """
        promotion_result = {
            "strategy_id": strategy_id,
            "reason": reason,
            "success": False,
            "previous_status": {}
        }
        
        try:
            if strategy_id not in self.path_arms:
                promotion_result["error"] = "ç­–ç•¥IDä¸å­˜åœ¨"
                return promotion_result
            
            # è®°å½•ä¹‹å‰çš„çŠ¶æ€
            arm = self.path_arms[strategy_id]
            promotion_result["previous_status"] = {
                "success_rate": arm.success_rate,
                "activations": arm.activation_count,
                "was_golden": strategy_id in self.golden_templates
            }
            
            # æ‰§è¡Œæå‡
            if strategy_id not in self.golden_templates:
                self.golden_templates.add(strategy_id)
                logger.info(f"ğŸ† è·¯å¾„ {strategy_id} å·²è¢«å¼ºåˆ¶æå‡ä¸ºé»„é‡‘æ¨¡æ¿: {reason}")
                
                # ä»å€™é€‰åå•ä¸­ç§»é™¤
                self.trial_ground["promotion_candidates"].discard(strategy_id)
                self.trial_ground["culling_candidates"].discard(strategy_id)
                
                # è®°å½•æå‡å†å²
                if "promotion_history" not in self.trial_ground:
                    self.trial_ground["promotion_history"] = []
                
                self.trial_ground["promotion_history"].append({
                    "strategy_id": strategy_id,
                    "promoted_at": time.time(),
                    "reason": reason,
                    "promotion_type": "manual_force",
                    "stats_at_promotion": {
                        "success_rate": arm.success_rate,
                        "activations": arm.activation_count
                    }
                })
                
                promotion_result["success"] = True
            else:
                promotion_result["error"] = "è·¯å¾„å·²ç»æ˜¯é»„é‡‘æ¨¡æ¿"
                logger.warning(f"âš ï¸ è·¯å¾„ {strategy_id} å·²ç»æ˜¯é»„é‡‘æ¨¡æ¿ï¼Œæ— éœ€é‡å¤æå‡")
        
        except Exception as e:
            promotion_result["error"] = str(e)
            logger.error(f"âŒ å¼ºåˆ¶æå‡è·¯å¾„ {strategy_id} å¤±è´¥: {e}")
        
        return promotion_result
    
    def revoke_golden_status(self, strategy_id: str, reason: str = "manual_revocation") -> Dict[str, Any]:
        """
        ğŸ”» æ’¤é”€é»„é‡‘æ¨¡æ¿çŠ¶æ€
        
        Args:
            strategy_id: ç­–ç•¥ID
            reason: æ’¤é”€åŸå› 
            
        Returns:
            æ’¤é”€ç»“æœ
        """
        revocation_result = {
            "strategy_id": strategy_id,
            "reason": reason,
            "success": False
        }
        
        try:
            if strategy_id in self.golden_templates:
                self.golden_templates.remove(strategy_id)
                logger.info(f"ğŸ”» è·¯å¾„ {strategy_id} çš„é»„é‡‘æ¨¡æ¿çŠ¶æ€å·²æ’¤é”€: {reason}")
                
                # è®°å½•æ’¤é”€å†å²
                if "revocation_history" not in self.trial_ground:
                    self.trial_ground["revocation_history"] = []
                
                arm = self.path_arms.get(strategy_id)
                self.trial_ground["revocation_history"].append({
                    "strategy_id": strategy_id,
                    "revoked_at": time.time(),
                    "reason": reason,
                    "stats_at_revocation": {
                        "success_rate": arm.success_rate if arm else 0.0,
                        "activations": arm.activation_count if arm else 0
                    }
                })
                
                revocation_result["success"] = True
            else:
                revocation_result["error"] = "è·¯å¾„ä¸æ˜¯é»„é‡‘æ¨¡æ¿"
                logger.warning(f"âš ï¸ è·¯å¾„ {strategy_id} ä¸æ˜¯é»„é‡‘æ¨¡æ¿ï¼Œæ— æ³•æ’¤é”€")
        
        except Exception as e:
            revocation_result["error"] = str(e)
            logger.error(f"âŒ æ’¤é”€è·¯å¾„ {strategy_id} é»„é‡‘çŠ¶æ€å¤±è´¥: {e}")
        
        return revocation_result
    
    def select_best_path(self, paths: List[ReasoningPath], algorithm: str = 'auto') -> ReasoningPath:
        """
        é˜¶æ®µä¸‰æ ¸å¿ƒæ–¹æ³•ï¼šä»æ€ç»´è·¯å¾„åˆ—è¡¨ä¸­é€‰æ‹©æœ€ä¼˜è·¯å¾„ï¼ˆé›†æˆé»„é‡‘æ¨¡æ¿ç³»ç»Ÿï¼‰
        
        Args:
            paths: æ€ç»´è·¯å¾„åˆ—è¡¨
            algorithm: ä½¿ç”¨çš„ç®—æ³• ('thompson_sampling', 'ucb_variant', 'epsilon_greedy', 'auto')
            
        Returns:
            é€‰æ‹©çš„æœ€ä¼˜æ€ç»´è·¯å¾„
        """
        if not paths:
            raise ValueError("è·¯å¾„åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if len(paths) == 1:
            logger.info(f"ğŸ¯ åªæœ‰ä¸€ä¸ªè·¯å¾„ï¼Œç›´æ¥é€‰æ‹©: {paths[0].path_type}")
            return paths[0]
        
        self.total_path_selections += 1
        logger.info(f"ğŸ›¤ï¸ å¼€å§‹ç¬¬ {self.total_path_selections} æ¬¡è·¯å¾„é€‰æ‹©ï¼Œå€™é€‰è·¯å¾„: {len(paths)}ä¸ª")
        
        # ğŸ† é»„é‡‘æ¨¡æ¿ä¼˜å…ˆæ£€æŸ¥ï¼šåœ¨MABç®—æ³•å‰å…ˆæ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„é»„é‡‘æ¨¡æ¿
        golden_match = self._check_golden_template_match(paths)
        if golden_match:
            selected_path = golden_match['path']
            template_id = golden_match['template_id']
            match_score = golden_match['match_score']
            
            # æ›´æ–°é»„é‡‘æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡
            self.template_usage_stats[template_id] += 1
            
            # è®°å½•æ¨¡æ¿åŒ¹é…å†å²
            self.template_match_history.append({
                'template_id': template_id,
                'path_id': selected_path.path_id,
                'path_type': selected_path.path_type,
                'match_score': match_score,
                'timestamp': time.time(),
                'selection_round': self.total_path_selections
            })
            
            logger.info(f"ğŸ† é»„é‡‘æ¨¡æ¿åŒ¹é…æˆåŠŸï¼")
            logger.info(f"   æ¨¡æ¿ID: {template_id}")
            logger.info(f"   åŒ¹é…è·¯å¾„: {selected_path.path_type}")
            logger.info(f"   åŒ¹é…åˆ†æ•°: {match_score:.3f}")
            logger.info(f"   è·³è¿‡MABç®—æ³•ï¼Œç›´æ¥ä½¿ç”¨é»„é‡‘æ¨¡æ¿")
            
            return selected_path
        
        # ğŸ”§ åŠ¨æ€åˆ›å»ºç­–ç•¥ï¼šåœ¨é€‰æ‹©è·¯å¾„æ—¶ç¡®ä¿æ‰€æœ‰ç­–ç•¥å†³ç­–è‡‚éƒ½å­˜åœ¨
        available_arms = []
        strategy_to_path_mapping = {}  # ç­–ç•¥IDåˆ°è·¯å¾„å®ä¾‹çš„æ˜ å°„
        
        for path in paths:
            strategy_id = path.strategy_id
            strategy_to_path_mapping[strategy_id] = path  # è®°å½•æ˜ å°„å…³ç³»
            
            # ğŸ”§ åŠ¨æ€åˆ›å»ºï¼šç¡®ä¿ç­–ç•¥å†³ç­–è‡‚å­˜åœ¨
            arm = self._create_strategy_arm_if_missing(strategy_id, path.path_type)
            available_arms.append(arm)
            
            logger.debug(f"âœ… ç­–ç•¥å†³ç­–è‡‚å°±ç»ª: {strategy_id} ({path.path_type})")
            logger.debug(f"   å¯¹åº”å®ä¾‹: {path.instance_id}")
        
        # è‡ªåŠ¨é€‰æ‹©ç®—æ³•
        if algorithm == 'auto':
            algorithm = self._select_best_algorithm_for_paths()
        
        # æ ¹æ®é€‰æ‹©çš„ç®—æ³•è¿›è¡Œå†³ç­–
        try:
            if algorithm == 'thompson_sampling':
                best_arm = self._thompson_sampling_for_paths(available_arms)
            elif algorithm == 'ucb_variant':
                best_arm = self._ucb_variant_for_paths(available_arms)
            elif algorithm == 'epsilon_greedy':
                best_arm = self._epsilon_greedy_for_paths(available_arms)
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥ç®—æ³• {algorithm}ï¼Œä½¿ç”¨Thompsoné‡‡æ ·")
                best_arm = self._thompson_sampling_for_paths(available_arms)
            
            # æ›´æ–°ä½¿ç”¨æ—¶é—´å’Œæ¿€æ´»æ¬¡æ•°
            best_arm.last_used = time.time()
            best_arm.activation_count += 1
            
            # ğŸ­ è¯•ç‚¼åœºæ›´æ–°ï¼šæ›´æ–°æ¢ç´¢å¢å¼ºçŠ¶æ€
            self._update_exploration_boost(best_arm.path_id)
            
            # ğŸ¯ ä¿®å¤ï¼šåŸºäºç­–ç•¥IDæ‰¾åˆ°å¯¹åº”çš„è·¯å¾„å®ä¾‹
            selected_path = strategy_to_path_mapping.get(best_arm.path_id)
            
            if selected_path is None:
                # å…¼å®¹æ€§ï¼šå¦‚æœæ˜ å°„å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹å¼
                logger.warning(f"âš ï¸ ç­–ç•¥æ˜ å°„å¤±è´¥: {best_arm.path_id}")
                for path in paths:
                    strategy_id = getattr(path, 'strategy_id', None)
                    if strategy_id == best_arm.path_id:
                        selected_path = path
                        break
                
                if selected_path is None:
                    logger.error(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„è·¯å¾„ç­–ç•¥: {best_arm.path_id}")
                    selected_path = paths[0]  # å›é€€åˆ°ç¬¬ä¸€ä¸ªè·¯å¾„
            
            # è®°å½•é€‰æ‹©å†å²
            self.path_selection_history.append({
                'path_id': best_arm.path_id,
                'path_type': selected_path.path_type,
                'algorithm': algorithm,
                'timestamp': time.time(),
                'selection_round': self.total_path_selections
            })
            
            logger.info(f"ğŸ¯ ä½¿ç”¨ {algorithm} é€‰æ‹©è·¯å¾„: {selected_path.path_type} (ID: {best_arm.path_id})")
            return selected_path
            
        except Exception as e:
            logger.error(f"âŒ MABè·¯å¾„é€‰æ‹©ç®—æ³•æ‰§è¡Œå¤±è´¥: {e}")
            # å›é€€åˆ°éšæœºé€‰æ‹©
            selected_path = np.random.choice(paths)
            logger.info(f"ğŸ”„ å›é€€åˆ°éšæœºé€‰æ‹©è·¯å¾„: {selected_path.path_type}")
            return selected_path
    
    def select_best_tool(self, available_tools: List[str], algorithm: str = 'auto') -> str:
        """
        ğŸ”§ æ–°å¢ï¼šä»å¯ç”¨å·¥å…·åˆ—è¡¨ä¸­é€‰æ‹©æœ€ä¼˜å·¥å…·
        
        Args:
            available_tools: å¯ç”¨å·¥å…·åç§°åˆ—è¡¨
            algorithm: ä½¿ç”¨çš„ç®—æ³• ('thompson_sampling', 'ucb_variant', 'epsilon_greedy', 'auto')
            
        Returns:
            é€‰æ‹©çš„æœ€ä¼˜å·¥å…·åç§°
        """
        if not available_tools:
            raise ValueError("å·¥å…·åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if len(available_tools) == 1:
            logger.info(f"ğŸ”§ åªæœ‰ä¸€ä¸ªå·¥å…·ï¼Œç›´æ¥é€‰æ‹©: {available_tools[0]}")
            return available_tools[0]
        
        self.total_tool_selections += 1
        logger.info(f"ğŸ”§ å¼€å§‹ç¬¬ {self.total_tool_selections} æ¬¡å·¥å…·é€‰æ‹©ï¼Œå€™é€‰å·¥å…·: {len(available_tools)}ä¸ª")
        
        # ğŸ”§ åŠ¨æ€åˆ›å»ºï¼šç¡®ä¿æ‰€æœ‰å·¥å…·çš„å†³ç­–è‡‚éƒ½å­˜åœ¨
        available_arms = []
        tool_to_arm_mapping = {}  # å·¥å…·åç§°åˆ°å†³ç­–è‡‚çš„æ˜ å°„
        
        for tool_name in available_tools:
            tool_id = tool_name  # ä½¿ç”¨å·¥å…·åç§°ä½œä¸ºID
            tool_to_arm_mapping[tool_name] = tool_id
            
            # ğŸ”§ åŠ¨æ€åˆ›å»ºï¼šç¡®ä¿å·¥å…·å†³ç­–è‡‚å­˜åœ¨
            arm = self._create_tool_arm_if_missing(tool_id, tool_name)
            available_arms.append(arm)
            
            logger.debug(f"âœ… å·¥å…·å†³ç­–è‡‚å°±ç»ª: {tool_id} ({tool_name})")
        
        # è‡ªåŠ¨é€‰æ‹©ç®—æ³•
        if algorithm == 'auto':
            algorithm = self._select_best_algorithm_for_tools()
        
        # æ ¹æ®é€‰æ‹©çš„ç®—æ³•è¿›è¡Œå†³ç­–
        try:
            if algorithm == 'thompson_sampling':
                best_arm = self._thompson_sampling_for_tools(available_arms)
            elif algorithm == 'ucb_variant':
                best_arm = self._ucb_variant_for_tools(available_arms)
            elif algorithm == 'epsilon_greedy':
                best_arm = self._epsilon_greedy_for_tools(available_arms)
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥ç®—æ³• {algorithm}ï¼Œä½¿ç”¨Thompsoné‡‡æ ·")
                best_arm = self._thompson_sampling_for_tools(available_arms)
            
            # æ›´æ–°ä½¿ç”¨æ—¶é—´å’Œæ¿€æ´»æ¬¡æ•°
            best_arm.last_used = time.time()
            best_arm.activation_count += 1
            
            # ğŸ¯ æ‰¾åˆ°å¯¹åº”çš„å·¥å…·åç§°
            selected_tool = best_arm.option  # å·¥å…·åç§°å­˜å‚¨åœ¨optionå­—æ®µä¸­
            
            # è®°å½•é€‰æ‹©å†å²
            self.tool_selection_history.append({
                'tool_id': best_arm.path_id,
                'tool_name': selected_tool,
                'algorithm': algorithm,
                'timestamp': time.time(),
                'selection_round': self.total_tool_selections
            })
            
            logger.info(f"ğŸ”§ ä½¿ç”¨ {algorithm} é€‰æ‹©å·¥å…·: {selected_tool} (ID: {best_arm.path_id})")
            return selected_tool
            
        except Exception as e:
            logger.error(f"âŒ MABå·¥å…·é€‰æ‹©ç®—æ³•æ‰§è¡Œå¤±è´¥: {e}")
            # å›é€€åˆ°éšæœºé€‰æ‹©
            selected_tool = np.random.choice(available_tools)
            logger.info(f"ğŸ”„ å›é€€åˆ°éšæœºé€‰æ‹©å·¥å…·: {selected_tool}")
            return selected_tool
    
    def is_tool_cold(self, tool_name: str) -> Dict[str, any]:
        """
        ğŸ” åˆ¤æ–­å·¥å…·æ˜¯å¦å¤„äºå†·å¯åŠ¨çŠ¶æ€
        
        è¿™ä¸ªæ–¹æ³•æ˜¯MABConvergerçš„"è‡ªæˆ‘è®¤çŸ¥"èƒ½åŠ›ï¼Œå½“MainControllerè¯¢é—®æ—¶ï¼Œ
        å®ƒèƒ½æ˜ç¡®å›ç­”ï¼š"æˆ‘æ¨èçš„è¿™ä¸ªå·¥å…·ï¼Œæˆ‘è‡ªå·±ç†Ÿä¸ç†Ÿï¼Ÿ"
        
        Args:
            tool_name: å·¥å…·åç§°
            
        Returns:
            DictåŒ…å«è¯¦ç»†çš„å†·å¯åŠ¨åˆ†æç»“æœ:
            {
                'is_cold_start': bool,      # æ˜¯å¦å¤„äºå†·å¯åŠ¨çŠ¶æ€
                'cold_score': float,        # å†·å¯åŠ¨å¾—åˆ† (0-1, è¶Šé«˜è¶Š"å†·")
                'confidence': float,        # ç»éªŒå¯ä¿¡åº¦ (0-1, è¶Šé«˜è¶Šå¯ä¿¡)
                'analysis': {
                    'usage_count': int,     # ä½¿ç”¨æ¬¡æ•°
                    'reliability_score': float,  # å¯é æ€§åˆ†æ•°
                    'idle_hours': float,    # ç©ºé—²æ—¶é—´(å°æ—¶)
                    'sample_size': int      # æ ·æœ¬æ•°é‡
                },
                'recommendation': str,      # æ¨èæ¨¡å¼ ('experience'/'exploration')
                'reason': str              # åˆ¤æ–­ç†ç”±
            }
        """
        logger.debug(f"ğŸ” å¼€å§‹å†·å¯åŠ¨æ£€æµ‹: å·¥å…· '{tool_name}'")
        
        # è·å–å†·å¯åŠ¨é…ç½®
        cold_start_config = MAB_CONFIG["cold_start_threshold"]
        detection_weights = cold_start_config["detection_weights"]
        
        # è·å–å·¥å…·çš„å†³ç­–è‡‚
        tool_arm = self.tool_arms.get(tool_name)
        
        if not tool_arm:
            # å®Œå…¨æœªä½¿ç”¨çš„å·¥å…· - ç»å¯¹å†·å¯åŠ¨
            logger.debug(f"ğŸ†• å·¥å…· '{tool_name}' ä»æœªä½¿ç”¨è¿‡ï¼Œåˆ¤å®šä¸ºå†·å¯åŠ¨")
            return {
                'is_cold_start': True,
                'cold_score': 1.0,
                'confidence': 0.0,
                'analysis': {
                    'usage_count': 0,
                    'reliability_score': 0.0,
                    'idle_hours': float('inf'),
                    'sample_size': 0
                },
                'recommendation': 'exploration',
                'reason': 'å·¥å…·ä»æœªè¢«ä½¿ç”¨è¿‡ï¼Œæ— ä»»ä½•ç»éªŒæ•°æ®'
            }
        
        # è®¡ç®—å„ä¸ªå†·å¯åŠ¨å› å­
        analysis = self._calculate_cold_start_factors(tool_arm, cold_start_config)
        
        # è®¡ç®—åŠ æƒå†·å¯åŠ¨å¾—åˆ†
        cold_score = (
            analysis['usage_factor'] * detection_weights['usage_frequency'] +
            analysis['reliability_factor'] * detection_weights['reliability'] +
            analysis['recency_factor'] * detection_weights['recency'] +
            analysis['sample_factor'] * detection_weights['sample_sufficiency']
        )
        
        # åˆ¤å®šæ˜¯å¦å†·å¯åŠ¨
        exploration_threshold = cold_start_config["exploration_trigger_threshold"]
        is_cold = cold_score > exploration_threshold
        
        # ç”Ÿæˆåˆ¤æ–­ç†ç”±
        reason = self._generate_cold_start_reason(analysis, cold_score, exploration_threshold)
        
        result = {
            'is_cold_start': is_cold,
            'cold_score': round(cold_score, 3),
            'confidence': round(1.0 - cold_score, 3),
            'analysis': {
                'usage_count': analysis['usage_count'],
                'reliability_score': round(analysis['reliability_score'], 3),
                'idle_hours': round(analysis['idle_hours'], 2),
                'sample_size': analysis['sample_size']
            },
            'recommendation': 'exploration' if is_cold else 'experience',
            'reason': reason
        }
        
        logger.info(f"ğŸ” å†·å¯åŠ¨æ£€æµ‹å®Œæˆ: {tool_name} -> "
                   f"{'å†·å¯åŠ¨' if is_cold else 'ç»éªŒä¸°å¯Œ'} "
                   f"(å¾—åˆ†: {cold_score:.3f}, ç½®ä¿¡åº¦: {result['confidence']:.3f})")
        
        return result
    
    def _calculate_cold_start_factors(self, tool_arm: EnhancedDecisionArm, 
                                    cold_start_config: Dict[str, any]) -> Dict[str, any]:
        """
        è®¡ç®—å†·å¯åŠ¨å„ä¸ªå› å­
        
        Args:
            tool_arm: å·¥å…·å†³ç­–è‡‚
            cold_start_config: å†·å¯åŠ¨é…ç½®
            
        Returns:
            åŒ…å«å„ä¸ªå› å­çš„åˆ†æç»“æœ
        """
        current_time = time.time()
        
        # 1. ä½¿ç”¨é¢‘ç‡å› å­ (ä½¿ç”¨æ¬¡æ•°è¶Šå°‘ï¼Œåˆ†æ•°è¶Šé«˜)
        usage_count = tool_arm.activation_count
        min_usage = cold_start_config["min_usage_count"]
        usage_factor = max(0.0, 1.0 - usage_count / max(min_usage, 1))
        
        # 2. å¯é æ€§å› å­ (æˆåŠŸç‡ä¸ç¨³å®šæˆ–æ ·æœ¬å°‘æ—¶åˆ†æ•°é«˜)
        total_samples = tool_arm.success_count + tool_arm.failure_count
        if total_samples >= 3:
            reliability_score = tool_arm.success_rate
            # æ ·æœ¬æ•°è°ƒæ•´ï¼šæ ·æœ¬è¶Šå°‘ï¼Œå¯é æ€§è¶Šä½
            sample_adjustment = min(1.0, total_samples / 10.0)  # 10ä¸ªæ ·æœ¬è§†ä¸ºå……è¶³
            adjusted_reliability = reliability_score * sample_adjustment
        else:
            adjusted_reliability = 0.0  # æ ·æœ¬å¤ªå°‘ï¼Œä¸å¯é 
        
        min_reliability = cold_start_config["min_reliability_score"]
        reliability_factor = max(0.0, 1.0 - adjusted_reliability / max(min_reliability, 0.1))
        
        # 3. æœ€è¿‘ä½¿ç”¨å› å­ (æ—¶é—´è¶Šä¹…ï¼Œåˆ†æ•°è¶Šé«˜)
        if tool_arm.last_used > 0:
            idle_hours = (current_time - tool_arm.last_used) / 3600
        else:
            idle_hours = float('inf')
        
        max_idle = cold_start_config["max_idle_hours"]
        recency_factor = min(1.0, idle_hours / max(max_idle, 1))
        
        # 4. æ ·æœ¬å……è¶³æ€§å› å­ (æ ·æœ¬è¶Šå°‘ï¼Œåˆ†æ•°è¶Šé«˜)
        min_samples = cold_start_config["min_sample_size"]
        sample_factor = max(0.0, 1.0 - total_samples / max(min_samples, 1))
        
        return {
            'usage_count': usage_count,
            'usage_factor': usage_factor,
            'reliability_score': adjusted_reliability,
            'reliability_factor': reliability_factor,
            'idle_hours': idle_hours if idle_hours != float('inf') else -1,
            'recency_factor': recency_factor,
            'sample_size': total_samples,
            'sample_factor': sample_factor
        }
    
    def _generate_cold_start_reason(self, analysis: Dict[str, any], 
                                   cold_score: float, threshold: float) -> str:
        """
        ç”Ÿæˆå†·å¯åŠ¨åˆ¤æ–­çš„è¯¦ç»†ç†ç”±
        
        Args:
            analysis: åˆ†æç»“æœ
            cold_score: å†·å¯åŠ¨å¾—åˆ†
            threshold: åˆ¤å®šé˜ˆå€¼
            
        Returns:
            åˆ¤æ–­ç†ç”±å­—ç¬¦ä¸²
        """
        reasons = []
        
        # ä½¿ç”¨é¢‘ç‡åˆ†æ
        if analysis['usage_factor'] > 0.7:
            reasons.append(f"ä½¿ç”¨æ¬¡æ•°è¿‡å°‘({analysis['usage_count']}æ¬¡)")
        elif analysis['usage_factor'] > 0.3:
            reasons.append(f"ä½¿ç”¨ç»éªŒæœ‰é™({analysis['usage_count']}æ¬¡)")
        
        # å¯é æ€§åˆ†æ
        if analysis['reliability_factor'] > 0.6:
            reasons.append(f"æ€§èƒ½æ•°æ®ä¸å¯é (å¯é æ€§:{analysis['reliability_score']:.2f})")
        elif analysis['reliability_factor'] > 0.3:
            reasons.append(f"æ€§èƒ½æ•°æ®ä¸å¤Ÿç¨³å®š")
        
        # æœ€è¿‘ä½¿ç”¨åˆ†æ
        if analysis['idle_hours'] > 72:
            reasons.append(f"é•¿æ—¶é—´æœªä½¿ç”¨({analysis['idle_hours']:.1f}å°æ—¶)")
        elif analysis['idle_hours'] > 24:
            reasons.append(f"è¾ƒé•¿æ—¶é—´æœªä½¿ç”¨")
        
        # æ ·æœ¬æ•°åˆ†æ
        if analysis['sample_factor'] > 0.7:
            reasons.append(f"æ ·æœ¬æ•°æ®ä¸è¶³({analysis['sample_size']}ä¸ª)")
        
        if not reasons:
            if cold_score > threshold:
                reasons.append("ç»¼åˆè¯„ä¼°æ˜¾ç¤ºç¼ºä¹è¶³å¤Ÿç»éªŒ")
            else:
                reasons.append("å…·æœ‰å……è¶³çš„ä½¿ç”¨ç»éªŒå’Œå¯é æ•°æ®")
        
        # ç»„åˆç†ç”±
        if cold_score > threshold:
            return f"å†·å¯åŠ¨çŠ¶æ€: {'; '.join(reasons)} (å¾—åˆ†:{cold_score:.3f} > {threshold})"
        else:
            return f"ç»éªŒä¸°å¯Œ: {'; '.join(reasons)} (å¾—åˆ†:{cold_score:.3f} â‰¤ {threshold})"
    
    def _thompson_sampling_for_paths(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹æ€ç»´è·¯å¾„çš„Thompsoné‡‡æ ·ç®—æ³• - ğŸŒŸ å¢å¼ºç‰ˆï¼šæ”¯æŒæ¢ç´¢å¢å¼º"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„è·¯å¾„å†³ç­–è‡‚")
        
        best_arm = None
        best_score = -1
        
        logger.debug(f"ğŸ² Thompsoné‡‡æ ·è·¯å¾„é€‰æ‹©ï¼Œå€™é€‰è·¯å¾„: {len(arms)}ä¸ª")
        
        for arm in arms:
            # ä½¿ç”¨Betaåˆ†å¸ƒè¿›è¡ŒThompsoné‡‡æ ·
            alpha = arm.success_count + 1
            beta = arm.failure_count + 1
            
            # ä»Betaåˆ†å¸ƒä¸­é‡‡æ ·
            sampled_value = np.random.beta(alpha, beta)
            
            # è·¯å¾„çº§åˆ«çš„å¥–åŠ±è€ƒè™‘
            if arm.rl_reward_history:
                avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                # å°†å¥–åŠ±è°ƒæ•´åˆ°0-1èŒƒå›´
                normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                sampled_value = sampled_value * 0.8 + normalized_reward * 0.2
            
            # ğŸŒŸ æ¢ç´¢å¢å¼ºï¼šä¸ºæ–°å­¦ä¹ è·¯å¾„æä¾›é¢å¤–æœºä¼š
            exploration_boost = self.get_exploration_boost(arm.path_id)
            if exploration_boost > 1.0:
                sampled_value *= exploration_boost
                logger.debug(f"   ğŸš€ è·¯å¾„ {arm.path_id} è·å¾—æ¢ç´¢å¢å¼º: {exploration_boost:.3f}x")
            
            # è·¯å¾„å¤šæ ·æ€§è€ƒè™‘ï¼šå‡å°‘è¿‡åº¦ä¾èµ–å•ä¸€è·¯å¾„
            usage_penalty = min(0.1, arm.activation_count / (self.total_path_selections + 1) * 0.2)
            sampled_value = max(0, sampled_value - usage_penalty)
            
            logger.debug(f"   è·¯å¾„ {arm.path_id}: sampled={sampled_value:.3f}, Î±={alpha}, Î²={beta}, boost={exploration_boost:.3f}")
            
            if sampled_value > best_score:
                best_score = sampled_value
                best_arm = arm
        
        logger.debug(f"ğŸ† Thompsoné‡‡æ ·é€‰æ‹©: {best_arm.path_id} (å¾—åˆ†: {best_score:.3f})")
        return best_arm
    
    def _ucb_variant_for_paths(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹æ€ç»´è·¯å¾„çš„UCB (Upper Confidence Bound) å˜ç§ç®—æ³• - ğŸŒŸ å¢å¼ºç‰ˆï¼šæ”¯æŒæ¢ç´¢å¢å¼º"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„è·¯å¾„å†³ç­–è‡‚")
        
        total_rounds = sum(arm.activation_count for arm in arms)
        if total_rounds == 0:
            # ç¬¬ä¸€è½®éšæœºé€‰æ‹©
            selected_arm = np.random.choice(arms)
            logger.debug(f"ğŸ² UCBé¦–è½®éšæœºé€‰æ‹©è·¯å¾„: {selected_arm.path_id}")
            return selected_arm
        
        best_arm = None
        best_ucb_value = -float('inf')
        
        logger.debug(f"ğŸ“Š UCBè·¯å¾„é€‰æ‹©ï¼Œæ€»è½®æ•°: {total_rounds}")
        
        for arm in arms:
            if arm.activation_count == 0:
                # æœªå°è¯•è¿‡çš„è·¯å¾„ä¼˜å…ˆé€‰æ‹©ï¼Œå­¦ä¹ è·¯å¾„äº«æœ‰æ›´é«˜ä¼˜å…ˆçº§
                exploration_boost = self.get_exploration_boost(arm.path_id)
                if exploration_boost > 1.0:
                    logger.debug(f"ğŸ†•ğŸš€ ä¼˜å…ˆé€‰æ‹©æœªä½¿ç”¨çš„å­¦ä¹ è·¯å¾„: {arm.path_id} (å¢å¼º: {exploration_boost:.3f}x)")
                else:
                    logger.debug(f"ğŸ†• ä¼˜å…ˆé€‰æ‹©æœªä½¿ç”¨è·¯å¾„: {arm.path_id}")
                return arm
            
            # è®¡ç®—UCBå€¼
            confidence_bound = np.sqrt(2 * np.log(total_rounds) / arm.activation_count)
            
            # åŸºç¡€æˆåŠŸç‡
            base_value = arm.success_rate
            
            # è·¯å¾„çº§åˆ«çš„RLå¥–åŠ±è€ƒè™‘
            if arm.rl_reward_history:
                avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                base_value = base_value * 0.7 + normalized_reward * 0.3
            
            # ğŸŒŸ æ¢ç´¢å¢å¼ºï¼šä¸ºæ–°å­¦ä¹ è·¯å¾„æä¾›é¢å¤–UCBå¥–åŠ±
            exploration_boost = self.get_exploration_boost(arm.path_id)
            if exploration_boost > 1.0:
                base_value *= exploration_boost
                logger.debug(f"   ğŸš€ è·¯å¾„ {arm.path_id} è·å¾—UCBæ¢ç´¢å¢å¼º: {exploration_boost:.3f}x")
            
            # è·¯å¾„æ¢ç´¢å¥–åŠ±ï¼šé¼“åŠ±å°è¯•ä¸åŒæ€ç»´æ–¹å¼
            exploration_bonus = confidence_bound * 1.2  # å¢å¼ºæ¢ç´¢
            ucb_value = base_value + exploration_bonus
            
            logger.debug(f"   è·¯å¾„ {arm.path_id}: UCB={ucb_value:.3f}, base={base_value:.3f}, conf={confidence_bound:.3f}, boost={exploration_boost:.3f}")
            
            if ucb_value > best_ucb_value:
                best_ucb_value = ucb_value
                best_arm = arm
        
        logger.debug(f"ğŸ† UCBé€‰æ‹©è·¯å¾„: {best_arm.path_id} (UCBå€¼: {best_ucb_value:.3f})")
        return best_arm
    
    def _epsilon_greedy_for_paths(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹æ€ç»´è·¯å¾„çš„Epsilon-Greedyç®—æ³• - ğŸŒŸ å¢å¼ºç‰ˆï¼šæ”¯æŒæ¢ç´¢å¢å¼º"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„è·¯å¾„å†³ç­–è‡‚")
        
        # è·¯å¾„çº§åˆ«çš„åŠ¨æ€epsilonå€¼ï¼Œé¼“åŠ±æ€ç»´å¤šæ ·æ€§
        total_activations = sum(arm.activation_count for arm in arms)
        epsilon = max(0.1, 0.4 / (1 + total_activations * 0.008))  # æ¯”ä¼ ç»Ÿæ›´é«˜çš„æ¢ç´¢ç‡
        
        # ğŸŒŸ å­¦ä¹ è·¯å¾„å¢å¼ºï¼šå¦‚æœæœ‰å­¦ä¹ è·¯å¾„ï¼Œé€‚å½“æé«˜æ¢ç´¢ç‡
        has_boosted_paths = any(self.get_exploration_boost(arm.path_id) > 1.0 for arm in arms)
        if has_boosted_paths:
            epsilon = min(0.6, epsilon * 1.3)  # å¢å¼ºæ¢ç´¢ï¼Œç»™å­¦ä¹ è·¯å¾„æ›´å¤šæœºä¼š
        
        logger.debug(f"ğŸ¯ Epsilon-Greedyè·¯å¾„é€‰æ‹©ï¼ŒÎµ={epsilon:.3f} {'(å­¦ä¹ å¢å¼º)' if has_boosted_paths else ''}")
        
        # ä½¿ç”¨epsilonå†³å®šæ˜¯å¦æ¢ç´¢
        if np.random.random() < epsilon:
            # ğŸŒŸ æ™ºèƒ½æ¢ç´¢ï¼šä¼˜å…ˆé€‰æ‹©æœ‰æ¢ç´¢å¢å¼ºçš„è·¯å¾„
            boosted_arms = [arm for arm in arms if self.get_exploration_boost(arm.path_id) > 1.0]
            if boosted_arms and np.random.random() < 0.7:  # 70%æ¦‚ç‡é€‰æ‹©å¢å¼ºè·¯å¾„
                selected_arm = np.random.choice(boosted_arms)
                boost = self.get_exploration_boost(selected_arm.path_id)
                logger.debug(f"ğŸ”ğŸš€ æ™ºèƒ½æ¢ç´¢é€‰æ‹©å¢å¼ºè·¯å¾„: {selected_arm.path_id} (å¢å¼º: {boost:.3f}x)")
            else:
                # å¸¸è§„éšæœºæ¢ç´¢
                selected_arm = np.random.choice(arms)
                logger.debug(f"ğŸ” æ¢ç´¢æ¨¡å¼é€‰æ‹©è·¯å¾„: {selected_arm.path_id}")
            return selected_arm
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©å½“å‰æœ€å¥½çš„è·¯å¾„
            best_arm = None
            best_score = -float('inf')
            
            for arm in arms:
                # è·¯å¾„çº§åˆ«çš„ç»¼åˆè¯„åˆ†
                score = arm.success_rate
                
                # RLå¥–åŠ±æƒé‡
                if arm.rl_reward_history:
                    avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                    normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                    score = score * 0.6 + normalized_reward * 0.4
                
                # ğŸŒŸ æ¢ç´¢å¢å¼ºï¼šå³ä½¿åœ¨åˆ©ç”¨æ¨¡å¼ä¸‹ï¼Œä¹Ÿç»™äºˆå­¦ä¹ è·¯å¾„ä¸€å®šä¼˜åŠ¿
                exploration_boost = self.get_exploration_boost(arm.path_id)
                if exploration_boost > 1.0:
                    # åœ¨åˆ©ç”¨æ¨¡å¼ä¸‹ç»™å­¦ä¹ è·¯å¾„ä¸€ä¸ªå°çš„é¢å¤–åˆ†æ•°
                    score += (exploration_boost - 1.0) * 0.1  # è½»å¾®å¢å¼ºï¼Œé¿å…è¿‡åº¦åå‘
                    logger.debug(f"   ğŸš€ è·¯å¾„ {arm.path_id} è·å¾—åˆ©ç”¨æ¨¡å¼å¢å¼º: +{(exploration_boost - 1.0) * 0.1:.3f}")
                
                # è·¯å¾„ä½¿ç”¨é¢‘ç‡å¹³è¡¡ï¼šé¿å…è¿‡åº¦ä¾èµ–å•ä¸€æ€ç»´æ¨¡å¼
                usage_ratio = arm.activation_count / (total_activations + 1)
                if usage_ratio > 0.5:  # å¦‚æœæŸè·¯å¾„ä½¿ç”¨è¿‡äºé¢‘ç¹ï¼Œç¨å¾®é™ä½è¯„åˆ†
                    score *= 0.95
                
                logger.debug(f"   è·¯å¾„ {arm.path_id}: score={score:.3f}, usage_ratio={usage_ratio:.3f}, boost={exploration_boost:.3f}")
                
                if score > best_score:
                    best_score = score
                    best_arm = arm
            
            logger.debug(f"ğŸ† åˆ©ç”¨æ¨¡å¼é€‰æ‹©è·¯å¾„: {best_arm.path_id} (å¾—åˆ†: {best_score:.3f})")
            return best_arm if best_arm else arms[0]
    
    def _select_best_algorithm_for_paths(self) -> str:
        """
        ä¸ºè·¯å¾„é€‰æ‹©é€‰æ‹©æœ€ä½³ç®—æ³•
        
        Returns:
            æœ€ä½³ç®—æ³•åç§°
        """
        # å¦‚æœæ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨Thompsoné‡‡æ ·è¿›è¡Œæ¢ç´¢
        if self.total_path_selections < 15:
            logger.debug("ğŸ“Š æ ·æœ¬è¾ƒå°‘ï¼Œé€‰æ‹©Thompsoné‡‡æ ·")
            return 'thompson_sampling'
        
        # è®¡ç®—è·¯å¾„çº§åˆ«çš„æ”¶æ•›æ°´å¹³
        if not self.path_arms:
            return 'thompson_sampling'
        
        arms_list = list(self.path_arms.values())
        convergence_level = self._calculate_path_convergence_level(arms_list)
        
        # è€ƒè™‘æ€ç»´å¤šæ ·æ€§ï¼šè·¯å¾„é€‰æ‹©éœ€è¦æ›´å¤šæ¢ç´¢
        if convergence_level < 0.4:
            # ä½æ”¶æ•›ï¼Œä½¿ç”¨æ¢ç´¢æ€§å¼ºçš„ç®—æ³•
            logger.debug(f"ğŸ“Š ä½æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©Thompsoné‡‡æ ·")
            return 'thompson_sampling'
        elif convergence_level < 0.7:
            # ä¸­ç­‰æ”¶æ•›ï¼Œä½¿ç”¨å¹³è¡¡çš„ç®—æ³•
            logger.debug(f"ğŸ“Š ä¸­ç­‰æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©UCB")
            return 'ucb_variant'
        else:
            # é«˜æ”¶æ•›ï¼Œä½†ä»éœ€ä¿æŒä¸€å®šæ¢ç´¢ï¼ˆæ€ç»´å¤šæ ·æ€§é‡è¦ï¼‰
            logger.debug(f"ğŸ“Š é«˜æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©Epsilon-Greedy")
            return 'epsilon_greedy'
    
    def _calculate_path_convergence_level(self, arms: List[EnhancedDecisionArm]) -> float:
        """
        è®¡ç®—è·¯å¾„çº§åˆ«çš„æ”¶æ•›æ°´å¹³
        
        Args:
            arms: è·¯å¾„å†³ç­–è‡‚åˆ—è¡¨
            
        Returns:
            æ”¶æ•›æ°´å¹³ (0.0-1.0)
        """
        if len(arms) < 2:
            return 0.0
        
        # è®¡ç®—è·¯å¾„æˆåŠŸç‡æ–¹å·®
        success_rates = []
        for arm in arms:
            total = arm.success_count + arm.failure_count
            if total > 0:
                success_rates.append(arm.success_count / total)
        
        if len(success_rates) < 2:
            return 0.0
        
        variance = np.var(success_rates)
        # å°†æ–¹å·®è½¬æ¢ä¸ºæ”¶æ•›æ°´å¹³ï¼ˆæ–¹å·®è¶Šå°ï¼Œæ”¶æ•›æ°´å¹³è¶Šé«˜ï¼‰
        # å¯¹äºæ€ç»´è·¯å¾„ï¼Œæˆ‘ä»¬å¸Œæœ›ä¿æŒä¸€å®šçš„å¤šæ ·æ€§ï¼Œæ‰€ä»¥æ”¶æ•›æ ‡å‡†ç¨å¾®å®½æ¾
        convergence_level = max(0.0, 1.0 - variance * 3.5)
        
        return convergence_level
    
    # ==================== ğŸ”§ å·¥å…·é€‰æ‹©MABç®—æ³•å®ç° ====================
    
    def _select_best_algorithm_for_tools(self) -> str:
        """
        ä¸ºå·¥å…·é€‰æ‹©é€‰æ‹©æœ€ä½³ç®—æ³•
        
        Returns:
            æœ€ä½³ç®—æ³•åç§°
        """
        # å¦‚æœæ ·æœ¬å¤ªå°‘ï¼Œä½¿ç”¨Thompsoné‡‡æ ·è¿›è¡Œæ¢ç´¢
        if self.total_tool_selections < 10:
            logger.debug("ğŸ“Š å·¥å…·é€‰æ‹©æ ·æœ¬è¾ƒå°‘ï¼Œé€‰æ‹©Thompsoné‡‡æ ·")
            return 'thompson_sampling'
        
        # è®¡ç®—å·¥å…·çº§åˆ«çš„æ”¶æ•›æ°´å¹³
        if not self.tool_arms:
            return 'thompson_sampling'
        
        arms_list = list(self.tool_arms.values())
        convergence_level = self._calculate_tool_convergence_level(arms_list)
        
        # å·¥å…·é€‰æ‹©å€¾å‘äºæ›´å¿«æ”¶æ•›åˆ°æœ€ä¼˜å·¥å…·
        if convergence_level < 0.3:
            # ä½æ”¶æ•›ï¼Œä½¿ç”¨æ¢ç´¢æ€§å¼ºçš„ç®—æ³•
            logger.debug(f"ğŸ“Š å·¥å…·é€‰æ‹©ä½æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©Thompsoné‡‡æ ·")
            return 'thompson_sampling'
        elif convergence_level < 0.6:
            # ä¸­ç­‰æ”¶æ•›ï¼Œä½¿ç”¨å¹³è¡¡çš„ç®—æ³•
            logger.debug(f"ğŸ“Š å·¥å…·é€‰æ‹©ä¸­ç­‰æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©UCB")
            return 'ucb_variant'
        else:
            # é«˜æ”¶æ•›ï¼Œä½¿ç”¨åˆ©ç”¨å‹ç®—æ³•
            logger.debug(f"ğŸ“Š å·¥å…·é€‰æ‹©é«˜æ”¶æ•›({convergence_level:.3f})ï¼Œé€‰æ‹©Epsilon-Greedy")
            return 'epsilon_greedy'
    
    def _calculate_tool_convergence_level(self, arms: List[EnhancedDecisionArm]) -> float:
        """
        è®¡ç®—å·¥å…·çº§åˆ«çš„æ”¶æ•›æ°´å¹³
        
        Args:
            arms: å·¥å…·å†³ç­–è‡‚åˆ—è¡¨
            
        Returns:
            æ”¶æ•›æ°´å¹³ (0.0-1.0)
        """
        if len(arms) < 2:
            return 0.0
        
        # è®¡ç®—å·¥å…·æˆåŠŸç‡æ–¹å·®
        success_rates = []
        for arm in arms:
            total = arm.success_count + arm.failure_count
            if total > 0:
                success_rates.append(arm.success_count / total)
        
        if len(success_rates) < 2:
            return 0.0
        
        variance = np.var(success_rates)
        # å·¥å…·é€‰æ‹©å¯ä»¥æ›´å¿«æ”¶æ•›ï¼Œæ”¶æ•›æ ‡å‡†ç›¸å¯¹ä¸¥æ ¼
        convergence_level = max(0.0, 1.0 - variance * 2.5)
        
        return convergence_level
    
    def _thompson_sampling_for_tools(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹å·¥å…·é€‰æ‹©çš„Thompsoné‡‡æ ·ç®—æ³•"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„å·¥å…·å†³ç­–è‡‚")
        
        best_arm = None
        best_score = -1
        
        logger.debug(f"ğŸ”§ Thompsoné‡‡æ ·å·¥å…·é€‰æ‹©ï¼Œå€™é€‰å·¥å…·: {len(arms)}ä¸ª")
        
        for arm in arms:
            # ä½¿ç”¨Betaåˆ†å¸ƒè¿›è¡ŒThompsoné‡‡æ ·
            alpha = arm.success_count + 1
            beta = arm.failure_count + 1
            
            # ä»Betaåˆ†å¸ƒä¸­é‡‡æ ·
            sampled_value = np.random.beta(alpha, beta)
            
            # å·¥å…·çº§åˆ«çš„å¥–åŠ±è€ƒè™‘
            if arm.rl_reward_history:
                avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                # å°†å¥–åŠ±è°ƒæ•´åˆ°0-1èŒƒå›´
                normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                sampled_value = sampled_value * 0.7 + normalized_reward * 0.3
            
            logger.debug(f"   å·¥å…· {arm.path_id}: sampled={sampled_value:.3f}, Î±={alpha}, Î²={beta}")
            
            if sampled_value > best_score:
                best_score = sampled_value
                best_arm = arm
        
        logger.debug(f"ğŸ† Thompsoné‡‡æ ·é€‰æ‹©å·¥å…·: {best_arm.path_id} (å¾—åˆ†: {best_score:.3f})")
        return best_arm
    
    def _ucb_variant_for_tools(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹å·¥å…·é€‰æ‹©çš„UCB (Upper Confidence Bound) å˜ç§ç®—æ³•"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„å·¥å…·å†³ç­–è‡‚")
        
        total_rounds = sum(arm.activation_count for arm in arms)
        if total_rounds == 0:
            # ç¬¬ä¸€è½®éšæœºé€‰æ‹©
            selected_arm = np.random.choice(arms)
            logger.debug(f"ğŸ”§ UCBé¦–è½®éšæœºé€‰æ‹©å·¥å…·: {selected_arm.path_id}")
            return selected_arm
        
        best_arm = None
        best_ucb_value = -float('inf')
        
        logger.debug(f"ğŸ“Š UCBå·¥å…·é€‰æ‹©ï¼Œæ€»è½®æ•°: {total_rounds}")
        
        for arm in arms:
            if arm.activation_count == 0:
                # æœªå°è¯•è¿‡çš„å·¥å…·ä¼˜å…ˆé€‰æ‹©
                logger.debug(f"ğŸ†• ä¼˜å…ˆé€‰æ‹©æœªä½¿ç”¨å·¥å…·: {arm.path_id}")
                return arm
            
            # è®¡ç®—UCBå€¼
            confidence_bound = np.sqrt(2 * np.log(total_rounds) / arm.activation_count)
            
            # åŸºç¡€æˆåŠŸç‡
            base_value = arm.success_rate
            
            # å·¥å…·çº§åˆ«çš„RLå¥–åŠ±è€ƒè™‘
            if arm.rl_reward_history:
                avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                base_value = base_value * 0.6 + normalized_reward * 0.4
            
            # å·¥å…·æ¢ç´¢å¥–åŠ±
            exploration_bonus = confidence_bound * 1.0  # æ ‡å‡†æ¢ç´¢
            ucb_value = base_value + exploration_bonus
            
            logger.debug(f"   å·¥å…· {arm.path_id}: UCB={ucb_value:.3f}, base={base_value:.3f}, conf={confidence_bound:.3f}")
            
            if ucb_value > best_ucb_value:
                best_ucb_value = ucb_value
                best_arm = arm
        
        logger.debug(f"ğŸ† UCBé€‰æ‹©å·¥å…·: {best_arm.path_id} (UCBå€¼: {best_ucb_value:.3f})")
        return best_arm
    
    def _epsilon_greedy_for_tools(self, arms: List[EnhancedDecisionArm]) -> EnhancedDecisionArm:
        """é’ˆå¯¹å·¥å…·é€‰æ‹©çš„Epsilon-Greedyç®—æ³•"""
        if not arms:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„å·¥å…·å†³ç­–è‡‚")
        
        # å·¥å…·çº§åˆ«çš„åŠ¨æ€epsilonå€¼
        total_activations = sum(arm.activation_count for arm in arms)
        epsilon = max(0.05, 0.3 / (1 + total_activations * 0.01))  # æ¯”è·¯å¾„é€‰æ‹©æ›´ä½çš„æ¢ç´¢ç‡
        
        logger.debug(f"ğŸ”§ Epsilon-Greedyå·¥å…·é€‰æ‹©ï¼ŒÎµ={epsilon:.3f}")
        
        # ä½¿ç”¨epsilonå†³å®šæ˜¯å¦æ¢ç´¢
        if np.random.random() < epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©å·¥å…·
            selected_arm = np.random.choice(arms)
            logger.debug(f"ğŸ” æ¢ç´¢æ¨¡å¼é€‰æ‹©å·¥å…·: {selected_arm.path_id}")
            return selected_arm
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©å½“å‰æœ€å¥½çš„å·¥å…·
            best_arm = None
            best_score = -float('inf')
            
            for arm in arms:
                # å·¥å…·çº§åˆ«çš„ç»¼åˆè¯„åˆ†
                score = arm.success_rate
                
                # RLå¥–åŠ±æƒé‡
                if arm.rl_reward_history:
                    avg_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history)
                    normalized_reward = max(0, min(1, (avg_reward + 1) / 2))
                    score = score * 0.5 + normalized_reward * 0.5
                
                logger.debug(f"   å·¥å…· {arm.path_id}: score={score:.3f}")
                
                if score > best_score:
                    best_score = score
                    best_arm = arm
            
            logger.debug(f"ğŸ† åˆ©ç”¨æ¨¡å¼é€‰æ‹©å·¥å…·: {best_arm.path_id} (å¾—åˆ†: {best_score:.3f})")
            return best_arm if best_arm else arms[0]
    
    # ==================== ğŸ“Š æ›´æ–°æ€§èƒ½åé¦ˆæ–¹æ³• ====================
    
    def update_path_performance(self, path_id: str, success: bool, reward: float = 0.0, source: str = "user_feedback"):
        """
        ğŸ”§ åŒå±‚å­¦ä¹ ï¼šæ›´æ–°è·¯å¾„æˆ–å·¥å…·çš„æ€§èƒ½åé¦ˆ - é€šç”¨æ€§åé¦ˆæ›´æ–°æ–¹æ³•ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        å¢å¼ºåŠŸèƒ½ï¼š
        - æ–°å¢sourceå‚æ•°ï¼Œæ”¯æŒè¿½è¸ªåé¦ˆæ¥æº
        - æ”¯æŒå›æº¯åˆ†æ("retrospection")å’Œç”¨æˆ·åé¦ˆ("user_feedback")çš„åŒºåˆ†
        - é’ˆå¯¹ä¸åŒæ¥æºçš„åé¦ˆï¼Œä½¿ç”¨ä¸åŒçš„æƒé‡å’Œå¤„ç†ç­–ç•¥
        
        Args:
            path_id: è·¯å¾„IDæˆ–å·¥å…·IDï¼ˆç”±è°ƒç”¨æ–¹å†³å®šæ˜¯è·¯å¾„è¿˜æ˜¯å·¥å…·ï¼‰
            success: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
            reward: RLå¥–åŠ±å€¼
            source: åé¦ˆæ¥æº ("user_feedback", "retrospection", "auto_evaluation", "tool_verification")
        """
        # ğŸ¯ æ™ºèƒ½è¯†åˆ«ï¼šæ£€æŸ¥æ˜¯è·¯å¾„åé¦ˆè¿˜æ˜¯å·¥å…·åé¦ˆ
        if path_id in self.path_arms:
            # è·¯å¾„åé¦ˆå¤„ç†
            target_arm = self.path_arms[path_id]
            
            # æ›´æ–°è·¯å¾„ç®—æ³•æ€§èƒ½ç»Ÿè®¡
            if self.path_selection_history:
                last_selection = self.path_selection_history[-1]
                if last_selection['path_id'] == path_id:
                    algorithm = last_selection['algorithm']
                    self.algorithm_performance[algorithm]['total'] += 1
                    if success:
                        self.algorithm_performance[algorithm]['successes'] += 1
                        
        elif path_id in self.tool_arms:
            # å·¥å…·åé¦ˆå¤„ç†
            target_arm = self.tool_arms[path_id]
            
            # æ›´æ–°å·¥å…·ç®—æ³•æ€§èƒ½ç»Ÿè®¡
            if self.tool_selection_history:
                last_selection = self.tool_selection_history[-1]
                if last_selection['tool_id'] == path_id:
                    algorithm = last_selection['algorithm']
                    self.tool_algorithm_performance[algorithm]['total'] += 1
                    if success:
                        self.tool_algorithm_performance[algorithm]['successes'] += 1
                        
        else:
            # åŠ¨æ€åˆ›å»ºå†³ç­–è‡‚ï¼ˆé»˜è®¤ä½œä¸ºè·¯å¾„å¤„ç†ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
            target_arm = self._create_strategy_arm_if_missing(path_id)
            logger.debug(f"ğŸ”§ ä¸ºæœªçŸ¥ID {path_id} åˆ›å»ºè·¯å¾„å†³ç­–è‡‚ï¼ˆå‘åå…¼å®¹ï¼‰")
        
        # âœ… å¢å¼ºç‰ˆæ€§èƒ½æ›´æ–°ï¼šæ ¹æ®æ¥æºè°ƒæ•´å¤„ç†ç­–ç•¥
        adjusted_reward = self._adjust_reward_by_source(reward, source, success)
        target_arm.update_performance(success, adjusted_reward)
        
        # ğŸ“Š è®°å½•æ¥æºè¿½è¸ªä¿¡æ¯
        self._record_feedback_source(path_id, source, success, reward)
        
        # è®°å½•æ›´æ–°æ—¥å¿—
        arm_type = "å·¥å…·" if path_id in self.tool_arms else "è·¯å¾„"
        logger.info(f"ğŸ“Š æ›´æ–°{arm_type}æ€§èƒ½: {path_id} -> æˆåŠŸç‡:{target_arm.success_rate:.3f}, å¥–åŠ±:{reward:.3f}, æ¥æº:{source}")
        logger.debug(f"   è¯¦ç»†: æˆåŠŸ{target_arm.success_count}æ¬¡, å¤±è´¥{target_arm.failure_count}æ¬¡, æ¿€æ´»{target_arm.activation_count}æ¬¡")
        
        # ğŸ† é»„é‡‘æ¨¡æ¿è¯†åˆ«é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦ç¬¦åˆé»„é‡‘æ¨¡æ¿æ¡ä»¶ï¼ˆä»…å¯¹è·¯å¾„åº”ç”¨ï¼‰
        if path_id in self.path_arms:
            self._check_and_promote_to_golden_template(path_id, target_arm)
            
            # ğŸ­ è¯•ç‚¼åœºç®¡ç†ï¼šæ£€æŸ¥æ·˜æ±°å€™é€‰
            self._check_culling_candidates(path_id, target_arm, success)
    
    def _adjust_reward_by_source(self, reward: float, source: str, success: bool) -> float:
        """
        æ ¹æ®åé¦ˆæ¥æºè°ƒæ•´å¥–åŠ±å€¼
        
        Args:
            reward: åŸå§‹å¥–åŠ±å€¼
            source: åé¦ˆæ¥æº
            success: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
            
        Returns:
            è°ƒæ•´åçš„å¥–åŠ±å€¼
        """
        # è·å–æ¥æºæƒé‡
        weight = self.source_weight_config.get(source, 1.0)
        
        # ç‰¹æ®Šå¤„ç†ï¼šå›æº¯åˆ†æçš„åˆå§‹æ¢ç´¢å¥–åŠ±
        if source == "retrospection":
            if success and reward > 0:
                # å›æº¯åˆ†ææˆåŠŸçš„åˆ›æ–°æƒ³æ³•ç»™äºˆé¢å¤–å¥–åŠ±
                adjusted_reward = reward * weight + 0.1
            else:
                # å›æº¯åˆ†æå¤±è´¥æ—¶ä»ç»™äºˆå°å¹…æ­£å‘å¥–åŠ±é¼“åŠ±æ¢ç´¢
                adjusted_reward = max(reward * weight, 0.05)
        else:
            # å…¶ä»–æ¥æºæŒ‰æƒé‡è°ƒæ•´
            adjusted_reward = reward * weight
        
        return adjusted_reward
    
    def _record_feedback_source(self, path_id: str, source: str, success: bool, reward: float):
        """
        è®°å½•åé¦ˆæ¥æºè¿½è¸ªä¿¡æ¯
        
        Args:
            path_id: è·¯å¾„ID
            source: åé¦ˆæ¥æº
            success: æ‰§è¡Œæ˜¯å¦æˆåŠŸ
            reward: å¥–åŠ±å€¼
        """
        if source in self.feedback_source_tracking:
            tracking = self.feedback_source_tracking[source]
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            old_count = tracking["count"]
            new_count = old_count + 1
            
            # å¢é‡æ›´æ–°æˆåŠŸç‡
            old_success_rate = tracking["success_rate"]
            new_success_rate = (old_success_rate * old_count + (1 if success else 0)) / new_count
            
            # å¢é‡æ›´æ–°å¹³å‡å¥–åŠ±
            old_avg_reward = tracking["avg_reward"]
            new_avg_reward = (old_avg_reward * old_count + reward) / new_count
            
            # æ›´æ–°è¿½è¸ªè®°å½•
            tracking.update({
                "count": new_count,
                "success_rate": new_success_rate,
                "avg_reward": new_avg_reward
            })
            
            logger.debug(f"ğŸ“Š æ¥æºè¿½è¸ªæ›´æ–°: {source} -> æ¬¡æ•°:{new_count}, æˆåŠŸç‡:{new_success_rate:.3f}, å¹³å‡å¥–åŠ±:{new_avg_reward:.3f}")
    
    def get_feedback_source_stats(self) -> Dict[str, Any]:
        """
        è·å–åé¦ˆæ¥æºç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            è¯¦ç»†çš„æ¥æºè¿½è¸ªç»Ÿè®¡
        """
        return {
            "source_tracking": self.feedback_source_tracking.copy(),
            "source_weights": self.source_weight_config.copy(),
            "total_feedback_by_source": {
                source: data["count"] 
                for source, data in self.feedback_source_tracking.items()
            },
            "retrospection_contribution": {
                "total_retrospection_feedback": self.feedback_source_tracking["retrospection"]["count"],
                "retrospection_success_rate": self.feedback_source_tracking["retrospection"]["success_rate"],
                "retrospection_avg_reward": self.feedback_source_tracking["retrospection"]["avg_reward"]
            }
        }
    
    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆæ ‡è®°ä¸ºè¿‡æ—¶ï¼‰
    def update_arm_performance(self, dimension_name: str, option: str, 
                             success: bool, reward: float = 0.0):
        """
        æ›´æ–°å†³ç­–è‡‚çš„æ€§èƒ½ - å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ update_path_performance
        
        Args:
            dimension_name: ç»´åº¦åç§°
            option: é€‰é¡¹åç§°
            success: æ˜¯å¦æˆåŠŸ
            reward: RLå¥–åŠ±å€¼
        """
        logger.warning("âš ï¸ update_arm_performance å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ update_path_performance")
        path_id = f"{dimension_name}_{option}"  # ä¸´æ—¶è½¬æ¢
        self.update_path_performance(path_id, success, reward)
    
    def check_path_convergence(self) -> bool:
        """
        æ£€æŸ¥è·¯å¾„é€‰æ‹©æ˜¯å¦æ”¶æ•›
        
        Returns:
            æ˜¯å¦æ”¶æ•›
        """
        if len(self.path_arms) < 2:
            return False
            
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬
        total_samples = sum(arm.success_count + arm.failure_count for arm in self.path_arms.values())
        if total_samples < self.min_samples:
            return False
            
        # è®¡ç®—è·¯å¾„æˆåŠŸç‡æ–¹å·®ï¼Œåˆ¤æ–­æ˜¯å¦æ”¶æ•›
        success_rates = []
        for arm in self.path_arms.values():
            total = arm.success_count + arm.failure_count
            if total > 0:
                success_rates.append(arm.success_count / total)
                
        if len(success_rates) < 2:
            return False
            
        variance = np.var(success_rates)
        # å¯¹äºæ€ç»´è·¯å¾„ï¼Œä½¿ç”¨ç¨å¾®å®½æ¾çš„æ”¶æ•›æ ‡å‡†ï¼Œä¿æŒå¤šæ ·æ€§
        adjusted_threshold = self.convergence_threshold * 1.2
        is_converged = variance < adjusted_threshold
        
        if is_converged:
            logger.info(f"âœ… è·¯å¾„é€‰æ‹©å·²æ”¶æ•› (æ–¹å·®:{variance:.4f}, é˜ˆå€¼:{adjusted_threshold:.4f})")
        
        return is_converged
    
    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆæ ‡è®°ä¸ºè¿‡æ—¶ï¼‰
    def check_convergence(self, dimension_name: str) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šç»´åº¦æ˜¯å¦æ”¶æ•› - å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ check_path_convergence
        """
        logger.warning("âš ï¸ check_convergence å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ check_path_convergence")
        return self.check_path_convergence()
    
    def get_path_statistics(self) -> Dict[str, Dict[str, any]]:
        """
        è·å–æ‰€æœ‰è·¯å¾„çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«é»„é‡‘æ¨¡æ¿çŠ¶æ€ï¼‰
        
        Returns:
            è·¯å¾„ç»Ÿè®¡æ•°æ®
        """
        statistics = {}
        
        for path_id, arm in self.path_arms.items():
            # æ£€æŸ¥æ˜¯å¦ä¸ºé»„é‡‘æ¨¡æ¿
            is_golden_template = path_id in self.golden_templates
            golden_template_info = None
            
            if is_golden_template:
                template_data = self.golden_templates[path_id]
                golden_template_info = {
                    'created_timestamp': template_data['created_timestamp'],
                    'last_updated': template_data['last_updated'],
                    'stability_score': template_data['stability_score'],
                    'usage_as_template': self.template_usage_stats.get(path_id, 0),
                    'promotion_reason': template_data['promotion_reason']
                }
            
            # è®¡ç®—è·¯å¾„ç‰¹å®šçš„ç»Ÿè®¡
            statistics[path_id] = {
                'path_type': arm.option,  # è·¯å¾„ç±»å‹
                'path_id': path_id,
                'activation_count': arm.activation_count,
                'success_count': arm.success_count,
                'failure_count': arm.failure_count,
                'success_rate': arm.success_rate,
                'total_reward': arm.total_reward,
                'average_reward': sum(arm.rl_reward_history) / len(arm.rl_reward_history) if arm.rl_reward_history else 0.0,
                'last_used': arm.last_used,
                'recent_trend': self._calculate_recent_trend(arm),
                'consecutive_successes': self._calculate_consecutive_successes(arm),
                'usage_ratio': arm.activation_count / max(self.total_path_selections, 1),
                
                # ğŸ† é»„é‡‘æ¨¡æ¿ç›¸å…³ä¿¡æ¯
                'is_golden_template': is_golden_template,
                'golden_template_info': golden_template_info,
                'meets_golden_criteria': self._check_golden_criteria(arm),
                'stability_score': self._calculate_stability_score(arm) if arm.activation_count >= 10 else 0.0
            }
        
        return statistics
    
    def _check_golden_criteria(self, arm: EnhancedDecisionArm) -> bool:
        """
        æ£€æŸ¥è·¯å¾„æ˜¯å¦ç¬¦åˆé»„é‡‘æ¨¡æ¿çš„åŸºæœ¬æ¡ä»¶ï¼ˆä¸åŒ…æ‹¬ç¨³å®šæ€§æ£€æŸ¥ï¼‰
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            æ˜¯å¦ç¬¦åˆåŸºæœ¬æ¡ä»¶
        """
        config = self.golden_template_config
        return (arm.success_rate >= config['success_rate_threshold'] and 
                arm.activation_count >= config['min_samples_required'])
    
    def get_system_path_summary(self) -> Dict[str, any]:
        """
        è·å–è·¯å¾„é€‰æ‹©ç³»ç»Ÿçš„æ•´ä½“æ‘˜è¦
        
        Returns:
            ç³»ç»Ÿæ‘˜è¦æ•°æ®
        """
        if not self.path_arms:
            return {
                'total_paths': 0,
                'total_selections': self.total_path_selections,
                'is_converged': False,
                'convergence_level': 0.0,
                'most_used_path': None,
                'best_performing_path': None
            }
        
        # æœ€å¸¸ç”¨è·¯å¾„
        most_used_arm = max(self.path_arms.values(), key=lambda a: a.activation_count)
        
        # æœ€ä½³æ€§èƒ½è·¯å¾„
        best_performing_arm = max(self.path_arms.values(), key=lambda a: a.success_rate)
        
        # ç®—æ³•æ€§èƒ½ç»Ÿè®¡
        algorithm_stats = {}
        for algo, stats in self.algorithm_performance.items():
            if stats['total'] > 0:
                algorithm_stats[algo] = {
                    'success_rate': stats['successes'] / stats['total'],
                    'total_uses': stats['total']
                }
        
        return {
            'total_paths': len(self.path_arms),
            'total_selections': self.total_path_selections,
            'is_converged': self.check_path_convergence(),
            'convergence_level': self._calculate_path_convergence_level(list(self.path_arms.values())),
            'most_used_path': {
                'path_id': most_used_arm.path_id,
                'path_type': most_used_arm.option,
                'usage_count': most_used_arm.activation_count
            },
            'best_performing_path': {
                'path_id': best_performing_arm.path_id,
                'path_type': best_performing_arm.option,
                'success_rate': best_performing_arm.success_rate
            },
            'algorithm_performance': algorithm_stats,
            'total_samples': sum(arm.success_count + arm.failure_count for arm in self.path_arms.values())
        }
    
    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆæ ‡è®°ä¸ºè¿‡æ—¶ï¼‰
    def get_dimension_statistics(self) -> Dict[str, Dict[str, any]]:
        """
        è·å–æ‰€æœ‰ç»´åº¦çš„ç»Ÿè®¡ä¿¡æ¯ - å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ get_path_statistics
        """
        logger.warning("âš ï¸ get_dimension_statistics å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ get_path_statistics")
        return self.get_path_statistics()
    
    def get_path_details(self, path_id: str = None) -> Dict[str, any]:
        """
        è·å–æŒ‡å®šè·¯å¾„çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            path_id: è·¯å¾„IDï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›æ‰€æœ‰è·¯å¾„çš„è¯¦ç»†ä¿¡æ¯
            
        Returns:
            è·¯å¾„è¯¦ç»†ä¿¡æ¯
        """
        if path_id is not None:
            if path_id not in self.path_arms:
                logger.warning(f"âš ï¸ è·¯å¾„ {path_id} ä¸å­˜åœ¨")
                return {}
            
            arm = self.path_arms[path_id]
            return {
                'path_id': path_id,
                'path_type': arm.option,
                'success_count': arm.success_count,
                'failure_count': arm.failure_count,
                'success_rate': arm.success_rate,
                'total_reward': arm.total_reward,
                'average_reward': sum(arm.rl_reward_history) / len(arm.rl_reward_history) if arm.rl_reward_history else 0.0,
                'activation_count': arm.activation_count,
                'last_used': arm.last_used,
                'recent_trend': self._calculate_recent_trend(arm),
                'consecutive_successes': self._calculate_consecutive_successes(arm),
                'rl_reward_history': arm.rl_reward_history.copy(),
                'recent_results': arm.recent_results.copy()
            }
        else:
            # è¿”å›æ‰€æœ‰è·¯å¾„çš„è¯¦ç»†ä¿¡æ¯
            all_details = {}
            for pid, arm in self.path_arms.items():
                all_details[pid] = self.get_path_details(pid)
            
            # æŒ‰æˆåŠŸç‡æ’åº
            sorted_paths = sorted(all_details.items(), 
                                key=lambda x: x[1]['success_rate'], 
                                reverse=True)
            return dict(sorted_paths)
    
    def get_selection_history(self, limit: int = 10) -> List[Dict[str, any]]:
        """
        è·å–è·¯å¾„é€‰æ‹©å†å²
        
        Args:
            limit: è¿”å›çš„å†å²è®°å½•æ•°é‡é™åˆ¶
            
        Returns:
            é€‰æ‹©å†å²åˆ—è¡¨
        """
        return self.path_selection_history[-limit:] if self.path_selection_history else []
    
    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆæ ‡è®°ä¸ºè¿‡æ—¶ï¼‰
    def get_arm_details(self, dimension_name: str) -> List[Dict[str, any]]:
        """
        è·å–æŒ‡å®šç»´åº¦çš„æ‰€æœ‰å†³ç­–è‡‚è¯¦ç»†ä¿¡æ¯ - å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ get_path_details
        """
        logger.warning("âš ï¸ get_arm_details å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ get_path_details")
        return list(self.get_path_details().values())
    
    def reset_path(self, path_id: str):
        """
        é‡ç½®æŒ‡å®šè·¯å¾„çš„æ‰€æœ‰æ•°æ®
        
        Args:
            path_id: è·¯å¾„ID
        """
        if path_id in self.path_arms:
            del self.path_arms[path_id]
            logger.info(f"ğŸ”„ è·¯å¾„ {path_id} å·²é‡ç½®")
        
        # æ¸…ç†é€‰æ‹©å†å²ä¸­çš„ç›¸å…³è®°å½•
        self.path_selection_history = [
            record for record in self.path_selection_history 
            if record['path_id'] != path_id
        ]
    
    def reset_all_paths(self):
        """
        é‡ç½®æ‰€æœ‰è·¯å¾„æ•°æ®ï¼Œå®Œå…¨æ¸…ç©ºå­¦ä¹ å†å²
        """
        self.path_arms.clear()
        self.path_selection_history.clear()
        self.total_path_selections = 0
        self.algorithm_performance.clear()
        logger.info("ğŸ”„ æ‰€æœ‰è·¯å¾„æ•°æ®å·²é‡ç½®")
    
    def get_system_status(self) -> Dict[str, any]:
        """
        è·å–MABè·¯å¾„é€‰æ‹©ç³»ç»Ÿçš„æ•´ä½“çŠ¶æ€
        
        Returns:
            ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
        """
        total_paths = len(self.path_arms)
        is_converged = self.check_path_convergence()
        
        # è®¡ç®—æ´»è·ƒè·¯å¾„æ•°ï¼ˆæœ€è¿‘ä½¿ç”¨è¿‡çš„ï¼‰
        current_time = time.time()
        active_paths = sum(
            1 for arm in self.path_arms.values() 
            if arm.last_used > 0 and (current_time - arm.last_used) < 3600  # 1å°æ—¶å†…ä½¿ç”¨è¿‡
        )
        
        # æœ€å—æ¬¢è¿çš„è·¯å¾„ç±»å‹
        path_type_usage = {}
        for arm in self.path_arms.values():
            path_type = arm.option
            path_type_usage[path_type] = path_type_usage.get(path_type, 0) + arm.activation_count
        
        most_popular_type = max(path_type_usage.items(), key=lambda x: x[1])[0] if path_type_usage else None
        
        # è·å–é»„é‡‘æ¨¡æ¿ç»Ÿè®¡
        golden_stats = self.get_golden_template_stats()
        
        return {
            'mode': 'path_selection',  # æ–°å¢ï¼šæ ‡è¯†å½“å‰ä¸ºè·¯å¾„é€‰æ‹©æ¨¡å¼
            'total_paths': total_paths,
            'active_paths': active_paths,
            'total_selections': self.total_path_selections,
            'is_converged': is_converged,
            'convergence_level': self._calculate_path_convergence_level(list(self.path_arms.values())) if self.path_arms else 0.0,
            'convergence_threshold': self.convergence_threshold,
            'min_samples': self.min_samples,
            'most_popular_path_type': most_popular_type,
            'path_type_distribution': path_type_usage,
            'algorithm_performance': dict(self.algorithm_performance),
            
            # ğŸ† é»„é‡‘æ¨¡æ¿ç³»ç»ŸçŠ¶æ€
            'golden_template_system': {
                'enabled': True,
                'total_templates': golden_stats['total_templates'],
                'avg_success_rate': golden_stats['avg_success_rate'],
                'total_usage_count': golden_stats['total_usage_count'],
                'most_used_template': golden_stats['most_used_template'],
                'match_history_count': golden_stats['match_history_count'],
                'config': self.golden_template_config
            }
        }
    
    # ä¿ç•™å‘åå…¼å®¹çš„æ–¹æ³•ï¼ˆæ ‡è®°ä¸ºè¿‡æ—¶ï¼‰
    def reset_dimension(self, dimension_name: str):
        """
        é‡ç½®æŒ‡å®šç»´åº¦çš„æ‰€æœ‰æ•°æ® - å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ reset_path æˆ– reset_all_paths
        """
        logger.warning("âš ï¸ reset_dimension å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ reset_path æˆ– reset_all_paths")
        # ä¸æ‰§è¡Œä»»ä½•æ“ä½œï¼Œé¿å…æ„å¤–æ¸…é™¤è·¯å¾„æ•°æ®
    
    # ==================== ğŸ† é»„é‡‘å†³ç­–æ¨¡æ¿ç³»ç»Ÿå®ç° ====================
    
    def _check_golden_template_match(self, paths: List[ReasoningPath]) -> Optional[Dict[str, any]]:
        """
        æ£€æŸ¥å½“å‰è·¯å¾„åˆ—è¡¨æ˜¯å¦ä¸å·²æœ‰é»„é‡‘æ¨¡æ¿åŒ¹é… - ğŸ¯ ä¿®å¤ç‰ˆï¼šåŸºäºç­–ç•¥IDåŒ¹é…
        
        Args:
            paths: å€™é€‰æ€ç»´è·¯å¾„åˆ—è¡¨
            
        Returns:
            åŒ¹é…ç»“æœå­—å…¸ï¼ŒåŒ…å«åŒ¹é…çš„æ¨¡æ¿å’Œè·¯å¾„ä¿¡æ¯ï¼Œå¦‚æœæ— åŒ¹é…åˆ™è¿”å›None
        """
        if not self.golden_templates:
            return None
        
        best_match = None
        best_score = 0.0
        match_threshold = 0.85  # åŒ¹é…é˜ˆå€¼
        
        logger.debug(f"ğŸ† æ£€æŸ¥ {len(self.golden_templates)} ä¸ªé»„é‡‘æ¨¡æ¿")
        
        for template_id, template_data in self.golden_templates.items():
            template_path_type = template_data['path_type']
            
            # ğŸ¯ æ ¹æºä¿®å¤ï¼šç›´æ¥ä½¿ç”¨è·¯å¾„çš„ç­–ç•¥IDï¼Œæ— éœ€æ¨å¯¼
            for path in paths:
                # ç›´æ¥ä½¿ç”¨è·¯å¾„çš„ç­–ç•¥ID
                path_strategy_id = path.strategy_id
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ï¼šç­–ç•¥IDåŒ¹é…æˆ–è·¯å¾„ç±»å‹åŒ¹é…
                is_strategy_match = (template_id == path_strategy_id)
                is_type_match = (template_path_type == path.path_type)
                
                if is_strategy_match or is_type_match:
                    # è®¡ç®—åŒ¹é…åˆ†æ•°
                    match_score = self._calculate_template_match_score(template_data, path)
                    
                    # ç­–ç•¥IDåŒ¹é…ç»™é¢å¤–åˆ†æ•°
                    if is_strategy_match:
                        match_score += 0.1  # ç­–ç•¥IDåŒ¹é…å¥–åŠ±
                    
                    logger.debug(f"   æ¨¡æ¿ {template_id} vs è·¯å¾„ç­–ç•¥ {path_strategy_id}: åŒ¹é…åˆ†æ•° {match_score:.3f}")
                    logger.debug(f"      ç­–ç•¥åŒ¹é…: {is_strategy_match}, ç±»å‹åŒ¹é…: {is_type_match}")
                    
                    if match_score > match_threshold and match_score > best_score:
                        best_match = {
                            'template_id': template_id,
                            'path': path,
                            'match_score': match_score,
                            'template_data': template_data,
                            'strategy_match': is_strategy_match
                        }
                        best_score = match_score
        
        if best_match:
            match_type = "ç­–ç•¥ID" if best_match['strategy_match'] else "è·¯å¾„ç±»å‹"
            logger.debug(f"ğŸ† æ‰¾åˆ°æœ€ä½³åŒ¹é…: æ¨¡æ¿ {best_match['template_id']} (åˆ†æ•°: {best_score:.3f}, åŒ¹é…ç±»å‹: {match_type})")
        else:
            logger.debug("ğŸ† æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„é»„é‡‘æ¨¡æ¿åŒ¹é…")
        
        return best_match
    
    def _calculate_template_match_score(self, template_data: Dict[str, any], path: ReasoningPath) -> float:
        """
        è®¡ç®—æ¨¡æ¿ä¸è·¯å¾„çš„åŒ¹é…åˆ†æ•° - ğŸ¯ ä¿®å¤ç‰ˆï¼šåŸºäºç­–ç•¥IDåŒ¹é…
        
        Args:
            template_data: é»„é‡‘æ¨¡æ¿æ•°æ®
            path: å€™é€‰è·¯å¾„
            
        Returns:
            åŒ¹é…åˆ†æ•° (0.0-1.0)
        """
        score = 0.0
        
        # ğŸ¯ æ ¹æºä¿®å¤ï¼šç›´æ¥ä½¿ç”¨è·¯å¾„çš„ç­–ç•¥ID
        path_strategy_id = path.strategy_id
        
        # 1. ç­–ç•¥IDå®Œå…¨åŒ¹é… (åŸºç¡€åˆ†æ•°60%)
        template_strategy_id = template_data.get('strategy_id', template_data.get('path_id', ''))
        if template_strategy_id == path_strategy_id:
            score += 0.6
        # 1b. è·¯å¾„ç±»å‹åŒ¹é…ä½œä¸ºå¤‡é€‰ (åŸºç¡€åˆ†æ•°40%)
        elif template_data['path_type'] == path.path_type:
            score += 0.4
        
        # 2. æè¿°ç›¸ä¼¼æ€§ (é¢å¤–20%)
        desc_similarity = self._calculate_description_similarity(
            template_data.get('description', ''), path.description
        )
        score += desc_similarity * 0.2
        
        # 3. å†å²æ€§èƒ½å¥–åŠ± (é¢å¤–20%)
        performance_bonus = min(template_data['success_rate'] - 0.8, 0.2) * 1.0  # è¶…è¿‡80%çš„éƒ¨åˆ†è½¬æ¢ä¸ºå¥–åŠ±
        score += performance_bonus
        
        logger.debug(f"ğŸ¯ åŒ¹é…åˆ†æ•°è¯¦æƒ…:")
        logger.debug(f"   æ¨¡æ¿ç­–ç•¥ID: {template_strategy_id}")
        logger.debug(f"   è·¯å¾„ç­–ç•¥ID: {path_strategy_id}")
        logger.debug(f"   ç­–ç•¥åŒ¹é…: {template_strategy_id == path_strategy_id}")
        logger.debug(f"   æè¿°ç›¸ä¼¼æ€§: {desc_similarity:.3f}")
        logger.debug(f"   æ€§èƒ½å¥–åŠ±: {performance_bonus:.3f}")
        logger.debug(f"   æ€»åˆ†: {score:.3f}")
        
        return min(score, 1.0)
    
    def _calculate_description_similarity(self, desc1: str, desc2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæè¿°æ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆç®€å•å®ç°ï¼‰
        
        Args:
            desc1: æè¿°æ–‡æœ¬1
            desc2: æè¿°æ–‡æœ¬2
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0.0-1.0)
        """
        if not desc1 or not desc2:
            return 0.0
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…ç®—æ³•
        words1 = set(desc1.lower().split())
        words2 = set(desc2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _check_and_promote_to_golden_template(self, path_id: str, arm: EnhancedDecisionArm):
        """
        æ£€æŸ¥è·¯å¾„æ˜¯å¦ç¬¦åˆé»„é‡‘æ¨¡æ¿æ¡ä»¶ï¼Œå¦‚æœç¬¦åˆåˆ™æå‡ä¸ºé»„é‡‘æ¨¡æ¿
        
        Args:
            path_id: è·¯å¾„ID
            arm: å†³ç­–è‡‚å¯¹è±¡
        """
        config = self.golden_template_config
        
        # æ£€æŸ¥åŸºæœ¬æ¡ä»¶
        if (arm.success_rate >= config['success_rate_threshold'] and 
            arm.activation_count >= config['min_samples_required']):
            
            # æ£€æŸ¥ç¨³å®šæ€§ï¼ˆæœ€è¿‘Næ¬¡çš„è¡¨ç°ï¼‰
            if self._check_path_stability(arm):
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯é»„é‡‘æ¨¡æ¿
                if path_id not in self.golden_templates:
                    self._promote_to_golden_template(path_id, arm)
                else:
                    # æ›´æ–°å·²æœ‰é»„é‡‘æ¨¡æ¿
                    self._update_golden_template(path_id, arm)
    
    def _check_path_stability(self, arm: EnhancedDecisionArm) -> bool:
        """
        æ£€æŸ¥è·¯å¾„çš„ç¨³å®šæ€§ï¼ˆæœ€è¿‘è¡¨ç°æ˜¯å¦æŒç»­è‰¯å¥½ï¼‰
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            æ˜¯å¦ç¨³å®š
        """
        window_size = self.golden_template_config['stability_check_window']
        
        # è·å–æœ€è¿‘çš„ç»“æœ
        recent_results = arm.recent_results[-window_size:] if len(arm.recent_results) >= window_size else arm.recent_results
        
        if len(recent_results) < window_size:
            return False  # æ ·æœ¬ä¸è¶³
        
        # è®¡ç®—æœ€è¿‘çª—å£çš„æˆåŠŸç‡
        recent_successes = sum(1 for result in recent_results if result)
        recent_success_rate = recent_successes / len(recent_results)
        
        # ç¨³å®šæ€§è¦æ±‚ï¼šæœ€è¿‘è¡¨ç°ä¸ä½äºæ•´ä½“è¡¨ç°çš„95%
        stability_threshold = arm.success_rate * 0.95
        
        return recent_success_rate >= stability_threshold
    
    def _promote_to_golden_template(self, strategy_id: str, arm: EnhancedDecisionArm):
        """
        å°†ç­–ç•¥æå‡ä¸ºé»„é‡‘æ¨¡æ¿ - ğŸ¯ ä¿®å¤ç‰ˆï¼šåŸºäºç­–ç•¥ID
        
        Args:
            strategy_id: ç­–ç•¥IDï¼ˆè€Œéå®ä¾‹IDï¼‰
            arm: å†³ç­–è‡‚å¯¹è±¡
        """
        # æ£€æŸ¥æ¨¡æ¿æ•°é‡é™åˆ¶
        if len(self.golden_templates) >= self.golden_template_config['max_golden_templates']:
            # ç§»é™¤è¡¨ç°æœ€å·®çš„æ¨¡æ¿
            self._remove_worst_golden_template()
        
        # ğŸ¯ ä¿®å¤ï¼šåŸºäºç­–ç•¥IDåˆ›å»ºé»„é‡‘æ¨¡æ¿
        template_data = {
            'strategy_id': strategy_id,        # ç­–ç•¥IDï¼ˆç”¨äºåŒ¹é…ï¼‰
            'path_id': strategy_id,           # å…¼å®¹æ€§å­—æ®µ
            'path_type': arm.option,          # è·¯å¾„ç±»å‹
            'description': getattr(arm, 'description', ''),
            'success_rate': arm.success_rate,
            'total_activations': arm.activation_count,
            'average_reward': sum(arm.rl_reward_history) / len(arm.rl_reward_history) if arm.rl_reward_history else 0.0,
            'created_timestamp': time.time(),
            'last_updated': time.time(),
            'promotion_reason': 'high_performance',
            'stability_score': self._calculate_stability_score(arm),
            'usage_count': 0  # ä½œä¸ºæ¨¡æ¿è¢«ä½¿ç”¨çš„æ¬¡æ•°
        }
        
        # ä½¿ç”¨ç­–ç•¥IDä½œä¸ºæ¨¡æ¿é”®
        self.golden_templates[strategy_id] = template_data
        
        logger.info(f"ğŸ† æ–°é»„é‡‘æ¨¡æ¿è¯ç”Ÿï¼")
        logger.info(f"   ç­–ç•¥ID: {strategy_id}")
        logger.info(f"   è·¯å¾„ç±»å‹: {arm.option}")
        logger.info(f"   æˆåŠŸç‡: {arm.success_rate:.1%}")
        logger.info(f"   æ¿€æ´»æ¬¡æ•°: {arm.activation_count}")
        avg_rl_reward = sum(arm.rl_reward_history) / len(arm.rl_reward_history) if arm.rl_reward_history else 0.0
        logger.info(f"   å¹³å‡å¥–åŠ±: {avg_rl_reward:.3f}")
        logger.info(f"   å½“å‰é»„é‡‘æ¨¡æ¿æ€»æ•°: {len(self.golden_templates)}")
    
    def _update_golden_template(self, strategy_id: str, arm: EnhancedDecisionArm):
        """
        æ›´æ–°å·²æœ‰çš„é»„é‡‘æ¨¡æ¿æ•°æ® - ğŸ¯ ä¿®å¤ç‰ˆï¼šåŸºäºç­–ç•¥ID
        
        Args:
            strategy_id: ç­–ç•¥IDï¼ˆè€Œéå®ä¾‹IDï¼‰
            arm: å†³ç­–è‡‚å¯¹è±¡
        """
        if strategy_id in self.golden_templates:
            template = self.golden_templates[strategy_id]
            template.update({
                'success_rate': arm.success_rate,
                'total_activations': arm.activation_count,
                'average_reward': sum(arm.rl_reward_history) / len(arm.rl_reward_history) if arm.rl_reward_history else 0.0,
                'last_updated': time.time(),
                'stability_score': self._calculate_stability_score(arm)
            })
            
            logger.debug(f"ğŸ† æ›´æ–°é»„é‡‘æ¨¡æ¿: {strategy_id} -> æˆåŠŸç‡:{arm.success_rate:.1%}")
    
    def _calculate_stability_score(self, arm: EnhancedDecisionArm) -> float:
        """
        è®¡ç®—è·¯å¾„çš„ç¨³å®šæ€§åˆ†æ•°
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            ç¨³å®šæ€§åˆ†æ•° (0.0-1.0)
        """
        if arm.activation_count < 10:
            return 0.0
        
        # è®¡ç®—æˆåŠŸç‡çš„æ–¹å·®ï¼ˆè¶Šå°è¶Šç¨³å®šï¼‰
        recent_results = arm.recent_results[-20:] if len(arm.recent_results) >= 20 else arm.recent_results
        
        if len(recent_results) < 5:
            return 0.5  # æ ·æœ¬ä¸è¶³ï¼Œç»™ä¸­ç­‰åˆ†æ•°
        
        # è®¡ç®—æ»‘åŠ¨çª—å£æˆåŠŸç‡çš„æ–¹å·®
        window_size = 5
        success_rates = []
        
        for i in range(len(recent_results) - window_size + 1):
            window = recent_results[i:i + window_size]
            window_success_rate = sum(window) / len(window)
            success_rates.append(window_success_rate)
        
        if len(success_rates) < 2:
            return 0.5
        
        # æ–¹å·®è¶Šå°ï¼Œç¨³å®šæ€§è¶Šé«˜
        variance = np.var(success_rates)
        stability_score = max(0.0, 1.0 - variance * 4)  # å°†æ–¹å·®è½¬æ¢ä¸ºç¨³å®šæ€§åˆ†æ•°
        
        return stability_score
    
    def _remove_worst_golden_template(self):
        """
        ç§»é™¤è¡¨ç°æœ€å·®çš„é»„é‡‘æ¨¡æ¿
        """
        if not self.golden_templates:
            return
        
        # æŒ‰ç»¼åˆåˆ†æ•°æ’åºï¼Œç§»é™¤æœ€å·®çš„
        worst_template_id = min(self.golden_templates.keys(), 
                               key=lambda tid: self._calculate_template_quality_score(self.golden_templates[tid]))
        
        removed_template = self.golden_templates.pop(worst_template_id)
        
        logger.info(f"ğŸ—‘ï¸ ç§»é™¤è¡¨ç°è¾ƒå·®çš„é»„é‡‘æ¨¡æ¿: {worst_template_id}")
        logger.info(f"   åŸå› : ä¸ºæ–°æ¨¡æ¿è…¾å‡ºç©ºé—´")
        logger.info(f"   è¢«ç§»é™¤æ¨¡æ¿æˆåŠŸç‡: {removed_template['success_rate']:.1%}")
    
    def _calculate_template_quality_score(self, template_data: Dict[str, any]) -> float:
        """
        è®¡ç®—æ¨¡æ¿çš„è´¨é‡åˆ†æ•°
        
        Args:
            template_data: æ¨¡æ¿æ•°æ®
            
        Returns:
            è´¨é‡åˆ†æ•°
        """
        # ç»¼åˆè€ƒè™‘æˆåŠŸç‡ã€ä½¿ç”¨æ¬¡æ•°ã€ç¨³å®šæ€§ç­‰å› ç´ 
        success_score = template_data['success_rate'] * 0.4
        usage_score = min(template_data.get('usage_count', 0) / 10, 1.0) * 0.3  # ä½¿ç”¨æ¬¡æ•°æ ‡å‡†åŒ–
        stability_score = template_data.get('stability_score', 0.5) * 0.2
        recency_score = self._calculate_recency_score(template_data) * 0.1
        
        return success_score + usage_score + stability_score + recency_score
    
    def _calculate_recency_score(self, template_data: Dict[str, any]) -> float:
        """
        è®¡ç®—æ¨¡æ¿çš„æ–°è¿‘æ€§åˆ†æ•°
        
        Args:
            template_data: æ¨¡æ¿æ•°æ®
            
        Returns:
            æ–°è¿‘æ€§åˆ†æ•° (0.0-1.0)
        """
        current_time = time.time()
        last_updated = template_data.get('last_updated', template_data.get('created_timestamp', current_time))
        
        # è®¡ç®—è·ç¦»ä¸Šæ¬¡æ›´æ–°çš„æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        hours_since_update = (current_time - last_updated) / 3600
        
        # 24å°æ—¶å†…æ›´æ–°å¾—æ»¡åˆ†ï¼Œè¶…è¿‡7å¤©å¼€å§‹è¡°å‡
        if hours_since_update <= 24:
            return 1.0
        elif hours_since_update <= 168:  # 7å¤©
            return 1.0 - (hours_since_update - 24) / 144  # çº¿æ€§è¡°å‡
        else:
            return 0.0
    
    # ==================== ğŸ† é»„é‡‘æ¨¡æ¿ç®¡ç†æ¥å£ ====================
    
    def get_golden_templates(self) -> Dict[str, Dict[str, any]]:
        """
        è·å–æ‰€æœ‰é»„é‡‘æ¨¡æ¿
        
        Returns:
            é»„é‡‘æ¨¡æ¿å­—å…¸
        """
        return self.golden_templates.copy()
    
    def get_golden_template_stats(self) -> Dict[str, any]:
        """
        è·å–é»„é‡‘æ¨¡æ¿ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.golden_templates:
            return {
                'total_templates': 0,
                'avg_success_rate': 0.0,
                'total_usage_count': 0,
                'most_used_template': None,
                'template_usage_stats': {},
                'match_history_count': len(self.template_match_history) if hasattr(self, 'template_match_history') else 0
            }
        
        success_rates = [t['success_rate'] for t in self.golden_templates.values()]
        usage_counts = [self.template_usage_stats.get(tid, 0) for tid in self.golden_templates.keys()]
        
        most_used_template_id = max(self.template_usage_stats.keys(), 
                                   key=self.template_usage_stats.get) if self.template_usage_stats else None
        
        return {
            'total_templates': len(self.golden_templates),
            'avg_success_rate': sum(success_rates) / len(success_rates),
            'total_usage_count': sum(usage_counts),
            'most_used_template': {
                'template_id': most_used_template_id,
                'usage_count': self.template_usage_stats.get(most_used_template_id, 0),
                'template_data': self.golden_templates.get(most_used_template_id)
            } if most_used_template_id else None,
            'template_usage_stats': dict(self.template_usage_stats),
            'match_history_count': len(self.template_match_history)
        }
    
    def remove_golden_template(self, template_id: str) -> bool:
        """
        æ‰‹åŠ¨ç§»é™¤æŒ‡å®šçš„é»„é‡‘æ¨¡æ¿
        
        Args:
            template_id: æ¨¡æ¿ID
            
        Returns:
            æ˜¯å¦æˆåŠŸç§»é™¤
        """
        if template_id in self.golden_templates:
            removed_template = self.golden_templates.pop(template_id)
            logger.info(f"ğŸ—‘ï¸ æ‰‹åŠ¨ç§»é™¤é»„é‡‘æ¨¡æ¿: {template_id}")
            logger.info(f"   æ¨¡æ¿ç±»å‹: {removed_template['path_type']}")
            return True
        else:
            logger.warning(f"âš ï¸ é»„é‡‘æ¨¡æ¿ {template_id} ä¸å­˜åœ¨")
            return False
    
    def clear_golden_templates(self):
        """
        æ¸…ç©ºæ‰€æœ‰é»„é‡‘æ¨¡æ¿
        """
        count = len(self.golden_templates)
        self.golden_templates.clear()
        self.template_usage_stats.clear()
        self.template_match_history.clear()
        
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç©ºæ‰€æœ‰é»„é‡‘æ¨¡æ¿ (å…± {count} ä¸ª)")
    
    def export_golden_templates(self) -> str:
        """
        å¯¼å‡ºé»„é‡‘æ¨¡æ¿æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
        
        Returns:
            JSONå­—ç¬¦ä¸²
        """
        import json
        export_data = {
            'golden_templates': self.golden_templates,
            'template_usage_stats': dict(self.template_usage_stats),
            'template_match_history': self.template_match_history[-50:],  # åªå¯¼å‡ºæœ€è¿‘50æ¡åŒ¹é…å†å²
            'export_timestamp': time.time(),
            'config': self.golden_template_config
        }
        
        return json.dumps(export_data, indent=2, ensure_ascii=False)
    
    def import_golden_templates(self, json_data: str) -> bool:
        """
        å¯¼å…¥é»„é‡‘æ¨¡æ¿æ•°æ®
        
        Args:
            json_data: JSONå­—ç¬¦ä¸²
            
        Returns:
            æ˜¯å¦æˆåŠŸå¯¼å…¥
        """
        try:
            import json
            data = json.loads(json_data)
            
            # éªŒè¯æ•°æ®æ ¼å¼
            if 'golden_templates' not in data:
                logger.error("âŒ å¯¼å…¥æ•°æ®æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘golden_templateså­—æ®µ")
                return False
            
            # å¯¼å…¥æ¨¡æ¿
            imported_count = 0
            for template_id, template_data in data['golden_templates'].items():
                if len(self.golden_templates) < self.golden_template_config['max_golden_templates']:
                    self.golden_templates[template_id] = template_data
                    imported_count += 1
                else:
                    break
            
            # å¯¼å…¥ä½¿ç”¨ç»Ÿè®¡
            if 'template_usage_stats' in data:
                for template_id, count in data['template_usage_stats'].items():
                    if template_id in self.golden_templates:
                        self.template_usage_stats[template_id] = count
            
            logger.info(f"âœ… æˆåŠŸå¯¼å…¥ {imported_count} ä¸ªé»„é‡‘æ¨¡æ¿")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥é»„é‡‘æ¨¡æ¿å¤±è´¥: {e}")
            return False
    
    # ==================== ğŸ† é»„é‡‘æ¨¡æ¿ä½¿ç”¨ç¤ºä¾‹ ====================
    
    def demo_golden_template_workflow(self):
        """
        æ¼”ç¤ºé»„é‡‘æ¨¡æ¿ç³»ç»Ÿçš„å®Œæ•´å·¥ä½œæµç¨‹
        
        è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–¹æ³•ï¼Œå±•ç¤ºäº†é»„é‡‘æ¨¡æ¿ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½
        """
        logger.info("ğŸ† å¼€å§‹é»„é‡‘æ¨¡æ¿ç³»ç»Ÿæ¼”ç¤º")
        
        # 1. æ˜¾ç¤ºå½“å‰çŠ¶æ€
        stats = self.get_golden_template_stats()
        logger.info(f"å½“å‰é»„é‡‘æ¨¡æ¿æ•°é‡: {stats['total_templates']}")
        
        # 2. æ˜¾ç¤ºé…ç½®
        config = self.golden_template_config
        logger.info(f"é»„é‡‘æ¨¡æ¿é…ç½®:")
        logger.info(f"  - æˆåŠŸç‡é˜ˆå€¼: {config['success_rate_threshold']:.1%}")
        logger.info(f"  - æœ€å°æ ·æœ¬æ•°: {config['min_samples_required']}")
        logger.info(f"  - æœ€å¤§æ¨¡æ¿æ•°: {config['max_golden_templates']}")
        
        # 3. æ˜¾ç¤ºç°æœ‰é»„é‡‘æ¨¡æ¿
        if self.golden_templates:
            logger.info("ğŸ† ç°æœ‰é»„é‡‘æ¨¡æ¿:")
            for template_id, template_data in self.golden_templates.items():
                logger.info(f"  - {template_id}: {template_data['path_type']} "
                           f"(æˆåŠŸç‡: {template_data['success_rate']:.1%}, "
                           f"ä½¿ç”¨æ¬¡æ•°: {self.template_usage_stats.get(template_id, 0)})")
        else:
            logger.info("ğŸ“ æš‚æ— é»„é‡‘æ¨¡æ¿")
        
        # 4. æ˜¾ç¤ºå€™é€‰è·¯å¾„
        candidate_paths = []
        for path_id, arm in self.path_arms.items():
            if self._check_golden_criteria(arm) and path_id not in self.golden_templates:
                candidate_paths.append((path_id, arm))
        
        if candidate_paths:
            logger.info("â­ ç¬¦åˆé»„é‡‘æ¨¡æ¿æ¡ä»¶çš„å€™é€‰è·¯å¾„:")
            for path_id, arm in candidate_paths:
                stability = self._calculate_stability_score(arm)
                logger.info(f"  - {path_id}: {arm.option} "
                           f"(æˆåŠŸç‡: {arm.success_rate:.1%}, "
                           f"æ ·æœ¬: {arm.activation_count}, "
                           f"ç¨³å®šæ€§: {stability:.2f})")
        else:
            logger.info("ğŸ“ æš‚æ— ç¬¦åˆæ¡ä»¶çš„å€™é€‰è·¯å¾„")
        
        logger.info("ğŸ† é»„é‡‘æ¨¡æ¿ç³»ç»Ÿæ¼”ç¤ºå®Œæˆ")
    
    # ==================== ğŸ’¡ Aha-Momentå†³ç­–æ”¯æŒç³»ç»Ÿ ====================
    
    def get_path_confidence(self, strategy_id: str) -> float:
        """
        è·å–æŒ‡å®šç­–ç•¥çš„ç½®ä¿¡åº¦åˆ†æ•°
        
        Args:
            strategy_id: ç­–ç•¥IDï¼ˆæ³¨æ„ï¼šè¿™é‡Œåº”è¯¥ä¼ é€’strategy_idè€Œä¸æ˜¯path_idå®ä¾‹IDï¼‰
            
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•° (0.0-1.0)ï¼Œ1.0è¡¨ç¤ºéå¸¸æœ‰ä¿¡å¿ƒï¼Œ0.0è¡¨ç¤ºå®Œå…¨æ²¡æœ‰ä¿¡å¿ƒ
        """
        # ğŸ”§ åŠ¨æ€åˆ›å»ºï¼šå¦‚æœç­–ç•¥ä¸å­˜åœ¨ï¼Œåˆ™åŠ¨æ€åˆ›å»ºï¼ˆç½®ä¿¡åº¦ä¸ºæœ€ä½ï¼‰
        arm = self._create_strategy_arm_if_missing(strategy_id)
        
        # å¦‚æœæ ·æœ¬æ•°ä¸è¶³ï¼Œç½®ä¿¡åº¦è¾ƒä½
        if arm.activation_count < 5:
            base_confidence = 0.2  # åŸºç¡€ä¿¡å¿ƒå¾ˆä½
        elif arm.activation_count < 10:
            base_confidence = 0.4
        elif arm.activation_count < 20:
            base_confidence = 0.6
        else:
            base_confidence = 0.8  # å……è¶³æ ·æœ¬çš„åŸºç¡€ä¿¡å¿ƒ
        
        # åŸºäºæˆåŠŸç‡è°ƒæ•´ç½®ä¿¡åº¦
        success_factor = arm.success_rate
        
        # åŸºäºç¨³å®šæ€§è°ƒæ•´ç½®ä¿¡åº¦
        stability_factor = self._calculate_stability_score(arm) if arm.activation_count >= 10 else 0.5
        
        # åŸºäºæœ€è¿‘è¡¨ç°è°ƒæ•´ç½®ä¿¡åº¦
        recent_performance_factor = self._calculate_recent_performance_factor(arm)
        
        # ç»¼åˆè®¡ç®—ç½®ä¿¡åº¦
        confidence = (
            base_confidence * 0.3 +          # æ ·æœ¬é‡è´¡çŒ®30%
            success_factor * 0.4 +           # æˆåŠŸç‡è´¡çŒ®40%
            stability_factor * 0.2 +         # ç¨³å®šæ€§è´¡çŒ®20%
            recent_performance_factor * 0.1  # æœ€è¿‘è¡¨ç°è´¡çŒ®10%
        )
        
        return min(max(confidence, 0.0), 1.0)  # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
    
    def _calculate_recent_performance_factor(self, arm: EnhancedDecisionArm) -> float:
        """
        è®¡ç®—æœ€è¿‘è¡¨ç°å› å­
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            æœ€è¿‘è¡¨ç°å› å­ (0.0-1.0)
        """
        if not arm.recent_results or len(arm.recent_results) < 3:
            return 0.5  # é»˜è®¤ä¸­ç­‰
        
        # è®¡ç®—æœ€è¿‘5æ¬¡çš„æˆåŠŸç‡
        recent_window = arm.recent_results[-5:]
        recent_success_rate = sum(recent_window) / len(recent_window)
        
        return recent_success_rate
    
    def get_all_paths_confidence(self) -> Dict[str, float]:
        """
        è·å–æ‰€æœ‰è·¯å¾„çš„ç½®ä¿¡åº¦
        
        Returns:
            è·¯å¾„IDåˆ°ç½®ä¿¡åº¦çš„æ˜ å°„
        """
        confidence_map = {}
        for path_id in self.path_arms.keys():
            confidence_map[path_id] = self.get_path_confidence(path_id)
        
        return confidence_map
    
    def check_low_confidence_scenario(self, threshold: float = 0.3) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å¤„äºä½ç½®ä¿¡åº¦åœºæ™¯ï¼ˆæ‰€æœ‰è·¯å¾„è¡¨ç°éƒ½å¾ˆå·®ï¼‰
        
        Args:
            threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¤ä¸ºæ˜¯ä½ç½®ä¿¡åº¦
            
        Returns:
            æ˜¯å¦æ‰€æœ‰è·¯å¾„éƒ½å¤„äºä½ç½®ä¿¡åº¦çŠ¶æ€
        """
        if not self.path_arms:
            return True  # æ²¡æœ‰è·¯å¾„æ•°æ®ï¼Œè®¤ä¸ºæ˜¯ä½ç½®ä¿¡åº¦åœºæ™¯
        
        confidence_scores = self.get_all_paths_confidence()
        
        if not confidence_scores:
            return True
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è·¯å¾„çš„ç½®ä¿¡åº¦éƒ½ä½äºé˜ˆå€¼
        max_confidence = max(confidence_scores.values())
        avg_confidence = sum(confidence_scores.values()) / len(confidence_scores)
        
        logger.debug(f"ğŸ’¡ ç½®ä¿¡åº¦æ£€æŸ¥: æœ€é«˜ç½®ä¿¡åº¦={max_confidence:.3f}, å¹³å‡ç½®ä¿¡åº¦={avg_confidence:.3f}, é˜ˆå€¼={threshold}")
        
        # å¦‚æœæœ€é«˜ç½®ä¿¡åº¦éƒ½ä½äºé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºéœ€è¦ç»•é“æ€è€ƒ
        return max_confidence < threshold
    
    def get_confidence_analysis(self) -> Dict[str, any]:
        """
        è·å–ç½®ä¿¡åº¦åˆ†ææŠ¥å‘Š
        
        Returns:
            ç½®ä¿¡åº¦åˆ†ææ•°æ®
        """
        confidence_scores = self.get_all_paths_confidence()
        
        if not confidence_scores:
            return {
                'total_paths': 0,
                'max_confidence': 0.0,
                'min_confidence': 0.0,
                'avg_confidence': 0.0,
                'low_confidence_paths': 0,
                'high_confidence_paths': 0,
                'confidence_distribution': {}
            }
        
        values = list(confidence_scores.values())
        low_confidence_count = sum(1 for conf in values if conf < 0.3)
        high_confidence_count = sum(1 for conf in values if conf > 0.7)
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒç»Ÿè®¡
        distribution = {
            'very_low (0.0-0.2)': sum(1 for conf in values if 0.0 <= conf < 0.2),
            'low (0.2-0.4)': sum(1 for conf in values if 0.2 <= conf < 0.4),
            'medium (0.4-0.6)': sum(1 for conf in values if 0.4 <= conf < 0.6),
            'high (0.6-0.8)': sum(1 for conf in values if 0.6 <= conf < 0.8),
            'very_high (0.8-1.0)': sum(1 for conf in values if 0.8 <= conf <= 1.0)
        }
        
        return {
            'total_paths': len(confidence_scores),
            'max_confidence': max(values),
            'min_confidence': min(values),
            'avg_confidence': sum(values) / len(values),
            'low_confidence_paths': low_confidence_count,
            'high_confidence_paths': high_confidence_count,
            'confidence_distribution': distribution,
            'detailed_scores': confidence_scores
        }
    
    # ==================== ğŸ”§ è¾…åŠ©è®¡ç®—æ–¹æ³• ====================
    
    def _calculate_recent_trend(self, arm: EnhancedDecisionArm) -> str:
        """
        è®¡ç®—è·¯å¾„çš„æœ€è¿‘æ€§èƒ½è¶‹åŠ¿
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            è¶‹åŠ¿å­—ç¬¦ä¸²: 'improving', 'declining', 'stable', 'insufficient_data'
        """
        if len(arm.recent_results) < 4:
            return 'insufficient_data'
        
        # å–æœ€è¿‘çš„ç»“æœï¼Œåˆ†ä¸ºä¸¤åŠè¿›è¡Œæ¯”è¾ƒ
        recent = arm.recent_results[-10:] if len(arm.recent_results) >= 10 else arm.recent_results
        mid_point = len(recent) // 2
        
        if mid_point < 2:
            return 'insufficient_data'
        
        # è®¡ç®—å‰åŠæ®µå’ŒååŠæ®µçš„æˆåŠŸç‡
        earlier_half = recent[:mid_point]
        later_half = recent[mid_point:]
        
        earlier_rate = sum(earlier_half) / len(earlier_half)
        later_rate = sum(later_half) / len(later_half)
        
        # åˆ¤æ–­è¶‹åŠ¿
        if later_rate > earlier_rate + 0.1:  # 10%çš„æ”¹å–„è§†ä¸ºimproving
            return 'improving'
        elif later_rate < earlier_rate - 0.1:  # 10%çš„ä¸‹é™è§†ä¸ºdeclining
            return 'declining'
        else:
            return 'stable'
    
    def _calculate_consecutive_successes(self, arm: EnhancedDecisionArm) -> int:
        """
        è®¡ç®—è¿ç»­æˆåŠŸæ¬¡æ•°
        
        Args:
            arm: å†³ç­–è‡‚å¯¹è±¡
            
        Returns:
            è¿ç»­æˆåŠŸæ¬¡æ•°
        """
        if not arm.recent_results:
            return 0
        
        consecutive_count = 0
        # ä»æœ€è¿‘çš„ç»“æœå¼€å§‹å¾€å‰æ•°
        for result in reversed(arm.recent_results):
            if result:  # å¦‚æœæ˜¯æˆåŠŸ
                consecutive_count += 1
            else:  # å¦‚æœå¤±è´¥äº†ï¼Œå°±åœæ­¢è®¡æ•°
                break
        
        return consecutive_count
    
    # ==================== ğŸ¯ æ ¹æºä¿®å¤å®Œæˆï¼šç§»é™¤å¤æ‚è§£æé€»è¾‘ ====================
    # æ³¨æ„ï¼š_resolve_strategy_id æ–¹æ³•å·²ç§»é™¤ï¼Œå› ä¸ºæ•°æ®æºå¤´ç°åœ¨ç›´æ¥æä¾›æ­£ç¡®çš„ç­–ç•¥ID
    
    def _infer_path_type_from_strategy_id(self, strategy_id: str) -> str:
        """
        ä»ç­–ç•¥IDæ¨æ–­è·¯å¾„ç±»å‹
        
        Args:
            strategy_id: ç­–ç•¥ID
            
        Returns:
            æ¨æ–­çš„è·¯å¾„ç±»å‹
        """
        # ç­–ç•¥IDåˆ°è·¯å¾„ç±»å‹çš„æ˜ å°„è¡¨
        strategy_to_type_mapping = {
            'systematic_analytical': 'ç³»ç»Ÿåˆ†æå‹',
            'creative_innovative': 'åˆ›æ–°çªç ´å‹',
            'critical_questioning': 'æ‰¹åˆ¤è´¨ç–‘å‹',
            'practical_pragmatic': 'å®ç”¨åŠ¡å®å‹',
            'holistic_comprehensive': 'æ•´ä½“ç»¼åˆå‹',
            'exploratory_investigative': 'æ¢ç´¢è°ƒç ”å‹',
            'collaborative_consultative': 'åä½œå’¨è¯¢å‹',
            'adaptive_flexible': 'é€‚åº”çµæ´»å‹',
            
            # å…¼å®¹æ€§æ˜ å°„ï¼ˆä¸­æ–‡è·¯å¾„ç±»å‹ï¼‰
            'ç³»ç»Ÿåˆ†æ': 'ç³»ç»Ÿåˆ†æå‹',
            'åˆ›æ–°çªç ´': 'åˆ›æ–°çªç ´å‹',
            'æ‰¹åˆ¤è´¨ç–‘': 'æ‰¹åˆ¤è´¨ç–‘å‹',
            'å®ç”¨åŠ¡å®': 'å®ç”¨åŠ¡å®å‹',
            'æ•´ä½“ç»¼åˆ': 'æ•´ä½“ç»¼åˆå‹',
            'æ¢ç´¢è°ƒç ”': 'æ¢ç´¢è°ƒç ”å‹',
            'åä½œå’¨è¯¢': 'åä½œå’¨è¯¢å‹',
            'é€‚åº”çµæ´»': 'é€‚åº”çµæ´»å‹'
        }
        
        # ç›´æ¥åŒ¹é…
        if strategy_id in strategy_to_type_mapping:
            return strategy_to_type_mapping[strategy_id]
        
        # æ¨¡ç³ŠåŒ¹é…
        strategy_lower = strategy_id.lower()
        for key, value in strategy_to_type_mapping.items():
            if key.lower() in strategy_lower or strategy_lower in key.lower():
                logger.debug(f"ğŸ” æ¨¡ç³ŠåŒ¹é…ç­–ç•¥ç±»å‹: {strategy_id} -> {value}")
                return value
        
        # åŸºäºå…³é”®è¯æ¨æ–­
        if 'systematic' in strategy_lower or 'analytical' in strategy_lower or 'ç³»ç»Ÿ' in strategy_id:
            return 'ç³»ç»Ÿåˆ†æå‹'
        elif 'creative' in strategy_lower or 'innovative' in strategy_lower or 'åˆ›æ–°' in strategy_id:
            return 'åˆ›æ–°çªç ´å‹'
        elif 'critical' in strategy_lower or 'questioning' in strategy_lower or 'æ‰¹åˆ¤' in strategy_id:
            return 'æ‰¹åˆ¤è´¨ç–‘å‹'
        elif 'practical' in strategy_lower or 'pragmatic' in strategy_lower or 'å®ç”¨' in strategy_id:
            return 'å®ç”¨åŠ¡å®å‹'
        elif 'holistic' in strategy_lower or 'comprehensive' in strategy_lower or 'æ•´ä½“' in strategy_id:
            return 'æ•´ä½“ç»¼åˆå‹'
        elif 'exploratory' in strategy_lower or 'investigative' in strategy_lower or 'æ¢ç´¢' in strategy_id:
            return 'æ¢ç´¢è°ƒç ”å‹'
        elif 'collaborative' in strategy_lower or 'consultative' in strategy_lower or 'åä½œ' in strategy_id:
            return 'åä½œå’¨è¯¢å‹'
        elif 'adaptive' in strategy_lower or 'flexible' in strategy_lower or 'é€‚åº”' in strategy_id:
            return 'é€‚åº”çµæ´»å‹'
        
        # é»˜è®¤è¿”å›
        logger.debug(f"âš ï¸ æ— æ³•æ¨æ–­è·¯å¾„ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤: {strategy_id} -> é€šç”¨æ–¹æ³•å‹")
        return 'é€šç”¨æ–¹æ³•å‹'
