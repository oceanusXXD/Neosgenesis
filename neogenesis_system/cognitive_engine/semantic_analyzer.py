#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¯­ä¹‰åˆ†æå™¨ - åŸºäºLLMçš„æ™ºèƒ½æ–‡æœ¬åˆ†æ
Semantic Analyzer - LLM-based intelligent text analysis

æ ¸å¿ƒèŒè´£ï¼š
1. æ›¿æ¢ç¡¬ç¼–ç å…³é”®è¯åˆ¤æ–­ï¼Œä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰ç†è§£
2. æ”¯æŒå¤šä»»åŠ¡å¹¶è¡Œåˆ†æï¼ˆæ„å›¾è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æã€å¤æ‚åº¦è¯„ä¼°ç­‰ï¼‰
3. è¿”å›ç»“æ„åŒ–çš„JSONåˆ†æç»“æœ
4. æä¾›å¯æ‰©å±•çš„åˆ†æä»»åŠ¡æ¡†æ¶

è®¾è®¡ç†å¿µï¼š
- æ™ºèƒ½è€Œéæ­»æ¿ï¼šç†è§£åŒä¹‰è¯ã€ä¸Šä¸‹æ–‡å’Œå¤æ‚å¥å¼
- å¯é…ç½®ä»»åŠ¡ï¼šæ”¯æŒè‡ªå®šä¹‰åˆ†æä»»åŠ¡å’Œè¾“å‡ºæ ¼å¼
- é«˜æ•ˆå¯é ï¼šå†…ç½®ç¼“å­˜ã€é”™è¯¯å¤„ç†å’Œé™çº§æœºåˆ¶
"""

import json
import time
import logging
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from enum import Enum
import hashlib

logger = logging.getLogger(__name__)

class AnalysisTaskType(Enum):
    """åˆ†æä»»åŠ¡ç±»å‹æšä¸¾"""
    INTENT_DETECTION = "intent_detection"           # æ„å›¾è¯†åˆ«
    SENTIMENT_ANALYSIS = "sentiment_analysis"       # æƒ…æ„Ÿåˆ†æ
    COMPLEXITY_ASSESSMENT = "complexity_assessment" # å¤æ‚åº¦è¯„ä¼°
    DOMAIN_CLASSIFICATION = "domain_classification" # é¢†åŸŸåˆ†ç±»
    URGENCY_EVALUATION = "urgency_evaluation"       # ç´§æ€¥ç¨‹åº¦è¯„ä¼°
    KEYWORD_EXTRACTION = "keyword_extraction"       # å…³é”®è¯æå–
    TOPIC_MODELING = "topic_modeling"               # ä¸»é¢˜å»ºæ¨¡
    LANGUAGE_DETECTION = "language_detection"       # è¯­è¨€æ£€æµ‹
    VISUAL_NEED_DETECTION = "visual_need_detection" # è§†è§‰éœ€æ±‚è¯†åˆ«
    OUTPUT_FORMAT_ANALYSIS = "output_format_analysis" # è¾“å‡ºå½¢æ€åˆ†æï¼ˆå·²å¼ƒç”¨ï¼‰
    VISUAL_ENHANCEMENT_OPPORTUNITY = "visual_enhancement_opportunity" # ğŸ¨ è§†è§‰å¢å¼ºæœºä¼šè¯„ä¼°
    INTERACTION_CONTEXT_ANALYSIS = "interaction_context_analysis" # ğŸ§  äº¤äº’æƒ…å¢ƒåˆ†æ
    AESTHETIC_PREFERENCE_INFERENCE = "aesthetic_preference_inference" # ğŸ­ å®¡ç¾åå¥½æ¨æ–­
    CUSTOM_ANALYSIS = "custom_analysis"             # è‡ªå®šä¹‰åˆ†æ

@dataclass
class AnalysisTask:
    """å•ä¸ªåˆ†æä»»åŠ¡å®šä¹‰"""
    task_type: AnalysisTaskType
    description: str
    expected_output_format: Dict[str, Any]
    prompt_template: Optional[str] = None
    confidence_threshold: float = 0.7
    
@dataclass 
class AnalysisResult:
    """å•ä¸ªåˆ†æç»“æœ"""
    task_type: AnalysisTaskType
    result: Dict[str, Any]
    confidence: float
    processing_time: float
    success: bool = True
    error_message: Optional[str] = None

@dataclass
class SemanticAnalysisResponse:
    """å®Œæ•´çš„è¯­ä¹‰åˆ†æå“åº”"""
    input_text: str
    analysis_results: Dict[str, AnalysisResult]
    total_processing_time: float
    overall_success: bool
    cache_hit: bool = False
    llm_provider: Optional[str] = None
    
class SemanticAnalyzer:
    """è¯­ä¹‰åˆ†æå™¨ - åŸºäºLLMçš„æ™ºèƒ½æ–‡æœ¬åˆ†æå¼•æ“"""
    
    def __init__(self, llm_manager=None, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ†æå™¨
        
        Args:
            llm_manager: LLMç®¡ç†å™¨å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»º
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«åˆ†æå‚æ•°å’ŒLLMè®¾ç½®
        """
        self.llm_manager = llm_manager
        self.config = config or self._get_default_config()
        
        # åˆ†æç»“æœç¼“å­˜
        self.analysis_cache = {}
        self.cache_ttl = self.config.get('cache_ttl', 300)  # 5åˆ†é’Ÿç¼“å­˜
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_analyses': 0,
            'cache_hits': 0,
            'successful_analyses': 0,
            'failed_analyses': 0,
            'total_processing_time': 0.0
        }
        
        # é¢„å®šä¹‰åˆ†æä»»åŠ¡æ¨¡æ¿
        self.builtin_tasks = self._initialize_builtin_tasks()
        
        logger.info("ğŸ” SemanticAnalyzer å·²åˆå§‹åŒ–")
        
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'cache_ttl': 300,  # ç¼“å­˜æ—¶é—´
            'max_retries': 3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
            'timeout': 30,     # è¯·æ±‚è¶…æ—¶æ—¶é—´
            'batch_size': 5,   # æ‰¹å¤„ç†å¤§å°
            'confidence_threshold': 0.7,  # é»˜è®¤ç½®ä¿¡åº¦é˜ˆå€¼
            'model_name': 'deepseek-chat',  # é»˜è®¤æ¨¡å‹
            'temperature': 0.1,  # ä½æ¸©åº¦ä¿è¯ç»“æœç¨³å®šæ€§
        }
        
    def _initialize_builtin_tasks(self) -> Dict[str, AnalysisTask]:
        """åˆå§‹åŒ–å†…ç½®åˆ†æä»»åŠ¡"""
        tasks = {}
        
        # æ„å›¾è¯†åˆ«ä»»åŠ¡
        tasks['intent_detection'] = AnalysisTask(
            task_type=AnalysisTaskType.INTENT_DETECTION,
            description="è¯†åˆ«ç”¨æˆ·è¾“å…¥çš„æ„å›¾å’Œç›®çš„",
            expected_output_format={
                "primary_intent": "string",  # ä¸»è¦æ„å›¾
                "secondary_intents": ["string"],  # æ¬¡è¦æ„å›¾
                "confidence": "float",  # ç½®ä¿¡åº¦
                "intent_category": "string",  # æ„å›¾ç±»åˆ«
                "action_required": "boolean"  # æ˜¯å¦éœ€è¦è¡ŒåŠ¨
            },
            prompt_template="""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æ„å›¾ï¼š

æ–‡æœ¬: "{text}"

è¯·è¯†åˆ«ç”¨æˆ·çš„æ„å›¾å¹¶è¿”å›JSONæ ¼å¼ç»“æœï¼š
{{
    "primary_intent": "ä¸»è¦æ„å›¾ï¼ˆå¦‚ï¼šä¿¡æ¯æŸ¥è¯¢ã€é—®é¢˜è§£å†³ã€ä»»åŠ¡æ‰§è¡Œç­‰ï¼‰",
    "secondary_intents": ["æ¬¡è¦æ„å›¾åˆ—è¡¨"],
    "confidence": 0.0-1.0ä¹‹é—´çš„ç½®ä¿¡åº¦åˆ†æ•°,
    "intent_category": "æ„å›¾ç±»åˆ«ï¼ˆå¦‚ï¼šquestion, request, command, greetingç­‰ï¼‰",
    "action_required": true/false æ˜¯å¦éœ€è¦å…·ä½“è¡ŒåŠ¨
}}

è¯·ä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        )
        
        # æƒ…æ„Ÿåˆ†æä»»åŠ¡
        tasks['sentiment_analysis'] = AnalysisTask(
            task_type=AnalysisTaskType.SENTIMENT_ANALYSIS,
            description="åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘å’Œæƒ…ç»ªçŠ¶æ€",
            expected_output_format={
                "overall_sentiment": "string",  # æ€»ä½“æƒ…æ„Ÿ
                "sentiment_score": "float",     # æƒ…æ„Ÿåˆ†æ•°
                "emotions": {"emotion": "float"},  # å…·ä½“æƒ…ç»ª
                "emotional_intensity": "string"   # æƒ…æ„Ÿå¼ºåº¦
            },
            prompt_template="""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå’Œæƒ…ç»ªï¼š

æ–‡æœ¬: "{text}"

è¯·è¿”å›JSONæ ¼å¼çš„æƒ…æ„Ÿåˆ†æç»“æœï¼š
{{
    "overall_sentiment": "positive/negative/neutral",
    "sentiment_score": -1.0åˆ°1.0ä¹‹é—´çš„åˆ†æ•°ï¼ˆ-1æœ€è´Ÿé¢ï¼Œ1æœ€æ­£é¢ï¼‰ï¼Œ
    "emotions": {{
        "joy": 0.0-1.0,
        "anger": 0.0-1.0,
        "fear": 0.0-1.0,
        "sadness": 0.0-1.0,
        "surprise": 0.0-1.0,
        "trust": 0.0-1.0
    }},
    "emotional_intensity": "low/medium/high"
}}

è¯·ä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        )
        
        # å¤æ‚åº¦è¯„ä¼°ä»»åŠ¡
        tasks['complexity_assessment'] = AnalysisTask(
            task_type=AnalysisTaskType.COMPLEXITY_ASSESSMENT,
            description="è¯„ä¼°ä»»åŠ¡æˆ–é—®é¢˜çš„å¤æ‚ç¨‹åº¦",
            expected_output_format={
                "complexity_level": "string",      # å¤æ‚åº¦ç­‰çº§
                "complexity_score": "float",       # å¤æ‚åº¦åˆ†æ•°
                "complexity_factors": ["string"],  # å¤æ‚åº¦å› ç´ 
                "estimated_effort": "string",      # é¢„ä¼°å·¥ä½œé‡
                "requires_expertise": "boolean"    # æ˜¯å¦éœ€è¦ä¸“ä¸šçŸ¥è¯†
            },
            prompt_template="""è¯·è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡æˆ–é—®é¢˜çš„å¤æ‚ç¨‹åº¦ï¼š

æ–‡æœ¬: "{text}"

è¯·è¿”å›JSONæ ¼å¼çš„å¤æ‚åº¦è¯„ä¼°ç»“æœï¼š
{{
    "complexity_level": "low/medium/high/expert",
    "complexity_score": 0.0-1.0ä¹‹é—´çš„å¤æ‚åº¦åˆ†æ•°,
    "complexity_factors": ["å¯¼è‡´å¤æ‚çš„å…·ä½“å› ç´ åˆ—è¡¨"],
    "estimated_effort": "minimal/moderate/substantial/extensive",
    "requires_expertise": true/false
}}

è¯„ä¼°æ ‡å‡†ï¼š
- Low: ç®€å•ç›´æ¥çš„ä»»åŠ¡ï¼Œå‡ åˆ†é’Ÿå†…å¯å®Œæˆ
- Medium: éœ€è¦ä¸€äº›æ€è€ƒå’Œæ­¥éª¤çš„ä»»åŠ¡
- High: å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ï¼Œéœ€è¦æ·±å…¥åˆ†æ
- Expert: éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œå¤§é‡æ—¶é—´çš„ä»»åŠ¡

è¯·ä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        )
        
        # é¢†åŸŸåˆ†ç±»ä»»åŠ¡
        tasks['domain_classification'] = AnalysisTask(
            task_type=AnalysisTaskType.DOMAIN_CLASSIFICATION,
            description="è¯†åˆ«æ–‡æœ¬æ‰€å±çš„ä¸“ä¸šé¢†åŸŸæˆ–ä¸»é¢˜åŸŸ",
            expected_output_format={
                "primary_domain": "string",        # ä¸»è¦é¢†åŸŸ
                "secondary_domains": ["string"],   # æ¬¡è¦é¢†åŸŸ  
                "domain_confidence": "float",      # é¢†åŸŸç½®ä¿¡åº¦
                "is_interdisciplinary": "boolean", # æ˜¯å¦è·¨é¢†åŸŸ
                "technical_level": "string"        # æŠ€æœ¯æ°´å¹³
            },
            prompt_template="""è¯·è¯†åˆ«ä»¥ä¸‹æ–‡æœ¬æ‰€å±çš„ä¸“ä¸šé¢†åŸŸï¼š

æ–‡æœ¬: "{text}"

è¯·è¿”å›JSONæ ¼å¼çš„é¢†åŸŸåˆ†ç±»ç»“æœï¼š
{{
    "primary_domain": "ä¸»è¦ä¸“ä¸šé¢†åŸŸ",
    "secondary_domains": ["ç›¸å…³çš„æ¬¡è¦é¢†åŸŸ"],
    "domain_confidence": 0.0-1.0ä¹‹é—´çš„ç½®ä¿¡åº¦,
    "is_interdisciplinary": true/false,
    "technical_level": "basic/intermediate/advanced/expert"
}}

é¢†åŸŸåŒ…æ‹¬ä½†ä¸é™äºï¼š
æŠ€æœ¯ï¼ˆprogramming, ai, database, systemï¼‰ï¼Œå•†ä¸šï¼ˆmarketing, finance, strategyï¼‰ï¼Œ
å­¦æœ¯ï¼ˆresearch, theory, analysisï¼‰ï¼Œç”Ÿæ´»ï¼ˆhealth, travel, educationï¼‰ï¼Œ
åˆ›æ„ï¼ˆdesign, art, writingï¼‰ç­‰

è¯·ä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        )
        
        # ç´§æ€¥ç¨‹åº¦è¯„ä¼°ä»»åŠ¡
        tasks['urgency_evaluation'] = AnalysisTask(
            task_type=AnalysisTaskType.URGENCY_EVALUATION,
            description="è¯„ä¼°ä»»åŠ¡çš„ç´§æ€¥ç¨‹åº¦å’Œä¼˜å…ˆçº§",
            expected_output_format={
                "urgency_level": "string",         # ç´§æ€¥ç¨‹åº¦
                "urgency_score": "float",          # ç´§æ€¥åº¦åˆ†æ•°
                "time_sensitivity": "string",      # æ—¶é—´æ•æ„Ÿåº¦
                "consequences": "string",          # å»¶è¿Ÿåæœ
                "priority_rank": "integer"         # ä¼˜å…ˆçº§æ’å
            },
            prompt_template="""è¯·è¯„ä¼°ä»¥ä¸‹ä»»åŠ¡çš„ç´§æ€¥ç¨‹åº¦ï¼š

æ–‡æœ¬: "{text}"

è¯·è¿”å›JSONæ ¼å¼çš„ç´§æ€¥ç¨‹åº¦è¯„ä¼°ï¼š
{{
    "urgency_level": "low/medium/high/critical",
    "urgency_score": 0.0-1.0ä¹‹é—´çš„ç´§æ€¥åº¦åˆ†æ•°,
    "time_sensitivity": "flexible/moderate/strict/immediate",
    "consequences": "å»¶è¿Ÿå¤„ç†çš„åæœæè¿°",
    "priority_rank": 1-10ä¹‹é—´çš„ä¼˜å…ˆçº§æ’åï¼ˆ10æœ€é«˜ï¼‰
}}

è¯„ä¼°æ ‡å‡†ï¼š
- Low: å¯ä»¥ç¨åå¤„ç†ï¼Œæ— æ˜ç¡®æ—¶é—´é™åˆ¶
- Medium: å»ºè®®åŠæ—¶å¤„ç†ï¼Œæœ‰ä¸€å®šæ—¶é—´è¦æ±‚
- High: éœ€è¦å°½å¿«å¤„ç†ï¼Œæœ‰æ˜ç¡®æœŸé™
- Critical: éœ€è¦ç«‹å³å¤„ç†ï¼Œå»¶è¿Ÿä¼šæœ‰ä¸¥é‡åæœ

è¯·ä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        )
        
        # è§†è§‰éœ€æ±‚è¯†åˆ«ä»»åŠ¡
        tasks['visual_need_detection'] = AnalysisTask(
            task_type=AnalysisTaskType.VISUAL_NEED_DETECTION,
            description="è¯†åˆ«ç”¨æˆ·æŸ¥è¯¢æ˜¯å¦éœ€è¦è§†è§‰åŒ–è¡¨è¾¾æˆ–å›¾åƒç”Ÿæˆ",
            expected_output_format={
                "needs_visual": "boolean",       # æ˜¯å¦éœ€è¦è§†è§‰åŒ–
                "visual_type": "string",         # è§†è§‰åŒ–ç±»å‹
                "confidence": "float",           # ç½®ä¿¡åº¦
                "visual_purpose": "string",      # è§†è§‰åŒ–ç›®çš„
                "suggested_elements": ["string"] # å»ºè®®å…ƒç´ 
            },
            prompt_template="""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬æ˜¯å¦éœ€è¦è§†è§‰åŒ–è¡¨è¾¾æˆ–å›¾åƒç”Ÿæˆï¼š

æ–‡æœ¬: "{text}"

è¯·è¯†åˆ«ç”¨æˆ·æ˜¯å¦éœ€è¦è§†è§‰å†…å®¹å¹¶è¿”å›JSONæ ¼å¼ç»“æœï¼š
{{
    "needs_visual": true/false - æ˜¯å¦éœ€è¦ç”Ÿæˆæˆ–å±•ç¤ºè§†è§‰å†…å®¹,
    "visual_type": "è®¾è®¡ç±»å‹ï¼ˆå¦‚ï¼šlogo, illustration, diagram, photo, art, ui_mockup, infographicç­‰ï¼‰",
    "confidence": 0.0-1.0ä¹‹é—´çš„ç½®ä¿¡åº¦åˆ†æ•°,
    "visual_purpose": "è§†è§‰åŒ–çš„ç›®çš„ï¼ˆå¦‚ï¼šå±•ç¤ºæ¦‚å¿µã€è¾…åŠ©è¯´æ˜ã€è‰ºæœ¯åˆ›ä½œã€è®¾è®¡åŸå‹ç­‰ï¼‰",
    "suggested_elements": ["å»ºè®®åŒ…å«çš„è§†è§‰å…ƒç´ æˆ–ç‰¹å¾åˆ—è¡¨"]
}}

åˆ¤æ–­æ ‡å‡†ï¼š
- ç›´æ¥è¯·æ±‚ï¼š"ç”»", "è®¾è®¡", "ç”Ÿæˆå›¾ç‰‡", "åˆ›ä½œ", "åˆ¶ä½œ"ç­‰
- éšå«éœ€æ±‚ï¼š"æƒ³è±¡ä¸€ä¸‹...", "å±•ç¤º...", "ä»€ä¹ˆæ ·å­ï¼Ÿ", "å¦‚ä½•çœ‹èµ·æ¥ï¼Ÿ"
- æè¿°æ€§å†…å®¹ï¼šè¯¦ç»†çš„å¤–è§‚ã€åœºæ™¯ã€é£æ ¼æè¿°
- è®¾è®¡ç›¸å…³ï¼šç•Œé¢ã€logoã€æ’å›¾ã€åŸå‹ç­‰éœ€æ±‚

è¯·ä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        )
        
        # ğŸ¨ è§†è§‰å¢å¼ºæœºä¼šè¯„ä¼°ä»»åŠ¡ - ä»"åˆ¤æ–­éœ€æ±‚"å‡çº§ä¸º"è¯„ä¼°æœºä¼š"
        tasks['visual_enhancement_opportunity'] = AnalysisTask(
            task_type=AnalysisTaskType.VISUAL_ENHANCEMENT_OPPORTUNITY,
            description="è¯„ä¼°è§†è§‰å†…å®¹å¢å¼ºç”¨æˆ·ä½“éªŒçš„æœºä¼šå’Œæ½œåŠ›ï¼Œä¸ä»…é™äºæ˜ç¡®çš„å›¾åƒè¯·æ±‚",
            expected_output_format={
                # åŸºæœ¬åˆ¤æ–­
                "has_visual_opportunity": "boolean",    # æ˜¯å¦å­˜åœ¨è§†è§‰å¢å¼ºæœºä¼š
                "opportunity_strength": "float",       # æœºä¼šå¼ºåº¦ (0.0-1.0)
                "opportunity_type": "string",          # æœºä¼šç±»å‹
                
                # æƒ…å¢ƒåˆ†æ
                "context_analysis": {
                    "conversation_tone": "string",     # å¯¹è¯è°ƒæ€§
                    "user_emotional_state": "string",  # ç”¨æˆ·æƒ…ç»ªçŠ¶æ€
                    "content_complexity": "string",    # å†…å®¹å¤æ‚åº¦
                    "interaction_phase": "string"      # äº¤äº’é˜¶æ®µ
                },
                
                # è§†è§‰å»ºè®®
                "visual_recommendations": {
                    "primary_visual_type": "string",   # ä¸»è¦è§†è§‰ç±»å‹
                    "style_suggestions": ["string"],   # é£æ ¼å»ºè®®
                    "mood_alignment": "string",        # æƒ…ç»ªåŒ¹é…
                    "aesthetic_direction": "string"    # å®¡ç¾æ–¹å‘
                },
                
                # æ—¶æœºåˆ¤æ–­
                "timing_assessment": {
                    "generation_timing": "string",     # ç”Ÿæˆæ—¶æœº
                    "user_readiness": "float",         # ç”¨æˆ·å‡†å¤‡åº¦
                    "context_appropriateness": "float" # æƒ…å¢ƒé€‚å®œåº¦
                },
                
                # ä¸ªæ€§åŒ–å»ºè®®
                "personalization": {
                    "suggested_elements": ["string"],  # å»ºè®®å…ƒç´ 
                    "avoid_elements": ["string"],      # éœ€è¦é¿å…çš„å…ƒç´ 
                    "cultural_considerations": "string" # æ–‡åŒ–è€ƒé‡
                }
            },
            prompt_template="""ä½œä¸ºä¸€åå…·å¤‡æ·±åº¦å®¡ç¾ç†è§£å’Œæƒ…å•†çš„AIåŠ©æ‰‹ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹å†…å®¹çš„è§†è§‰å¢å¼ºæœºä¼šï¼š

ç”¨æˆ·è¾“å…¥: "{text}"

è¯·ä»¥ä¸€åç»éªŒä¸°å¯Œçš„äº¤äº’è®¾è®¡å¸ˆå’Œæƒ…ç»ªæ™ºèƒ½ä¸“å®¶çš„è§†è§’ï¼Œç»¼åˆåˆ†æå¹¶è¿”å›JSONç»“æœï¼š

{{
    "has_visual_opportunity": true/false,
    "opportunity_strength": 0.0-1.0ä¹‹é—´çš„æœºä¼šå¼ºåº¦åˆ†æ•°,
    "opportunity_type": "æœºä¼šç±»å‹ï¼ˆå¦‚ï¼šexplicit_request, implicit_enhancement, educational_support, emotional_resonance, creative_inspirationï¼‰",
    
    "context_analysis": {{
        "conversation_tone": "å¯¹è¯è°ƒæ€§ï¼ˆå¦‚ï¼šformal, casual, playful, serious, creative, professionalï¼‰",
        "user_emotional_state": "ç”¨æˆ·æƒ…ç»ªï¼ˆå¦‚ï¼šcurious, frustrated, excited, focused, overwhelmed, inspiredï¼‰",
        "content_complexity": "å†…å®¹å¤æ‚åº¦ï¼ˆå¦‚ï¼šsimple, moderate, complex, highly_technicalï¼‰",
        "interaction_phase": "äº¤äº’é˜¶æ®µï¼ˆå¦‚ï¼šinitial_inquiry, deep_exploration, problem_solving, creative_brainstormingï¼‰"
    }},
    
    "visual_recommendations": {{
        "primary_visual_type": "ä¸»è¦è§†è§‰ç±»å‹ï¼ˆå¦‚ï¼šillustration, diagram, infographic, artistic_concept, ui_mockup, photo_realisticï¼‰",
        "style_suggestions": ["é£æ ¼å»ºè®®åˆ—è¡¨ã€å¦‚ï¼šminimalist, vibrant, professional, whimsical, modern, classic"],
        "mood_alignment": "æƒ…ç»ªåŒ¹é…ï¼ˆå¦‚ï¼šcalm_and_focused, energetic_and_inspiring, warm_and_friendly, sleek_and_modernï¼‰",
        "aesthetic_direction": "å®¡ç¾æ–¹å‘ï¼ˆå¦‚ï¼šclean_and_simple, rich_and_detailed, bold_and_dramatic, subtle_and_elegantï¼‰"
    }},
    
    "timing_assessment": {{
        "generation_timing": "ç”Ÿæˆæ—¶æœºï¼ˆå¦‚ï¼šimmediate, after_text_response, on_request, contextually_appropriateï¼‰",
        "user_readiness": 0.0-1.0ä¹‹é—´çš„ç”¨æˆ·å‡†å¤‡åº¦åˆ†æ•°,
        "context_appropriateness": 0.0-1.0ä¹‹é—´çš„æƒ…å¢ƒé€‚å®œåº¦åˆ†æ•°
    }},
    
    "personalization": {{
        "suggested_elements": ["å»ºè®®åŒ…å«çš„è§†è§‰å…ƒç´ åˆ—è¡¨"],
        "avoid_elements": ["åº”é¿å…çš„å…ƒç´ åˆ—è¡¨"],
        "cultural_considerations": "æ–‡åŒ–æ•æ„Ÿæ€§è€ƒé‡å’Œå»ºè®®"
    }}
}}

è¯„ä¼°æ ‡å‡†ï¼š
1. **æœºä¼šè¯†åˆ«**ï¼šä¸ä»…è¯†åˆ«æ˜ç¡®çš„å›¾åƒè¯·æ±‚ï¼Œæ›´è¦æŒ–æ˜éšå«çš„è§†è§‰å¢å¼ºæœºä¼š
2. **æƒ…å¢ƒæ•æ„Ÿ**ï¼šç†è§£å¯¹è¯æ°›å›´ã€ç”¨æˆ·æƒ…ç»ªå’Œäº¤äº’é˜¶æ®µ
3. **å®¡ç¾åˆ¤æ–­**ï¼šæä¾›ç¬¦åˆæƒ…å¢ƒå’Œç”¨æˆ·éœ€æ±‚çš„è§†è§‰é£æ ¼å»ºè®®
4. **æ—¶æœºæ™ºèƒ½**ï¼šåˆ¤æ–­ä½•æ—¶ç”Ÿæˆè§†è§‰å†…å®¹æœ€é€‚å®œ
5. **ä¸ªæ€§åŒ–é€‚åº”**ï¼šåŸºäºå†…å®¹å’Œæƒ…å¢ƒæä¾›ä¸ªæ€§åŒ–å»ºè®®

è¯·ä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        )
        
        # ğŸ§  äº¤äº’æƒ…å¢ƒåˆ†æä»»åŠ¡ - æ·±åº¦ç†è§£å¯¹è¯æƒ…å¢ƒ
        tasks['interaction_context_analysis'] = AnalysisTask(
            task_type=AnalysisTaskType.INTERACTION_CONTEXT_ANALYSIS,
            description="åˆ†æå½“å‰äº¤äº’çš„æƒ…å¢ƒã€æ°›å›´å’Œç”¨æˆ·çŠ¶æ€ï¼Œä¸ºè§†è§‰å†³ç­–æä¾›æƒ…å¢ƒæ”¯æŒ",
            expected_output_format={
                "interaction_context": {
                    "session_continuity": "string",   # ä¼šè¯è¿ç»­æ€§
                    "topic_evolution": "string",      # è¯é¢˜æ¼”åŒ–
                    "user_engagement_level": "float"  # ç”¨æˆ·å‚ä¸åº¦
                },
                "emotional_intelligence": {
                    "detected_emotions": ["string"],  # æ£€æµ‹åˆ°çš„æƒ…ç»ª
                    "emotional_trajectory": "string", # æƒ…ç»ªè½¨è¿¹
                    "empathy_opportunities": ["string"] # å…±æƒ…æœºä¼š
                },
                "cognitive_load_assessment": {
                    "information_density": "float",   # ä¿¡æ¯å¯†åº¦
                    "mental_effort_required": "string", # æ‰€éœ€å¿ƒæ™ºåŠªåŠ›
                    "attention_span_match": "float"   # æ³¨æ„åŠ›åŒ¹é…åº¦
                }
            },
            prompt_template="""ä½œä¸ºä¸€åäº¤äº’å¿ƒç†å­¦å’Œæƒ…ç»ªæ™ºèƒ½ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹äº¤äº’æƒ…å¢ƒï¼š

ç”¨æˆ·è¾“å…¥: "{text}"

è¯·è¿”å›JSONæ ¼å¼çš„æƒ…å¢ƒåˆ†æï¼š

{{
    "interaction_context": {{
        "session_continuity": "ä¼šè¯è¿ç»­æ€§ï¼ˆnew_topic, topic_deepening, follow_up, context_switchï¼‰",
        "topic_evolution": "è¯é¢˜æ¼”åŒ–ï¼ˆintroduction, exploration, refinement, conclusionï¼‰",
        "user_engagement_level": 0.0-1.0ä¹‹é—´çš„å‚ä¸åº¦åˆ†æ•°
    }},
    
    "emotional_intelligence": {{
        "detected_emotions": ["æ£€æµ‹åˆ°çš„æƒ…ç»ªåˆ—è¡¨ï¼Œå¦‚ï¼šcuriosity, excitement, frustration, confidence"],
        "emotional_trajectory": "æƒ…ç»ªè½¨è¿¹ï¼ˆå¦‚ï¼šsteady_positive, growing_enthusiasm, initial_confusion_to_clarityï¼‰",
        "empathy_opportunities": ["å¯ä»¥è¡¨è¾¾å…±æƒ…çš„æœºä¼šåˆ—è¡¨"]
    }},
    
    "cognitive_load_assessment": {{
        "information_density": 0.0-1.0ä¹‹é—´çš„ä¿¡æ¯å¯†åº¦åˆ†æ•°,
        "mental_effort_required": "æ‰€éœ€å¿ƒæ™ºåŠªåŠ›çº§åˆ«ï¼ˆlow, moderate, high, very_highï¼‰",
        "attention_span_match": 0.0-1.0ä¹‹é—´çš„æ³¨æ„åŠ›åŒ¹é…åº¦åˆ†æ•°
    }}
}}

åˆ†æé‡ç‚¹ï¼š
- è¯†åˆ«ç”¨æˆ·çš„æƒ…ç»ªçŠ¶æ€å’Œå‚ä¸ç¨‹åº¦
- è¯„ä¼°å½“å‰äº¤äº’çš„è®¤çŸ¥è´Ÿè·
- æ‰¾åˆ°å…±æƒ…å’Œæƒ…ç»ªè¿æ¥çš„æœºä¼š

è¯·ä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        )
        
        # ğŸ­ å®¡ç¾åå¥½æ¨æ–­ä»»åŠ¡ - ç†è§£ç”¨æˆ·çš„è§†è§‰å“å‘³
        tasks['aesthetic_preference_inference'] = AnalysisTask(
            task_type=AnalysisTaskType.AESTHETIC_PREFERENCE_INFERENCE,
            description="åŸºäºç”¨æˆ·çš„è¡¨è¾¾æ–¹å¼ã€å†…å®¹åå¥½å’Œäº¤äº’é£æ ¼ï¼Œæ¨æ–­å…¶å®¡ç¾åå¥½",
            expected_output_format={
                "aesthetic_profile": {
                    "style_preference": "string",     # é£æ ¼åå¥½
                    "complexity_tolerance": "string", # å¤æ‚åº¦è€å—åº¦
                    "color_personality": "string",    # é¢œè‰²ä¸ªæ€§
                    "cultural_context": "string"      # æ–‡åŒ–èƒŒæ™¯
                },
                "inferred_preferences": {
                    "visual_elements": ["string"],    # åå¥½çš„è§†è§‰å…ƒç´ 
                    "avoided_elements": ["string"],   # å¯èƒ½ä¸å–œæ¬¢çš„å…ƒç´ 
                    "mood_preferences": ["string"]    # æƒ…ç»ªè°ƒæ€§åå¥½
                },
                "confidence_metrics": {
                    "preference_certainty": "float",  # åå¥½ç¡®å®šæ€§
                    "cultural_accuracy": "float",     # æ–‡åŒ–å‡†ç¡®æ€§
                    "personalization_potential": "float" # ä¸ªæ€§åŒ–æ½œåŠ›
                }
            },
            prompt_template="""ä½œä¸ºä¸€åè·¨æ–‡åŒ–å®¡ç¾å¿ƒç†å­¦å’Œè®¾è®¡äººç±»å­¦ä¸“å®¶ï¼Œè¯·åˆ†æç”¨æˆ·çš„æ½œåœ¨å®¡ç¾åå¥½ï¼š

ç”¨æˆ·è¾“å…¥: "{text}"

è¯·åŸºäºè¯­è¨€é£æ ¼ã€è¡¨è¾¾æ–¹å¼ã€å†…å®¹ç±»å‹ç­‰çº¿ç´¢ï¼Œæ¨æ–­ç”¨æˆ·çš„å®¡ç¾åå¥½å¹¶è¿”å›JSONï¼š

{{
    "aesthetic_profile": {{
        "style_preference": "é£æ ¼åå¥½ï¼ˆå¦‚ï¼šminimalist, maximalist, classic, modern, artistic, functionalï¼‰",
        "complexity_tolerance": "å¤æ‚åº¦è€å—åº¦ï¼ˆlow, moderate, high, very_highï¼‰",
        "color_personality": "é¢œè‰²ä¸ªæ€§ï¼ˆwarm, cool, neutral, vibrant, muted, monochromeï¼‰",
        "cultural_context": "æ–‡åŒ–èƒŒæ™¯æ¨æ–­ï¼ˆå¦‚ï¼šeastern, western, contemporary, traditionalï¼‰"
    }},
    
    "inferred_preferences": {{
        "visual_elements": ["å¯èƒ½å–œæ¬¢çš„è§†è§‰å…ƒç´ åˆ—è¡¨ï¼Œå¦‚ï¼šclean_lines, organic_shapes, geometric_patterns"],
        "avoided_elements": ["å¯èƒ½ä¸å–œæ¬¢çš„å…ƒç´ åˆ—è¡¨ï¼Œå¦‚ï¼šclutter, harsh_contrasts, overly_decorative"],
        "mood_preferences": ["æƒ…ç»ªè°ƒæ€§åå¥½ï¼Œå¦‚ï¼šcalm, energetic, sophisticated, playful"]
    }},
    
    "confidence_metrics": {{
        "preference_certainty": 0.0-1.0ä¹‹é—´çš„åå¥½ç¡®å®šæ€§åˆ†æ•°,
        "cultural_accuracy": 0.0-1.0ä¹‹é—´çš„æ–‡åŒ–èƒŒæ™¯å‡†ç¡®æ€§åˆ†æ•°,
        "personalization_potential": 0.0-1.0ä¹‹é—´çš„ä¸ªæ€§åŒ–æ½œåŠ›åˆ†æ•°
    }}
}}

æ¨æ–­ä¾æ®ï¼š
- è¯­è¨€é£æ ¼ï¼šæ­£å¼/éæ­£å¼ã€æŠ€æœ¯æ€§/åˆ›æ„æ€§ç­‰
- å†…å®¹åå¥½ï¼šç®€æ´/è¯¦ç»†ã€æŠ½è±¡/å…·è±¡ç­‰
- äº¤äº’æ–¹å¼ï¼šç›´æ¥/å§”å©‰ã€å¿«é€Ÿ/æ·±å…¥ç­‰
- æ–‡åŒ–çº¿ç´¢ï¼šè¡¨è¾¾ä¹ æƒ¯ã€ä»·å€¼è§‚ç­‰

è¯·ä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        )
        
        return tasks
    
    def analyze(self, 
                text: str, 
                tasks: Union[List[str], List[AnalysisTask], str],
                **kwargs) -> SemanticAnalysisResponse:
        """
        æ‰§è¡Œè¯­ä¹‰åˆ†æ
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            tasks: åˆ†æä»»åŠ¡åˆ—è¡¨ï¼Œå¯ä»¥æ˜¯ä»»åŠ¡åç§°å­—ç¬¦ä¸²åˆ—è¡¨ã€AnalysisTaskå¯¹è±¡åˆ—è¡¨æˆ–å•ä¸ªä»»åŠ¡å
            **kwargs: é¢å¤–çš„åˆ†æå‚æ•°
            
        Returns:
            SemanticAnalysisResponse: åˆ†æç»“æœ
        """
        start_time = time.time()
        self.stats['total_analyses'] += 1
        
        try:
            # ç»Ÿä¸€ä»»åŠ¡æ ¼å¼
            task_list = self._prepare_tasks(tasks)
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(text, task_list)
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                self.stats['cache_hits'] += 1
                logger.debug(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {cache_key}")
                return cached_result
                
            # æ‰§è¡Œåˆ†æ
            results = {}
            overall_success = True
            llm_provider = None
            
            for task in task_list:
                try:
                    result = self._execute_single_task(text, task, **kwargs)
                    results[task.task_type.value] = result
                    
                    if not result.success:
                        overall_success = False
                        
                    # è®°å½•ä½¿ç”¨çš„LLMæä¾›å•†
                    if llm_provider is None and hasattr(self.llm_manager, 'last_used_provider'):
                        llm_provider = getattr(self.llm_manager, 'last_used_provider', None)
                        
                except Exception as e:
                    logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ {task.task_type.value}: {e}")
                    results[task.task_type.value] = AnalysisResult(
                        task_type=task.task_type,
                        result={},
                        confidence=0.0,
                        processing_time=0.0,
                        success=False,
                        error_message=str(e)
                    )
                    overall_success = False
            
            # åˆ›å»ºå“åº”å¯¹è±¡
            total_time = time.time() - start_time
            response = SemanticAnalysisResponse(
                input_text=text,
                analysis_results=results,
                total_processing_time=total_time,
                overall_success=overall_success,
                cache_hit=False,
                llm_provider=llm_provider
            )
            
            # ç¼“å­˜ç»“æœ
            self._cache_result(cache_key, response)
            
            # æ›´æ–°ç»Ÿè®¡
            if overall_success:
                self.stats['successful_analyses'] += 1
            else:
                self.stats['failed_analyses'] += 1
            self.stats['total_processing_time'] += total_time
            
            logger.info(f"ğŸ” è¯­ä¹‰åˆ†æå®Œæˆ: {len(results)}é¡¹ä»»åŠ¡, è€—æ—¶{total_time:.2f}s")
            return response
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            self.stats['failed_analyses'] += 1
            
            return SemanticAnalysisResponse(
                input_text=text,
                analysis_results={},
                total_processing_time=time.time() - start_time,
                overall_success=False,
                cache_hit=False,
                llm_provider=None
            )
    
    def _prepare_tasks(self, tasks: Union[List[str], List[AnalysisTask], str]) -> List[AnalysisTask]:
        """å‡†å¤‡åˆ†æä»»åŠ¡åˆ—è¡¨"""
        if isinstance(tasks, str):
            # å•ä¸ªä»»åŠ¡åç§°
            if tasks in self.builtin_tasks:
                return [self.builtin_tasks[tasks]]
            else:
                raise ValueError(f"æœªçŸ¥çš„å†…ç½®ä»»åŠ¡: {tasks}")
                
        elif isinstance(tasks, list):
            task_list = []
            for task in tasks:
                if isinstance(task, str):
                    # ä»»åŠ¡åç§°å­—ç¬¦ä¸²
                    if task in self.builtin_tasks:
                        task_list.append(self.builtin_tasks[task])
                    else:
                        raise ValueError(f"æœªçŸ¥çš„å†…ç½®ä»»åŠ¡: {task}")
                elif isinstance(task, AnalysisTask):
                    # AnalysisTaskå¯¹è±¡
                    task_list.append(task)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {type(task)}")
            return task_list
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡æ ¼å¼: {type(tasks)}")
    
    def _execute_single_task(self, text: str, task: AnalysisTask, **kwargs) -> AnalysisResult:
        """æ‰§è¡Œå•ä¸ªåˆ†æä»»åŠ¡"""
        start_time = time.time()
        
        try:
            # å‡†å¤‡æç¤ºè¯
            prompt = task.prompt_template.format(text=text) if task.prompt_template else f"è¯·åˆ†æ: {text}"
            
            # è°ƒç”¨LLM (è¿™é‡Œéœ€è¦å®ç°LLMè°ƒç”¨é€»è¾‘)
            llm_response = self._call_llm(prompt, task, **kwargs)
            
            # è§£æå“åº”
            try:
                result_data = json.loads(llm_response) if isinstance(llm_response, str) else llm_response
                confidence = result_data.get('confidence', task.confidence_threshold)
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é™çº§å¤„ç†: {e}")
                result_data = {"raw_response": llm_response, "parse_error": str(e)}
                confidence = 0.5
            
            processing_time = time.time() - start_time
            
            return AnalysisResult(
                task_type=task.task_type,
                result=result_data,
                confidence=confidence,
                processing_time=processing_time,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸ {task.task_type.value}: {e}")
            
            return AnalysisResult(
                task_type=task.task_type,
                result={},
                confidence=0.0,
                processing_time=processing_time,
                success=False,
                error_message=str(e)
            )
    
    def _call_llm(self, prompt: str, task: AnalysisTask, **kwargs) -> str:
        """è°ƒç”¨LLMè¿›è¡Œåˆ†æ"""
        if not self.llm_manager:
            # å¦‚æœæ²¡æœ‰LLMç®¡ç†å™¨ï¼Œå°è¯•åˆ›å»ºä¸€ä¸ª
            try:
                from ..providers.llm_manager import LLMManager
                self.llm_manager = LLMManager()
                logger.info("ğŸ¤– è‡ªåŠ¨åˆ›å»ºLLMç®¡ç†å™¨")
            except ImportError:
                logger.error("âŒ æ— æ³•å¯¼å…¥LLMManagerï¼ŒSemanticAnalyzeréœ€è¦LLMæ”¯æŒ")
                raise RuntimeError("SemanticAnalyzer requires LLM support")
        
        try:
            # æ„å»ºç³»ç»Ÿæ¶ˆæ¯
            system_message = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­ä¹‰åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£åˆ†ææ–‡æœ¬å¹¶è¿”å›ç»“æ„åŒ–çš„JSONç»“æœã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—è¯´æ˜ã€‚"
            
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ]
            
            # ä½¿ç”¨LLMç®¡ç†å™¨è°ƒç”¨
            response = self.llm_manager.chat_completion(
                messages=messages,
                model=kwargs.get('model', self.config['model_name']),
                temperature=kwargs.get('temperature', self.config['temperature']),
                max_tokens=kwargs.get('max_tokens', 2000),
                timeout=kwargs.get('timeout', self.config['timeout'])
            )
            
            if response and response.success:
                # è®°å½•ä½¿ç”¨çš„æä¾›å•†
                if hasattr(response, 'provider'):
                    self.llm_manager.last_used_provider = response.provider
                return response.content.strip()
            else:
                error_msg = response.error_message if response else "LLMè°ƒç”¨æ— å“åº”"
                logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {error_msg}")
                raise RuntimeError(f"LLMè°ƒç”¨å¤±è´¥: {error_msg}")
                
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¼‚å¸¸: {e}")
            raise
    
    def _generate_cache_key(self, text: str, tasks: List[AnalysisTask]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        task_signatures = [f"{task.task_type.value}:{task.description[:50]}" for task in tasks]
        combined = f"{text}|{','.join(task_signatures)}"
        return hashlib.md5(combined.encode('utf-8')).hexdigest()
    
    def _get_cached_result(self, cache_key: str) -> Optional[SemanticAnalysisResponse]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if cache_key in self.analysis_cache:
            cached_data, timestamp = self.analysis_cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                cached_data.cache_hit = True
                return cached_data
            else:
                # ç¼“å­˜å·²è¿‡æœŸ
                del self.analysis_cache[cache_key]
        return None
    
    def _cache_result(self, cache_key: str, result: SemanticAnalysisResponse):
        """ç¼“å­˜åˆ†æç»“æœ"""
        self.analysis_cache[cache_key] = (result, time.time())
        
        # ç®€å•çš„ç¼“å­˜æ¸…ç†ï¼šå¦‚æœç¼“å­˜è¿‡å¤šï¼Œæ¸…ç†ä¸€åŠ
        if len(self.analysis_cache) > 1000:
            keys_to_remove = list(self.analysis_cache.keys())[:500]
            for key in keys_to_remove:
                del self.analysis_cache[key]
            logger.info("ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ")
    
    def add_custom_task(self, task: AnalysisTask) -> None:
        """æ·»åŠ è‡ªå®šä¹‰åˆ†æä»»åŠ¡"""
        self.builtin_tasks[task.task_type.value] = task
        logger.info(f"âœ… å·²æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡: {task.task_type.value}")
    
    def get_available_tasks(self) -> Dict[str, str]:
        """è·å–å¯ç”¨çš„åˆ†æä»»åŠ¡åˆ—è¡¨"""
        return {task_id: task.description for task_id, task in self.builtin_tasks.items()}
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        stats['cache_hit_rate'] = (
            self.stats['cache_hits'] / max(self.stats['total_analyses'], 1)
        )
        stats['success_rate'] = (
            self.stats['successful_analyses'] / max(self.stats['total_analyses'], 1)  
        )
        stats['average_processing_time'] = (
            self.stats['total_processing_time'] / max(self.stats['total_analyses'], 1)
        )
        return stats
    
    def clear_cache(self):
        """æ¸…ç©ºåˆ†æç¼“å­˜"""
        self.analysis_cache.clear()
        logger.info("ğŸ§¹ åˆ†æç¼“å­˜å·²æ¸…ç©º")
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_analyses': 0,
            'cache_hits': 0,
            'successful_analyses': 0,
            'failed_analyses': 0,
            'total_processing_time': 0.0
        }
        logger.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")


# ä¾¿æ·å‡½æ•°
def create_semantic_analyzer(llm_manager=None, config=None) -> SemanticAnalyzer:
    """åˆ›å»ºè¯­ä¹‰åˆ†æå™¨å®ä¾‹"""
    return SemanticAnalyzer(llm_manager=llm_manager, config=config)

def quick_analyze(text: str, tasks: Union[List[str], str] = "intent_detection", 
                 llm_manager=None) -> Dict[str, Any]:
    """å¿«é€Ÿè¯­ä¹‰åˆ†æ - ä¾¿æ·æ–¹æ³•"""
    analyzer = create_semantic_analyzer(llm_manager)
    response = analyzer.analyze(text, tasks)
    
    # è¿”å›ç®€åŒ–çš„ç»“æœå­—å…¸
    results = {}
    for task_type, result in response.analysis_results.items():
        if result.success:
            results[task_type] = result.result
        else:
            results[task_type] = {"error": result.error_message}
    
    return results


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    print("ğŸ” SemanticAnalyzer æµ‹è¯•")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = create_semantic_analyzer()
    
    # æµ‹è¯•åˆ†æ
    test_text = "æˆ‘æ€¥éœ€ä¸€ä¸ªé«˜æ€§èƒ½çš„æœºå™¨å­¦ä¹ APIè§£å†³æ–¹æ¡ˆ"
    test_tasks = ['intent_detection', 'sentiment_analysis', 'complexity_assessment', 'domain_classification']
    
    print(f"æµ‹è¯•æ–‡æœ¬: {test_text}")
    print(f"åˆ†æä»»åŠ¡: {test_tasks}")
    
    # æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•éœ€è¦æœ‰LLMç®¡ç†å™¨æ”¯æŒæ‰èƒ½æ­£å¸¸è¿è¡Œ
    try:
        response = analyzer.analyze(test_text, test_tasks)
        print(f"åˆ†æç»“æœ: {response}")
        print(f"ç»Ÿè®¡ä¿¡æ¯: {analyzer.get_stats()}")
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥ï¼ˆæ­£å¸¸ï¼Œéœ€è¦LLMæ”¯æŒï¼‰: {e}")
    
    print("âœ… SemanticAnalyzer æ¨¡å—åŠ è½½æˆåŠŸ")
